:toc2:
:doctitle: {_doctitle} - The Apache Hadoop Platform

[[apache-hadoop]]
== The Apache Hadoop Platforms

This chapter covers some of the operational mechanics of running an application
that uses Cascading with the Hadoop platform, including building the application
JAR file and configuring the operating mode.

Cascading requires that Apache Hadoop be installed and correctly configured.
Hadoop is an open-source Apache project, which is freely available for download
from the link:http://hadoop.apache.org/core/[Hadoop website].

=== What is Apache Hadoop?

The Hadoop website describes the technology as "a software platform that lets
one easily write and run applications that process vast amounts of data." Hadoop
does this by providing a storage layer that holds vast amounts of data and an
execution layer that runs an application in parallel across the cluster. The
platform  distributes the application load as subsets of the stored data across
the cluster and coordinates the distributed processing tasks to optimize the
compute resources of the environment.

The _Hadoop Distributed File System (HDFS)_ is the storage layer,  which serves
as a single storage volume that is optimized for many concurrent serialized
reads of large data files -- where "large" might be measured in gigabytes or
petabytes. However, HDFS does have limitations. For example, random access to
the data is not really possible in an efficient manner. Also, Hadoop only
supports a single  writer for output. But this limit helps make Hadoop
high-performance and  reliable, in part because it allows for the data to be
replicated across the  cluster. The replication reduces the chance of data loss.

_MapReduce_ is the default execution layer, which relies on a
"divide-and-conquer" strategy to manage massive data sets and computing
processes. Fully explaining MapReduce is beyond the scope of this document.
However, the difficulty of developing real-world applications for the complex
MapReduce framework is the original driving force behind the creation of
Cascading.

As of Cascading 3.0, http://tez.apache.org[Apache Tez] has become a third
execution layer, or platform, option replacing _MapReduce_. See
<<mr-tez,Hadoop 2 MapReduce vs Hadoop 2 Tez>> below.

Hadoop, according to its documentation, can be configured to run in three modes:

* Stand-alone mode (i.e., on the local computer, useful for testing and
  debugging in an IDE)
* Pseudo-distributed mode (i.e., on an emulated "cluster" of one computer, which
  generally is not useful)
* Fully-distributed mode (on a full cluster, for staging or production purposes)

The pseudo-distributed mode does not add value for most purposes. This
documentation does not explore this mode further.

Cascading itself can run in <<ch09-local.adoc#local-platform,local mode>> or on
the Hadoop platform, where Hadoop itself may be in stand-alone or distributed
mode.

The primary difference between these two platforms, local or Hadoop, is that
local-mode Cascading does not use Hadoop APIs and processes data in memory.
In-memory processing quickens the runtime of applications, but consequently
running on local mode is not as robust or scalable as running on the Hadoop
platform.

=== Hadoop 1 MapReduce vs. Hadoop 2 MapReduce

Cascading supports both Hadoop 1.x and 2.x by providing two Java dependencies:
[code]+cascading-hadoop.jar+ and [code]+cascading-hadoop2-mr1.jar+. These
dependencies can be interchanged but the [code]+hadoop2-mr1.jar+ introduces new
APIs and deprecates older API calls where appropriate. It should be pointed out
[code]+hadoop2-mr1.jar+ only supports MapReduce 1 API conventions. With this
naming scheme new API conventions can be introduced without risk of naming
collisions on dependencies.

NOTE: It is extremely important to use the correct Cascading Hadoop dependencies
that correspond with the cluster version to which the Cascading application
is deployed. There are a number of subtle API and configuration property
differences that can be difficult to diagnose.

[[mr-tez]]
=== Hadoop 2 MapReduce vs Hadoop 2 Tez

Apache Hadoop 2 introduces YARN as the resource manager for the cluster. In
short, this allows a single physical cluster to contain and execute different
computing foundations simultaneously. By default a Hadoop 2 installation
includes an implementation of MapReduce that is API-compatible with Apache
Hadoop 1.

But YARN also allows other technologies to co-exist and share the same HDFS
deployment. Apache Tez is one such technology. A single Hadoop YARN cluster can
now execute some applications on MapReduce and others on Tez, where both
frameworks could share or pass data via HDFS. The two resource managers can
run concurrently without cluster-wide reconfiguration.

Apache Tez has yet to reach a 1.0 status, but is now currently supported by
Cascading via the [code]+cascading-hadoop2-tez.jar+ dependency.

Since both MapReduce and Tez share HDFS, both implementations share much of the
same APIs provided by Cascading. This greatly simplifies porting applications
from MapReduce to Tez, in most cases the changes are to the build and to one
line of code to switch to the appropriate [code]+FlowConnector+.

The following sections describe the common features of each supported
Hadoop-based platform.

See the <<ch12-hadoop-tez.adoc#tez-platform, "Apache Tez Platform">>
documentation of this _Cascading User Guide_ for more information on running
Apache Tez.

[[configuring]]
=== Configuring Applications

During runtime, Hadoop must be told which application JAR file to push
to the cluster. The specific details are described in the platform-specific
topics of this _Cascading User Guide_.

But to remain platform-independent, the [classname]+AppProps+ class can be used,
which is a helper fluent API for setting application-level configuration
settings. Use this class for maximum portability.

.Configuring the application JAR
====

include::app-props.adoc[]

====

Other Hadoop-specific Props classes include:

|====

| [classname]+cascading.tap.hadoop.HfsProps+ | Allows for setting
Hadoop-specific filesystem properties, specifically properties around enabling
the "combined input format" support. Combining inputs minimizes the performance
penalty around processing large numbers of small files.

| [classname]+cascading.tuple.hadoop.TupleSerializationProps+ | Allows for
setting Hadoop-specific serialization and deserialization properties. See
<<custom-types,Custom Types>>.

|====

[[building]]
=== Building an Application

See the platform-specific topics of this _Cascading User Guide_ for information
about building an application for execution on a particular platform.

[[executing]]
=== Executing an Application

Running a Cascading application is the same as running any Hadoop application.

After packaging your application into a single JAR (see <<building>>), you must
use [code]+bin/hadoop+ on Hadoop 1, or [code]+bin/yarn+ on Hadoop 2, to submit
the application to the cluster.

For example, to execute an application stuffed into
[code]+your-application.jar+, call the Hadoop 2 shell script:

.Running a Cascading application
====
----
$HADOOP_HOME/bin/yarn jar your-application.jar [your parameters]
----
====

If the configuration scripts in [code]+$HADOOP_CONF_DIR+ are configured to use a
cluster, the JAR is pushed into that cluster for execution.

Cascading does not rely on any environment variables like [code]+$HADOOP_HOME+
or [code]+$HADOOP_CONF_DIR+. Only the executable binary files in
[code]+bin/hadoop+ and [code]+bin/yarn+ rely on those environment variables.

It should be noted that even though [code]+your-application.jar+ is passed on
the command line to [code]+bin/hadoop+ or [code]+bin/yarn+, this in no way
configures Hadoop to push the JAR into the cluster. You must still call one of
the property setters mentioned above to set the proper path to the application
JAR. If misconfigured, it's likely that one of the internal libraries (found in
the lib folder) will be pushed to the cluster instead, and "Class Not Found"
exceptions will be thrown on the cluster.

[[debugging]]
=== Troubleshooting and Debugging

Read about debugging and troubleshooting Cascading in
<<ch09-local.adoc#debugging,local-mode troubleshooting>> before you start
debugging.

For planner-related errors that present during runtime when executing a Flow,
see <<ch21-query-process-planner.adoc#process-planner,The Cascading
Process Planner>>.

[[source-sink]]
=== Source and Sink Taps

Cascading provides a few HDFS-specific Tap and Scheme implementations.
Regardless of the build dependencies, the package names remain consistent.

It is important to understand how Hadoop deals with directories. By default,
Hadoop cannot source data from directories with nested subdirectories, and it
cannot write to directories that already exist.

However, the good news is that you can simply point the [classname]+Hfs+ tap
(described below) to a directory of data files. The data files are all used as
input. There is no need to enumerate each individual file into a
[classname]+MultiSourceTap+. If there are nested directories, use
[classname]+GlobHfs+.

==== Schemes

See the section on
<<ch03-basic-concepts.adoc#common-schemes, Common Schemes>> for information
about Hadoop-specific versions of [code]+TextLine+ and
[classname]+TextDelimited+.

NOTE: The Hadoop version of [code]+TextLine+, does not provide a line number.
The [code]+offset+ field is used, instead of the [code]+num+ field that is
provided in the local-mode version.

[classname]+SequenceFile+::

[classname]+cascading.scheme.hadoop.SequenceFile+ is based on the Hadoop
Sequence file format, which is a binary format. When written to or read from,
all Tuple values are saved in a binary form. This is the most efficient file
format. However, be aware that the resulting files are binary and can only be
read by applications running on the Hadoop platform.

[classname]+WritableSequenceFile+::

Like the [classname]+SequenceFile+ Scheme,
[classname]+cascading.scheme.hadoop.WritableSequenceFile+ is based on the Hadoop
Sequence file, but it was designed to read and write key and/or value Hadoop
[classname]+Writable+ objects directly.

+

This is very useful if you have sequence files created by other applications.
During writing (sinking), specified key and/or value fields are serialized
directly into the sequence file. During reading (sourcing), the key and/or value
objects are deserialized and wrapped in a Cascading Tuple object and passed to
the downstream pipe assembly.

TIP: For best performance when running on the Hadoop platform, enable sequence
file compression (either block or record-based compression) in the Hadoop
property settings. Refer to the Hadoop documentation for the available
properties and compression types.

==== Taps

The following sample code creates a new Hadoop filesystem tap that can read and
write raw text files. The "offset" field is discarded because only one field
name is provided. The resulting input tuple stream has only "line" values.

.Creating a new tap
====
include::h2mr1-simple-tap.adoc[]
====

Here are the most commonly-used tap types:

[classname]+Hfs+::

The [classname]+cascading.tap.hadoop.Hfs+ tap uses the current Hadoop default
filesystem.

+

If Hadoop is configured for "Hadoop stand-alone mode," the default filesystem
is local filesystem (using a URI scheme of [code]+file://+). If Hadoop is
configured for distributed mode, the default filesystem is typically the Hadoop
distributed file system ([code]+hdfs://+).

+

Note that Hadoop can be forced to use an external filesystem by specifying a
prefix to the URL passed into a new Hfs tap. For instance, using
"s3://somebucket/path" tells Hadoop to use the S3 [classname]+FileSystem+
implementation to access files in an Amazon S3 bucket. More information on this
can be found in the Javadoc.

[classname]+GlobHfs+::

The [classname]+cascading.tap.hadoop.GlobHfs+ tap accepts Hadoop-style "file
globbing" expression patterns. This allows for multiple paths to be used as a
single source, where all paths match the given pattern.

[classname]+DistCacheTap+::

The [classname]+cascading.tap.hadoop.DistCacheTap+ is a subclass of the
<<ch03-basic-concepts.adoc#common-taps,[classname]+cascading.tap.DecoratorTap+>>,
which can wrap an [classname]+Hfs+ instance. The tap enables writing to HDFS,
but directs read operations to the Hadoop Distributed Cache if the  tap is being
read into the small side of a [classname]+cascading.pipe.HashJoin+.

NOTE: At the time of this writing, the Apache Hadoop project does not have
documentation for a "Distributed  Cache" implementation for Tez, but Cascading
leverages YARN's ability to distribute files transparently.

[classname]+PartitionTap+::

See
<<ch03-basic-concepts.adoc#common-taps,[classname]+cascading.tap.hadoop.PartitionTap+>>
for details.

NOTE: On Hadoop you can only create subdirectories to partition data into.
Hadoop must still write "part files" into each partition sub-directory, and
there is no safe mechanism for manipulating "part file" names.

=== Custom Taps and Schemes

You can integrate Cascading with either remote systems or file types by
developing code that executes one of the following implementations:

Support of a new _file type_::

This implementation involves introducing a different file format for a file
stored on an HDFS filesystem. You only need to create a new Scheme for use
with the [classname]+Hfs+ tap. See below for details.

Support of a new _filesystem_::

A new file system, like Amazon S3, can be implemented by creating a new Hadoop
[classname]+FileSystem+ subclass. This implementation can be referenced by
registering a new URI scheme (Amazon S3 uses [code]+s3://+).

+

Such an implementation allows for the [classname]+Hfs+ tap to be used with
any pre-existing [classname]+Scheme+ implementations (like
[classname]+TextDelimited+).

Support of a new _data storage system_::

By creating a new Tap and possibly a compatible Scheme, your application can
read or write data on a back-end system, such as a database or key-value
store.

NOTE: Tap and Scheme implementations are typically tied together. The purpose of
the Tap and Scheme interface is to provide a consistent interface
over the disparate technologies that operate with Cascading. The interface is
not designed to hide implementation details.

Before beginning, or for inspiration, please see if any integrations for your
file type or storage system already exist by visiting the
http://cascading.org/extensions/[Cascading extensions page].

==== Schemes

If the goal is to simply add a new _file type_ to be stored on a
Hadoop-compatible filesystem, only a new [classname]+Scheme+ needs to be
implemented.

As noted above [classname]+Tap+ and [classname]+Scheme+ implementations are
coupled. In this case the goal is to create a Scheme that works with the
[classname]+Hfs+ tap. Hfs is an encapsulation of the Hadoop FileSystem API, in
part. Creating a new file type involves only finding or implementing an
appropriate [classname]+org.apache.hadoop.mapred.FileInputFormat+ and
[classname]+org.apache.hadoop.mapred.FileOutputFormat+ interface.

Every [classname]+Scheme+ can set any custom properties that the underlying
platform requires, via the [methodname]+sourceConfInit()+ and
[methodname]+sinkConfInit()+ methods. These  methods may be called more than
once with new configuration objects, and should be idempotent (that is, the
methods do not initialize resources).

Once the proper [classname]+FileInputFormat+ and [classname]+FileOutputFormat+
are created, creating the [classname]+Scheme+ implementation is a matter of
implementing the [methodname]+*ConfInit()+ methods and the
[methodname]+source()+ and [methodname]+sink()+ methods to perform the key-value
conversion to a Tuple (when sourcing data), and back again (on the sinking of
data).

TIP: It is highly recommended that the Cascading source code is checked. In
many cases simply overriding [classname]+SequenceFile+ or
[classname]+WritableSequenceFile+ might be suitable.

NOTE: Cascading only supports the [code]+mapred.*+ APIs, not the
[code]+mapreduce.*+ APIs.  Unfortunately, Cascading cannot support both APIs
simultaneously on the MapReduce framework, so the more stable original version
was chosen. Apache Tez relaxes this restriction, allowing future versions
to support Input/OutputFormat implementations from both Java package namespaces.

==== Taps

If the goal is to simply add a new HDFS-compatible file type, see the
<<ch10-hadoop-common.adoc#source-sink, Source and Sink Taps>> section above.

If the goal is to add a new data store type, the following information explains
the procedure.

TIP: Creating custom taps for HDFS and Hadoop MapReduce requires intermediate
to advanced knowledge of Hadoop and the Hadoop FileSystem API.

First the developer must implement Hadoop
[classname]+org.apache.hadoop.mapred.InputFormat+ and/or
[classname]+org.apache.hadoop.mapred.OutputFormat+ interfaces so that Hadoop
knows how to split and handle the incoming and outgoing data. See the note
above on API compatibility.

In order to configure Hadoop to use these implementations*, the custom
[classname]+Scheme+ is responsible for setting the [classname]+InputFormat+ and
[classname]+OutputFormat+ on the [classname]+Configuration+ object, using the
[methodname]+sinkConfInit()+ and [methodname]+sourceConfInit()+ methods.

The [classname]+Scheme+ is also responsible for translating the data from the
data storage native type to a Cascading [classname]+Tuple+, and the back to the
data storage native type when writing.

The Tap implementation is typically straightforward except for one detail.
During runtime the [methodname]+openForRead()+ and [methodname]+openForWrite()+
methods are _always_ called. Respectively they return a
[classname]+TupleEntryIterator+ and [classname]+TupleEntryCollector+, which in
turn _always_ produce or receive Tuple and TupleEntry instances to  consume or
store.

There are two cases in which these open methods are called: when the
[code]+Input+ and [code]+Output+ arguments on the [code]+abstract+ methods
[methodname]+openForRead()+ and [methodname]+openForWrite()+

* are [code]+null+ (case 1)
* are not [code]+null+ (case 2)

*Case 1:* The first case happens when the developer wants to read or write data
directly to the underlying system (put/retrieve records into/from a database
table). The following scenarios can produce Case 1:

* Client-side code is running in a test.
* The application loads data into memory and uses the [classname]+Tap+ instance
  as a convenience.
* A lookup table is loaded from the Hadoop Distributed Cache.
* Cascading reads or writes data. For example, when Cascading reads a stream to
  populate the accumulated side of a [classname]+HashJoin+ or when a
  [classname]+PartitionTap+ is writing data to bin directories.

*Case 2:* The second case, not [code]+null+ argument, happens when Hadoop does
one of the following actions:

* Initiates the read, sourcing data specified in an [classname]+InputSplit+
* Manages the write operation from the output of a Map/Reduce/Vertice task

When the method arguments are [code]+null+, it is the responsibility of the Tap
implementation to create the declared input/output types. When they are not,
they must be used directly and not ignored.

On the Hadoop platform, the [code]+Input+ is a [classname]+RecordReader+, which
is created by Hadoop and passed to the Tap. This [classname]+RecordReader+ is
already configured to read data from the current [classname]+InputSplit+.

Also on the Hadoop platform, the [code]+Ouput+ is an
[classname]+OutputCollector+, which is created by Hadoop and passed to the Tap.
This [classname]+OutputCollector+ is already configured to to write data to the
current resource.

To ease the development at this stage, you can reuse the classes
[classname]+cascading.tap.hadoop.io.HadoopTupleEntrySchemeIterator+ and
[classname]+cascading.tap.hadoop.io.HadoopTupleEntrySchemeCollector+ if doing so
is appropriate for your code.

=== Partial Aggregation instead of Combiners

In Hadoop mode, Cascading does not support MapReduce "Combiners." Combiners are
a simple optimization allowing some Reduce functions to run on the Map side of
MapReduce. Combiners are very powerful in that they reduce the I/O between the
Mappers and Reducers -- why send all of your Mapper data to Reducers when you can
compute some values on the Map side and combine them in the Reducer?

But Combiners are limited to Associative and Commutative functions only, such as
"sum" and "max." In addition, the process requires that the values emitted by
the Map task must be serialized, sorted (which involves deserialization and
comparison), deserialized again, and operated on -- after which the results are
again serialized and sorted. Combiners trade CPU for gains in I/O.

Cascading takes a different approach to partial aggregations on the Map side.
When the results are combined on the Reduce side,  Cascading trades memory
instead of CPU for I/O gains. The Cascading mechanism functions by caching
values (up to a threshold limit). This bypasses the redundant serialization,
deserialization, and sorting. Also, Cascading allows any aggregate function to
be implemented -- not just Associative and Commutative functions.

[[custom-types]]
=== Custom Types and Serialization

Cascading supports any class type, except for an enum type, as values stored and
passed in [classname]+Tuple+ instances.

But for this to work when using the Cascading Hadoop mode, any Class that isn't
a primitive type or a Hadoop [classname]+Writable+ type requires a corresponding
Hadoop serialization class registered in the Hadoop configuration files for your
cluster. Hadoop [classname]+Writable+ types work because there is already a
generic serialization implementation built into Hadoop. See the Hadoop
documentation for information on registering a new serialization helper or
creating [classname]+Writable+ types.

Registered serialization implementations are automatically inherited by
Cascading.

During serialization and deserialization of [classname]+Tuple+ instances that
contain custom types, the Cascading [classname]+Tuple+ serialization framework
must store the class name (as a [classname]+String+) before serializing the
custom object. This can be very space-inefficient. To overcome this, custom
types can add the [classname]+SerializationToken+ Java annotation to the custom
type class. The [classname]+SerializationToken+ annotation expects two arrays --
one of integers that are used as tokens, and one of Class name strings. Both
arrays must be the same size. The integer tokens must all have values of 128 or
greater because the first 128 values are reserved for internal use.

During serialization and deserialization, token values are used instead of the
[classname]+String+ Class names in order to reduce storage needs.

Serialization tokens may also be stored in the Hadoop configuration files.
Alternatively, the tokens can be set as a property passed to the
[classname]+FlowConnector+, with the property name
[code]+cascading.serialization.tokens+. The value of this property is a
comma-separated list of [code]+token=classname+ values.

Note that Cascading natively serializes and deserializes all primitives and byte
arrays ([code]+byte[]+) if the developer registers the
[classname]+BytesSerialization+ class by using
[code]+TupleSerializationProps.addSerialization( properties,
BytesSerialization.class.getName() )+. The token 127 is used for the Hadoop
[classname]+BytesWritable+ class.

By default, Cascading uses lazy deserialization on Tuple elements during
comparisons when Hadoop sorts keys during the "shuffle" phase.

Cascading supports custom serialization for custom types, as well as lazy
deserialization of custom types during comparisons. This is accomplished by
implementing the [classname]+StreamComparator+ interface. See the Javadoc for
detailed instructions on implementation, and the unit tests for examples.
