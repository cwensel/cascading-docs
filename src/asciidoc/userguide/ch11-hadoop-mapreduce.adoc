:toc2:
:doctitle: {_doctitle} - Apache Hadoop MapReduce Platform

[[mapreduce-platform]]
== Apache Hadoop MapReduce Platform

By default, Apache Hadoop provides an API called _MapReduce_ for performing
computation at scale.

The following documentation covers details about using Cascading on the
MapReduce platform that are not covered in the
<<ch10-hadoop-common.adoc#apache-hadoop,Apache Hadoop>> documentation of this
guide.

=== Configuring Applications

At runtime, Hadoop must be told which application JAR file should be pushed to
the cluster. Historically, this is done via the Hadoop API [classname]+JobConf+
object, as seen in the example below.

In order to remain platform-independent, use the [classname]+AppProps+ class  as
described in <<ch10-hadoop-common.adoc#configuring,Configuring Applications>>.

If you must use an existing [classname]+JobConf+ instance, consider the example
below:

.Configuring the application JAR with a JobConf
====
include::flow-jobconf.adoc[]
====

In the example above we see two ways to use methods to set the same property:

[methodname]+setJarClass()+::

In this method, you invoke a Class object by name. In the example, the Class
that is named owns the "main" function for this application. The assumption here
is that [code]+Main.class+ is not located in a Java JAR that is stored in the
[code]+lib+ folder of the application JAR. If it is, the dependent lib folder
JAR will be pushed to the cluster, not to the parent application JAR (the JAR
containing the lib folder).

[methodname]+setJarPath()+::

This method requires setting a literal path to the Java JAR as a property.

In your application, only one of these methods must be called to properly
configure Hadoop.

=== Creating Flows from a JobConf

If a MapReduce job already exists, then the
[classname]+cascading.flow.hadoop.MapReduceFlow+ class should be used. To do
this, create a Hadoop [classname]+JobConf+ instance and simply pass it into the
[classname]+MapReduceFlow+ constructor. The resulting [classname]+Flow+ instance
can be used like any other [classname]+Flow+.

Note both multiple [classname]+MapReduceFlow+ instances and other
[classname]+Flow+ instances can be passed to a [classname]+CascadeConnector+ to
produce a [classname]+Cascade+.

[[building]]
=== Building

Cascading ships with several JARs and dependencies in the download archive.

Alternatively, Cascading is available via Maven and Ivy through the Conjars
repository, along with a number of other Cascading-related projects. See
http://conjars.org for more information.

The Cascading Hadoop artifacts include the following:

[code]+cascading-core-3.x.y.jar+::

This JAR contains the Cascading Core class files. It should be packaged with
[code]+lib/*.jar+ when using Hadoop.

[code]+cascading-hadoop-3.x.y.jar+::

This JAR contains the Cascading _Hadoop 1_ specific dependencies. It should be
packaged with [code]+lib/*.jar+ when using Hadoop.

[code]+cascading-hadoop2-mr1-3.x.y.jar+::

This JAR contains the Cascading _Hadoop 2_ specific dependencies. It should be
packaged with [code]+lib/*.jar+ when using Hadoop.

NOTE: Do not package both [code]+cascading-hadoop-3.x.y.jar+ and
[code]+cascading-hadoop2-mr1-3.x.y.jar+ JAR files into your application. Choose
the version that matches your Hadoop distribution version.

Cascading works with either of the Hadoop processing modes: the default local
stand-alone mode and the distributed cluster mode. As specified in the Hadoop
documentation, running in cluster mode requires the creation of a Hadoop job JAR
that includes the Cascading JARs, plus any needed third-party JARs, in its
[code]+lib+ directory. This is true regardless of whether they are Cascading
Hadoop-mode applications or raw Hadoop MapReduce applications.
