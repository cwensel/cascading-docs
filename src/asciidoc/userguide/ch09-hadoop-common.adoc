:toc2:
:doctitle: {_doctitle} - The Apache Hadoop Platform

= The Apache Hadoop Platforms

== Introduction

This section covers some of the operational mechanics of running
an application that uses Cascading with the Hadoop platform, including
building the application jar file and configuring the operating
mode.

To use the [classname]+HadoopFlowConnector+ (i.e., to
run in Hadoop mode), Cascading requires that Apache Hadoop be installed
and correctly configured. Hadoop is an Open Source Apache project,
freely available for download from the Hadoop website, <<,http://hadoop.apache.org/core/>>.

== What is Apache Hadoop?

From the Hadoop website, it "is a software platform that lets one easily write
and run applications that process vast amounts of data". Hadoop does this by
providing a storage layer that holds vast amounts of data, and an execution
layer that runs an application in parallel across the cluster, using coordinated
subsets of the stored data.

The storage layer, called the Hadoop File System (HDFS), looks like a single
storage volume that has been optimized for many concurrent serialized reads of
large data files - where "large" might be measured in gigabytes or petabytes.
However, it does have limitations. For example, random access to the data is not
really possible in an efficient manner. And Hadoop only supports a single writer
for output. But this limit helps make Hadoop very performant and reliable, in
part because it allows for the data to be replicated across the cluster,
reducing the chance of data loss.

The execution layer, called MapReduce, relies on a divide-and-conquer strategy
to manage massive data sets and computing processes. Explaining MapReduce is
beyond the scope of this document, but its complexity, and the difficulty of
creating real-world applications against it, are the chief driving force behind
the creation of Cascading.

Hadoop, according to its documentation, can be configured to run in three modes:
* standalone mode (i.e., on the local computer, useful for testing and debugging in an IDE),
* pseudo-distributed mode (i.e., on an emulated "cluster" of one computer, not useful for much), and
* fully-distributed mode (on a full cluster, for staging or production purposes).

The pseudo-distributed mode does not add
value for most purposes, and will not be discussed further. Cascading itself can
run locally or on the Hadoop platform, where Hadoop itself may be in standalone
or distributed mode. The primary difference between these two platforms, local
or Hadoop, is that, when Cascading is running in local mode, it makes no use of
Hadoop APIs and performs all of its work in memory, allowing it to be very fast -
but consequently not as robust or scalable as when it is running on the Hadoop
platform.

Apache Hadoop is an Open Source Apache project and is freely available. It can
be downloaded from the Hadoop website: <<,http://hadoop.apache.org/core/>>

== Hadoop 1 MapReduce vs Hadoop 2 MapReduce

Cascading supports both Hadoop 1.x and 2.x by providing two Java dependencies,
_cascading-hadoop.jar_ and _cascading-hadoop2-mr1.jar_. These dependencies can
be interchanged but the _hadoop2-mr1.jar_ introduces new and deprecates older
API calls where appropriate. It should be pointed out _hadoop1-mr1.jar_ only
supports MapReduce 1 API conventions. With this naming scheme new API
conventions can be introduced without risk of naming collisions on dependencies.



[[source-sink]]
== Source and Sink Taps

It's also important to understand how Hadoop deals with
directories. By default, Hadoop cannot source data from directories with
nested sub-directories, and it cannot write to directories that already
exist. However, the good news is that you can simply point the
[classname]+Hfs+ tap to a directory of data files, and they
are all used as input - there's no need to enumerate each individual
file into a [classname]+MultiSourceTap+. If there are nested
directories, use [classname]+GlobHfs+.




SequenceFile::

[classname]+SequenceFile+ is based on the Hadoop Sequence file, which is a
binary format. When written to or read from, all Tuple values are saved in their
native binary form. This is the most efficient file format - but be aware that
the resulting files are binary and can only be read by Hadoop applications
running on the Hadoop platform.


WritableSequenceFile::
Like the [classname]+SequenceFile+ Scheme,
[classname]+WritableSequenceFile+ is based on the
Hadoop Sequence file, but it was designed to read and write key
and/or value Hadoop [classname]+Writable+ objects
directly. This is very useful if you have sequence files created
by other applications. During writing (sinking), specified key
and/or value fields are serialized directly into the sequence
file. During reading (sourcing), the key and/or value objects
are deserialized and wrapped in a Cascading Tuple object and
passed to the downstream pipe assembly. This class is only
available when running on the Hadoop platform.


There's a key difference between the
[classname]+TextLine+ and
[classname]+SequenceFile+ schemes. With the
[classname]+SequenceFile+ scheme, data is stored as binary
tuples, which can be read without having to be parsed. But with the
[classname]+TextLine+ option, Cascading must parse each line
into a [classname]+Tuple+ before processing it, causing a
performance hit.



==== Sequence File Compression

For best performance when running on the Hadoop platform,
enable Sequence File Compression in the Hadoop property settings -
either block or record-based compression. Refer to the Hadoop
documentation for the available properties and compression
types.


=== Taps

The following sample code creates a new Hadoop FileSystem Tap that can read and
write raw text files. Since only one field name is provided, the "offset" field
is discarded, resulting in an input tuple stream with only "line" values.

.Creating a new tap
====
include::simple-tap.adoc[]
====

Here are the most commonly-used tap types:

Hfs::

The [classname]+cascading.tap.hadoop.Hfs+ tap
uses the current Hadoop default file system, when running on the
Hadoop platform.

+

If Hadoop is configured for "Hadoop local mode" (not to be confused with
Cascading local mode), its default file system is the local file system. If
configured for distributed mode, its default file system is typically the Hadoop
distributed file system.

+

Note that Hadoop can be forced to use an external file system by specifying a
prefix to the URL passed into a new Hfs tap. For instance, using
"s3://somebucket/path" tells Hadoop to use the S3 [classname]+FileSystem+
implementation to access files in an Amazon S3 bucket. More information on this
can be found in the Javadoc.

GlobHfs::

The [classname]+cascading.tap.hadoop.GlobHfs+ tap accepts Hadoop style "file
globbing" expression patterns. This allows for multiple paths to be used as a
single source, where all paths match the given pattern. This tap is only
available when running on the Hadoop platform.

DistCacheTap::

The [classname]+cascading.tap.hadoop.DistCacheTap+ is a sub-class of the
[classname]+cascading.tap.DecoratorTap+ that can wrap an
cascading.tap.hadoop.Hfs instance. It allows for writing to HDFS, but reading
from the Hadoop Distributed Cache under the write circumstances, specifically if
the Tap is being read into the small side of a
[classname]+cascading.pipe.HashJoin+.


== PartitionTap

Note that you can only create sub-directories to bin data into.
Hadoop must still write "part" files into each bin directory, and there
is no safe mechanism for manipulating part file names.


== Creating Flows from a JobConf

If a MapReduce job already exists and needs to be managed by a
Cascade, then the
[classname]+cascading.flow.hadoop.MapReduceFlow+ class
should be used. To do this, after creating a Hadoop
[classname]+JobConf+ instance simply pass it into the
[classname]+MapReduceFlow+ constructor. The resulting
[classname]+Flow+ instance can be used like any other
Flow.


== Custom Taps and Schemes

=== Taps
When using the Cascading Hadoop mode, it requires some knowledge of Hadoop and
the Hadoop FileSystem API. If a flow needs to support a new file system, passing
a fully-qualified URL to the [classname]+Hfs+ constructor may be sufficient -
the [classname]+Hfs+ tap will look up a file system based on the URL scheme via
the Hadoop FileSystem API. If not, a new system is commonly constructed by
subclassing the [classname]+cascading.tap.Hfs+ class.

Delegating to the Hadoop FileSystem API is not a strict requirement. But if not
using it, the developer must implement Hadoop
[classname]+org.apache.hadoop.mapred.InputFormat+ and/or
[classname]+org.apache.hadoop.mapred.OutputFormat+ classes so that Hadoop knows
how to split and handle the incoming/outgoing data. The custom
[classname]+Scheme+ is responsible for setting the [classname]+InputFormat+ and
[classname]+OutputFormat+ on the [classname]+JobConf+, via the
[methodname]+sinkConfInit+ and [methodname]+sourceConfInit+ methods.


On some platforms, [methodname]+openForRead()+ is called with a pre-instantiated
Input type. Typically this Input type should be used instead of instantiating a
new instance of the appropriate type.

In the case of the Hadoop platform, a
[classname]+RecordReader+ is created by Hadoop and passed to
the Tap. This [classname]+RecordReader+ is already configured
to read data from the current [classname]+InputSplit+.

Again, on some platforms, [methodname]+openForWrite()+ will be called with a
pre-instantiated Output type. Typically this Output type should be used instead
of instantiating a new instance of the appropriate type.

In the case of the Hadoop platform, an [classname]+OutputCollector+ is created
by Hadoop and passed to the Tap. This [classname]+OutputCollector+ is already
configured to to write data to the current resource.


=== Schemes

Every [classname]+Scheme+ is presented the opportunity to set any custom
properties the underlying platform requires, via the methods
[methodname]+sourceConfInit()+ and [methodname]+sinkConfInit()+. These methods
may be called more than once with new configuration objects, and should be
idempotent.

On the Hadoop platform, these methods should be used to configure
the appropriate
[classname]+org.apache.hadoop.mapred.InputFormat+ and
[classname]+org.apache.hadoop.mapred.OutputFormat+.


== Partial Aggregation instead of Combiners

In Hadoop mode, Cascading does not support MapReduce "Combiners". Combiners are
a simple optimization allowing some Reduce functions to run on the Map side of
MapReduce. Combiners are very powerful in that they reduce the I/O between the
Mappers and Reducers - why send all of your Mapper data to Reducers when you can
compute some values on the Map side and combine them in the Reducer? But
Combiners are limited to Associative and Commutative functions only, such as
"sum" and "max". And the process requires that the values emitted by the Map
task must be serialized, sorted (which involves deserialization and comparison),
deserialized again, and operated on - after which the results are again
serialized and sorted. Combiners trade CPU for gains in I/O.

Cascading takes a different approach. It provides a mechanism to
perform partial aggregations on the Map side and combine the results on
the Reduce side, but trades memory, instead of CPU, for I/O gains by
caching values (up to a threshold limit). This bypasses the redundant
serialization, deserialization, and sorting. Also, Cascading allows any
aggregate function to be implemented - not just Associative and
Commutative functions.


[[custom-types]]
== Custom Types and Serialization

But for this to work when using the Cascading Hadoop mode, any
Class that isn't a primitive type or a Hadoop
[classname]+Writable+ type requires a corresponding Hadoop
serialization class registered in the Hadoop configuration files for
your cluster. Hadoop [classname]+Writable+ types work because
there is already a generic serialization implementation built into
Hadoop. See the Hadoop documentation for information on registering a
new serialization helper or creating [classname]+Writable+
types. Registered serialization implementations are automatically
inherited by Cascading.

During serialization and deserialization of
[classname]+Tuple+ instances that contain custom types, the
Cascading [classname]+Tuple+ serialization framework must
store the class name (as a [classname]++String++) before
serializing the custom object. This can be very space-inefficient. To
overcome this, custom types can add the
[classname]+SerializationToken+ Java annotation to the custom
type class. The [classname]+SerializationToken+ annotation
expects two arrays - one of integers that are used as tokens, and one of
Class name strings. Both arrays must be the same size. The integer
tokens must all have values of 128 or greater, since the first 128
values are reserved for internal use.

During serialization and deserialization, the token values are
used instead of the [classname]+String+ Class names, in order
to reduce the amount of storage used.

Serialization tokens may also be stored in the Hadoop config files
or set as a property passed to the [classname]+FlowConnector+,
with the property name [code]+cascading.serialization.tokens+. The
value of this property is a comma separated list of
[code]+token=classname+ values.

Note that Cascading natively serializes/deserializes all
primitives and byte arrays ([code]++byte[]++), if the developer
registers the [classname]++BytesSerialization++ class by using
[code]++TupleSerializationProps.addSerialization(properties,
BytesSerialization.class.getName()++. The token 127 is used for the
Hadoop [classname]++BytesWritable++ class.

By default, Cascading uses lazy deserialization on Tuple elements
during comparisons when Hadoop sorts keys during the "shuffle"
phase.

Cascading supports custom serialization for custom types, as well
as lazy deserialization of custom types during comparisons. This is
accomplished by implementing the [classname]+StreamComparator+
interface. See the Javadoc for detailed instructions on implemention,
and the unit tests for examples.




[[building]]
== Building

Cascading ships with several jars and dependencies in the download
archive. Alternatively, Cascading is available over Maven and Ivy
through the Conjars repository, along with a number of other
Cascading-related projects. See <<,http://conjars.org >> for more
information.

The core Cascading artifacts include the following:

cascading-core-2.6.x.jar::
This jar contains the Cascading Core class files. It should
be packaged with _lib/*.jar_ when using
Hadoop.


cascading-local-2.6.x.jar::
This jar contains the Cascading local mode class files. It
is not needed when using Hadoop.


cascading-hadoop-2.6.x.jar::
This jar contains the Cascading Hadoop 1 specific
dependencies. It should be packaged with
_lib/*.jar_ when using Hadoop.


cascading-hadoop2-mr1-2.6.x.jar::
This jar contains the Cascading Hadoop 2 specific
dependencies. It should be packaged with
_lib/*.jar_ when using Hadoop.


cascading-xml-2.6.x.jar::
This jar contains Cascading XML module class files and is
optional. It should be packaged with
_lib/xml/*.jar_ when using Hadoop.


Cascading works with either of the Hadoop processing modes - the
default local standalone mode and the distributed cluster mode. As
specified in the Hadoop documentation, running in cluster mode requires
the creation of a Hadoop job jar that includes the Cascading jars, plus
any needed third-party jars, in its _lib_ directory.
This is true regardless of whether they are Cascading Hadoop-mode
applications or raw Hadoop MapReduce applications.



== Configuring

During runtime, Hadoop must be told which application jar file
should be pushed to the cluster. Typically, this is done via the Hadoop
API [classname]+JobConf+ object.

Cascading offers a shorthand for configuring this parameter,
demonstrated here:

Above we see two ways to set the same property - via the
[methodname]+setJarClass()+ method, and via the
[methodname]+setJarPath()+ method. One is based on a Class
name, and the other is based on a literal path.

The first method takes a Class object that owns the "main"
function for this application. The assumption here is that
[code]+Main.class+ is not located in a Java Jar that is stored in
the _lib_ folder of the application Jar. If it is,
that Jar is pushed to the cluster, not the parent application
jar.

The second method simply sets the path to the Java Jar as a
property.

In your application, only one of these methods needs to be called,
but one of them must be called to properly configure Hadoop.

.Configuring the Application Jar with a JobConf
====
include::flow-jobconf.adoc[]
====

Above we are starting with an existing Hadoop
[classname]+JobConf+ instance and building a Properties object
with it as the default.

Note that [classname]+AppProps+ is a helper fluent API
for setting properties that define Flows or configure the underlying
platform. There are quite a few "Props" based classes that expose fluent
API calls, the ones most commonly used are below.


|===============
|[classname]+cascading.property.AppProps+|Allows for setting application specific properties. Some
properties are required by the underlying platform, like
application Jar. Others are simple meta-data used by compatible
management tools, like tags.
|[classname]+cascading.flow.FlowConnectorProps+|Allows for setting a [classname]+DebugLevel+ or
[classname]+AssertionLevel+ for a given FlowConnector
to target. Also allows for setting intermediate
[classname]+DecoratorTap+ sub-classes to be used if
any.
|[classname]+cascading.flow.FlowProps+|Allows for setting any Flow specific properties like the
maximum concurrent steps to be scheduled, or changing the
default Tuple Comparator class.
|[classname]+cascading.cascade.CascadeProps+|Allows for setting any Cascade specific properties like
the maximum concurrent Flows to be scheduled.
|[classname]+cascading.tap.hadoop.HfsProps+|Allows for setting Hadoop specific FileSystem properties,
specifically properties around enabling the 'combined input
format' support. Combining inputs minimized the performance
penalty around processing large numbers of small files.

|===============




== Executing

Running a Cascading application is the same as running any Hadoop
application. After packaging your application into a single jar (see
<<building>>), you must use
_bin/hadoop_ to submit the application to the
cluster.

For example, to execute an application stuffed into
_your-application.jar_, call the Hadoop shell
script:

.Running a Cascading Application
===
----
$HADOOP_HOME/bin/hadoop jar your-application.jar [some params]
----
====

If the configuration scripts in $HADOOP_CONF_DIR
are configured to use a cluster, the Jar is pushed into the cluster for
execution.

Cascading does not rely on any environment variables like
$HADOOP_HOME or $HADOOP_CONF_DIR, only
_bin/hadoop_ does.

It should be noted that even though
_your-application.jar_ is passed on the command line
to _bin/hadoop_, this in no way configures Hadoop to
push this jar into the cluster. You must still call one of the property
setters mentioned above to set the proper path to the application jar.
If misconfigured, it's likely that one of the internal libraries (found
in the lib folder) will be pushed to the cluster instead, and "Class Not
Found" exceptions will be thrown.



[[debugging-hadoop]]
== Debugging

Debugging and testing in Cascading local mode, unlike Cascading
Hadoop mode, is trivial as all the work and processing happens in the
local JVM and in local memory. This dramatically simplifies the use of
an IDE and Debugger.Thus the very first recommendation for debugging
Cascading applications on Hadoop is to first write tests that run in
Cascading local mode.

Along with the use of an IDE Debugger, Cascading provides two
tools to help sort out runtime issues. First is the use of the
[classname]+Debug+ filter.

It is a best practice to sprinkle [classname]+Debug+
operators (see <<debug-function>>) in the pipe assembly
and rely on the planner to remove them at runtime by setting a
[classname]+DebugLevel+. [classname]+Debug+ can only
print to the local console via std out or std error, thus making it
harder for use on Hadoop, as Operations do not execute locally but on
the cluster side. [classname]+Debug+ can optionally print the
current field names, and a prefix can be set to help distinguish between
instances of the [classname]+Debug+ operation.

Additionally, the actual execution plan for a given Flow can be
written out (and visualized) via the Flow.writeDOT() method. DOT files
are simply text representation of graph data and can be read by tools
like GraphViz and Omni Graffle.

In Cascading local mode, these execution plans are exactly as the
pipe assemblies were coded, except the sub-assemblies are unwound and
the field names across the Flow are resolved by the local mode planner.
That is, [code]+Fields.ALL+ and other wild cards are converted the
actual field names or ordinals.

In the case of Hadoop mode, using the
[classname]+HadoopFlowConnector+, the DOT files also contain
the intermediate [classname]+Tap+ instances created to join
MapReduce jobs together. Thus the branches between Tap instances are
effectively MapReduce jobs. See the [code]+Flow.writeStepsDOT()+
method to write out all the MapReduce jobs that will be
scheduled.

This information can also be misleading to what is actually
happening per Map or Reduce task cluster side. For a more detailed view
of the data pipeline actually executing on a given Map or Reduce task,
set the "cascading.stream.dotfile.path" property on the
[classname]+FlowConnector+. This will write, cluster side, a
DOT representation of the current data pipeline path the current Map or
Reduce task is handling which is a function of which file(s) the Map or
Reduce task are reading and processing. And if multiple files, which
files are being read to which [classname]+HashJoin+ instances.
It is recommended to use a relative path like
[code]+stepPlan/+.

If the [methodname]+connect()+ method on the current
[classname]+FlowConnector+ fails, the resulting
[classname]+PlannerException+ has a
[methodname]+writeDOT()+ method that shows the progress of
the current planner.

If Cascading is failing with an unknown internal runtime exception
during Map or Reduce task startup, setting the
"cascading.stream.error.dotfile" property will tell Cascading where to
write a DOT representation of the pipeline it was attempting to build,
if any. This file will allow the Cascading community to better identify
and resolve issues.
