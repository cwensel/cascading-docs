
== Data Processing



=== Terminology

The Cascading processing model is based on a metaphor of pipes
(data streams) and filters (data operations). Thus the Cascading API
allows the developer to assemble pipe assemblies that split, merge,
group, or join streams of data while applying operations to each data
record or groups of records.

In Cascading, we call a data record a _tuple_, a simple chain of pipes without forks or
merges a _branch_, an interconnected
set of pipe branches a _pipe assembly_, and a series
of tuples passing through a pipe branch or assembly a _tuple stream_.

Pipe assemblies are specified independently of the data source
they are to process. So before a pipe assembly can be executed, it must
be bound to _taps_, i.e., data sources
and sinks. The result of binding one or more pipe assemblies to taps is
a _flow_, which is executed on a
computer or cluster using the Hadoop framework.

Multiple flows can be grouped together and executed as a single
process. In this context, if one flow depends on the output of another,
it is not executed until all of its data dependencies are satisfied.
Such a collection of flows is called a _cascade_.



=== Pipe Assemblies

Pipe assemblies define what work should be done against tuple
streams, which are read from tap _sources_ and written to tap _sinks_.
The work performed on the data stream may include actions such as
filtering, transforming, organizing, and calculating. Pipe assemblies
may use multiple sources and multiple sinks, and may define splits,
merges, and joins to manipulate the tuple streams.



==== Pipe Assembly Workflow

Pipe assemblies are created by chaining
[classname]+cascading.pipe.Pipe+ classes and subclasses
together. Chaining is accomplished by passing the previous
[classname]+Pipe+ instances to the constructor of the next
[classname]+Pipe+ instance.

The following example demonstrates this type of chaining. It
creates two pipes - a "left-hand side" (lhs) and a "right-hand side"
(rhs) - and performs some processing on them both, using the Each
pipe. Then it joins the two pipes into one, using the CoGroup pipe,
and performs several operations on the joined pipe using Every and
GroupBy. The specific operations performed are not important in the
example; the point is to show the general flow of the data streams.
The diagram after the example gives a visual representation of the
workflow.

.Chaining Pipes
====
include::simple-pipe-assembly.adoc[]
====

The following diagram is a visual representation of the example
above.

image:images/simple-pipe-assembly.svg[align="center"]



==== Common Stream Patterns

As data moves through the pipe, streams may be separated or
combined for various purposes. Here are the three basic
patterns:

Split::
A split takes a single stream and sends it down multiple
paths - that is, it feeds a single [classname]+Pipe+
instance into two or more subsequent separate
[classname]+Pipe+ instances with unique branch
names.


Merge::
A merge combines two or more streams that have identical
fields into a single stream. This is done by passing two or more
[classname]+Pipe+ instances to a
[classname]+Merge+ or [classname]+GroupBy+
pipe.


Join::
A join combines data from two or more streams that have
different fields, based on common field values (analogous to a
SQL join.) This is done by passing two or more
[classname]+Pipe+ instances to a
[classname]+HashJoin+ or
[classname]+CoGroup+ pipe. The code sequence and
diagram above give an example.




==== Data Processing

In addition to directing the tuple streams - using splits,
merges, and joins - pipe assemblies can examine, filter, organize, and
transform the tuple data as the streams move through the pipe
assemblies. To facilitate this, the values in the tuple are typically
given field names, just as database columns are given names, so that
they may be referenced or selected. The following terminology is
used:

Operation::
Operations
([classname]++cascading.operation.Operation++) accept an
input argument Tuple, and output zero or more result tuples.
There are a few sub-types of operations defined below. Cascading
has a number of generic Operations that can be used, or
developers can create their own custom Operations.


Tuple::
In Cascading, data is processed as a stream of Tuples
([classname]++cascading.tuple.Tuple++), which are
composed of fields, much like a database record or row. A Tuple
is effectively an array of (field) values, where each value can
be any [classname]++java.lang.Object++ Java type (or
[code]++byte[]++ array). For information on supporting
non-primitive types, see <<custom-types>>.


Fields::
Fields ([classname]++cascading.tuple.Fields++) are
used either to declare the field names for fields in a Tuple, or
reference field values in a Tuple. They can either be strings
(such as "firstname" or "birthdate"), integers (for the field
position, starting at [code]++0++ for the first position, or
starting at [code]++-1++ for the last position), or one of
the predefined __Fields sets__
(such as [code]++Fields.ALL++, which selects all values in
the Tuple, like an asterisk in SQL). For more on Fields sets,
see <<field-algebra>>).




=== Pipes

The code for the sample pipe assembly above, <<chaining-pipes>>, consists almost entirely of a series of
[classname]+Pipe+ constructors. This section describes the
various [classname]+Pipe+ classes in detail. The base class
[classname]+cascading.pipe.Pipe+ and its subclasses are shown
in the diagram below.

image:images/pipes.svg[align="center"]



==== Types of Pipes

[classname]+Each+::
These pipes perform operations based on the data contents
of tuples - analyze, transform, or filter. The
[classname]+Each+ pipe operates on individual tuples
in the stream, applying functions or filters such as
conditionally replacing certain field values, removing tuples
that have values outside a target range, etc.


+
You can also use [classname]+Each+ to split or
branch a stream, simply by routing the output of an
[classname]+Each+ into a different pipe or
sink.


+
Note that with [classname]+Each+, as with other
types of pipe, you can specify a list of fields to output,
thereby removing unwanted fields from a stream.


[classname]+Merge+::
Just as [classname]+Each+ can be used to split
one stream into two, [classname]+Merge+ can be used to
combine two or more streams into one, as long as they have the
same fields.


+
A [classname]+Merge+ accepts two or more streams
that have identical fields, and emits a single stream of tuples
(in arbitrary order) that contains all the tuples from all the
specified input streams. Thus a Merge is just a mingling of all
the tuples from the input streams, as if shuffling multiple card
decks into one.


+
Use [classname]+Merge+ when no grouping is
required (i.e., no aggregator or buffer operations will be
performed). [classname]+Merge+ is much faster than
[classname]+GroupBy+ (see below) for merging.


+
To combine streams that have different fields, based on
one or more common values, use [classname]+CoGroup+ or
[classname]+HashJoin+.


[classname]+GroupBy+::
[classname]+GroupBy+ groups the tuples of a
stream based on common values in a specified field.


+
If passed multiple streams as inputs, it performs a merge
before the grouping. As with [classname]+Merge+, a
[classname]+GroupBy+ requires that multiple input
streams share the same field structure.


+
The purpose of grouping is typically to prepare a stream
for processing by the [classname]+Every+ pipe, which
performs aggregator and buffer operations on the groups, such as
counting, totalling, or averaging values within that
group.


+
It should be clear that "grouping" here essentially means
sorting all the tuples into groups based on the value of a
particular field. However, within a given group, the tuples are
in arbitrary order unless you specify a secondary sort key. For
most purposes, a secondary sort is not required and only
increases the execution time.


[classname]+Every+::
The [classname]+Every+ pipe operates on a tuple
stream that has been grouped (by [classname]+GroupBy+
or [classname]++CoGroup++) on the values of a particular
field, such as timestamp or zipcode. It's used to apply
aggregator or buffer operations such as counting, totaling, or
averaging field values within each group. Thus the
[classname]+Every+ class is only for use on the output
of [classname]+GroupBy+ or
[classname]+CoGroup+, and cannot be used with the
output of [classname]+Each+,
[classname]+Merge+, or
[classname]+HashJoin+.


+
An [classname]+Every+ instance may follow
another [classname]+Every+ instance, so
[classname]+Aggregator+ operations can be chained.
This is not true for [classname]+Buffer+
operations.


[classname]+CoGroup+::
[classname]+CoGroup+ performs a join on two or
more streams, similar to a SQL join, and groups the single
resulting output stream on the value of a specified field. As
with SQL, the join can be inner, outer, left, or right.
Self-joins are permitted, as well as mixed joins (for three or
more streams) and custom joins. Null fields in the input streams
become corresponding null fields in the output stream.


+
The resulting output stream contains fields from all the
input streams. If the streams contain any field names in common,
they must be renamed to avoid duplicate field names in the
resulting tuples.


[classname]+HashJoin+::
[classname]+HashJoin+ performs a join on two or
more streams, similar to a SQL join, and emits a single stream
in arbitrary order. As with SQL, the join can be inner, outer,
left, or right. Self-joins are permitted, as well as mixed joins
(for three or more streams) and custom joins. Null fields in the
input streams become corresponding null fields in the output
stream.


+
For applications that do not require grouping,
[classname]+HashJoin+ provides faster execution than
[classname]+CoGroup+, but only within certain
prescribed cases. It is optimized for joining one or more small
streams to no more than one large stream. Developers should
thoroughly understand the limitations of this class, as
described below, before attempting to use it.


The following table summarizes the different types of
pipes.

.Comparison of pipe types

|===============
|* _Pipe type_ *|_ *Purpose* _|_ *Input* _|_ *Output* _
|[classname]+Pipe+|instantiate a pipe; create or name a branch|name|a (named) pipe
|[classname]+SubAssembly+|create nested subassemblies||
|[classname]+Each+|apply a filter or function, or branch a stream|tuple stream (grouped or not)|a tuple stream, optionally filtered or
transformed
|[classname]+Merge+|merge two or more streams with identical fields|two or more tuple streams|a tuple stream, unsorted
|[classname]+GroupBy+|sort/group on field values; optionally merge two or
more streams with identical fields|one or more tuple streams with identical fields|a single tuple stream, grouped on key field(s) with
optional secondary sort
|[classname]+Every+|apply aggregator or buffer operation|grouped tuple stream|a tuple stream plus new fields with operation
results
|[classname]+CoGroup+|join 1 or more streams on matching field values|one or more tuple streams|a single tuple stream, joined on key field(s)
|[classname]+HashJoin+|join 1 or more streams on matching field values|one or more tuple streams|a tuple stream in arbitrary order

|===============




[[each-every]]
==== The Each and Every Pipes

The [classname]+Each+ and [classname]+Every+
pipes perform operations on tuple data - for instance, perform a
search-and-replace on tuple contents, filter out some of the tuples
based on their contents, or count the number of tuples in a stream
that share a common field value.

Here is the syntax for these pipes:


----
new Each( previousPipe, argumentSelector, operation, outputSelector )
----




----
new Every( previousPipe, argumentSelector, operation, outputSelector )
----



Both types take four arguments:

* a Pipe instance
* an argument selector
* an Operation instance
*
an output selector on the constructor (selectors here are
Fields instances)







The key difference between [classname]+Each+ and
[classname]+Every+ is that the [classname]+Each+
operates on individual tuples, and [classname]+Every+
operates on groups of tuples emitted by [classname]+GroupBy+
or [classname]+CoGroup+. This affects the kind of operations
that these two pipes can perform, and the kind of output they produce
as a result.

The [classname]+Each+ pipe applies operations that are
subclasses of [classname]+Functions+ and
[classname]+Filters+ (described in the Javadoc). For
example, using [classname]+Each+ you can parse lines from a
logfile into their constituent fields, filter out all lines except the
HTTP GET requests, and replace the timestring fields with date
fields.

Similarly, since the [classname]+Every+ pipe works on
tuple groups (the output of a [classname]+GroupBy+ or
[classname]+CoGroup+ pipe), it applies operations that are
subclasses of [classname]+Aggregators+ and
[classname]+Buffers+. For example, you could use
[classname]+GroupBy+ to group the output of the above
[classname]+Each+ pipe by date, then use an
[classname]+Every+ pipe to count the GET requests per date.
The pipe would then emit the operation results as the date and count
for each group.

image:images/pipe-operation-relationship.svg[align="center"]

In the syntax shown at the start of this section, the _argument selector_ specifies fields from the
input tuple to use as input values. If the argument selector is not
specified, the whole input tuple ([code]++Fields.ALL++) is passed
to the operation as a set of argument values.

Most [classname]+Operation+ subclasses declare result
fields (shown as "declared fields" in the diagram). The _output selector_ specifies the fields of the
output [classname]+Tuple+ from the fields of the input
[classname]+Tuple+ and the operation result. This new output
[classname]+Tuple+ becomes the input
[classname]+Tuple+ to the next pipe in the pipe assembly. If
the output selector is [code]+Fields.ALL+, the output is the
input [classname]+Tuple+ plus the operation result, merged
into a single [classname]+Tuple+.

Note that it's possible for a [classname]+Function+ or
[classname]+Aggregator+ to return more than one output
[classname]+Tuple+ per input [classname]+Tuple+.
In this case, the input tuple is duplicated as many times as necessary
to create the necessary output tuples. This is similar to the
reiteration of values that happens during a join. If a function is
designed to always emit three result tuples for every input tuple,
each of the three outgoing tuples will consist of the selected input
tuple values plus one of the three sets of function result
values.

image:images/each-operation-relationship.svg[align="center"]

If the result selector is not specified for an
[classname]+Each+ pipe performing a
[classname]+Functions+ operation, the operation results are
returned by default ([code]++Fields.RESULTS++), discarding the
input tuple values in the tuple stream. (This is not true of
[classname]++Filters++ , which either discard the input tuple
or return it intact, and thus do not use an output selector.)

image:images/every-operation-relationship.svg[align="center"]

For the [classname]+Every+ pipe, the Aggregator
results are appended to the input Tuple ([code]++Fields.ALL++) by
default.

Note that the [classname]+Every+ pipe associates
[classname]+Aggregator+ results with the current group
[classname]+Tuple+ (the unique keys currently being grouped
on). For example, if you are grouping on the field "department" and
counting the number of "names" grouped by that department, the
resulting output Fields will be ["department","num_employees"].

If you are also adding up the salaries associated with each
"name" in each "department", the output Fields will be
["department","num_employees","total_salaries"].

This is only true for chains of
[classname]+Aggregator+ Operations - you are not allowed to
chain [classname]+Buffer+ operations, as explained
below.

image:images/buffer-operation-relationship.svg[align="center"]

When the [classname]+Every+ pipe is used with a
[classname]+Buffer+ operation, instead of an
[classname]+Aggregator+, the behavior is different. Instead
of being associated with the current grouping tuple, the operation
results are associated with the current values tuple. This is
analogous to how an [classname]+Each+ pipe works with a
[classname]+Function+. This approach may seem slightly
unintuitive, but provides much more flexibility. To put it another
way, the results of the buffer operation are not appended to the
current keys being grouped on. It is up to the buffer to emit them if
they are relevant. It is also possible for a Buffer to emit more than
one result Tuple per unique grouping. That is, a Buffer may or may not
emulate an Aggregator, where an Aggregator is just a special optimized
case of a Buffer.

For more information on how operations process fields, see <<field-processing>> .



==== Merge

The [classname]+Merge+ pipe is very simple. It accepts
two or more streams that have the same fields, and emits a single
stream containing all the tuples from all the input streams. Thus a
merge is just a mingling of all the tuples from the input streams, as
if shuffling multiple card decks into one. Note that the output of
[classname]+Merge+ is in arbitrary order.

.Merging Two Tuple Streams
====
include::simple-merge.adoc[]
====

The example above simply combines all the tuples from two
existing streams ("lhs" and "rhs") into a new tuple stream
("merge").



==== GroupBy

[classname]+GroupBy+ groups the tuples of a stream
based on common values in a specified field. If passed multiple
streams as inputs, it performs a merge before the grouping. As with
[classname]+Merge+, a [classname]+GroupBy+
requires that multiple input streams share the same field
structure.

The output of [classname]+GroupBy+ is suitable for the
[classname]+Every+ pipe, which performs
[classname]+Aggregator+ and [classname]+Buffer+
operations, such as counting, totalling, or averaging groups of tuples
that have a common value (e.g., the same date). By default,
[classname]+GroupBy+ performs no secondary sort, so within
each group the tuples are in arbitrary order. For instance, when
grouping on "lastname", the tuples [code]+[doe, john]+ and
[code]+[doe, jane]+ end up in the same group, but in arbitrary
sequence.



[[secondary-sorting]]
===== Secondary sorting

If multi-level sorting is desired, the names of the sort
fields on must be specified to the [classname]+GroupBy+
instance, as seen below. In this example, [code]+value1+ and
[code]+value2+ will arrive in their natural sort order
(assuming they are
[classname]++java.lang.Comparable++).

.Secondary Sorting
====
include::simple-groupby-secondary.adoc[]
====

If we don't care about the order of [code]+value2+, we
can leave it out of the [code]+sortFields+
[classname]+Fields+ constructor.

In the next example, we reverse the order of
[code]+value1+ while keeping the natural order of
[code]+value2+.

.Reversing Secondary Sort Order
====
include::simple-groupby-secondary-comparator.adoc[]
====

Whenever there is an implied sort during grouping or secondary
sorting, a custom [classname]+java.util.Comparator+ can
optionally be supplied to the grouping [classname]+Fields+
or secondary sort [classname]+Fields+. This allows the
developer to use the [code]+Fields.setComparator()+ call to
control the sort.

To sort or group on non-Java-comparable classes, consider
creating a custom [classname]+Comparator+.

Below is a more practical example, where we group by the "day
of the year", but want to reverse the order of the tuples within
that grouping by "time of day".

.Reverse Order by Time
====
include::simple-groupby-secondary-time.adoc[]
====



==== CoGroup

The [classname]+CoGroup+ pipe is similar to
[classname]+GroupBy+, but instead of a merge, performs a
join. That is, [classname]+CoGroup+ accepts two or more
input streams and groups them on one or more specified keys, and
performs a join operation on equal key values, similar to a SQL
join.

The output stream contains all the fields from all the input
streams.

As with SQL, the join can be inner, outer, left, or right.
Self-joins are permitted, as well as mixed joins (for three or more
streams) and custom joins. Null fields in the input streams become
corresponding null fields in the output stream.

Since the output is grouped, it is suitable for the
[classname]+Every+ pipe, which performs
[classname]+Aggregator+ and [classname]+Buffer+
operations - such as counting, totalling, or averaging groups of
tuples that have a common value (e.g., the same date).

The output stream is sorted by the natural order of the grouping
fields. To control this order, at least the first
[classname]+groupingFields+ value given should be an
instance of [classname]+Fields+ containing
[classname]+Comparator+ instances for the appropriate
fields. This allows fine-grained control of the sort grouping
order.



===== Field names

In a join operation, all the field names used in any of the
input tuples must be unique; duplicate field names are not allowed.
If the names overlap there is a collision, as shown in the following
diagram.

image:images/cogrouping-fields-fail.svg[align="center"]

In this figure, two streams are to be joined on the "url"
field, resulting in a new Tuple that contains fields from the two
input tuples. However, the resulting tuple would include two fields
with the same name ("url"), which is unworkable. To handle the
conflict, developers can use the
[parameter]+declaredFields+ argument (described in the
Javadoc) to declare unique field names for the output tuple, as in
the following example.

.Joining Two Tuple Streams with Duplicate Field Names
====
include::duplicate-cogroup.adoc[]
====

image:images/cogrouping-fields-pass.svg[align="center"]

This revised figure demonstrates the use of declared field
names to prevent a planning failure.

It might seem preferable for Cascading to automatically
recognize the duplication and simply merge the identically-named
fields, saving effort for the developer. However, consider the case
of an outer type join in which one field (or set of fields used for
the join) for a given join side happens to be [code]+null+.
Discarding one of the duplicate fields would lose this
information.

Further, the internal implementation relies on field position,
not field names, when reading tuples; the field names are a device
for the developer. This approach allows the behavior of the
[classname]+CoGroup+ to be deterministic and
predictable.



[[joiner-class]]
===== The Joiner class

In the example above, we explicitly specified a Joiner class
(InnerJoin) to perform a join on our data. There are five Joiner
subclasses, as shown in this diagram.

image:images/joins.svg[align="center"]

In [classname]+CoGroup+, the join is performed after
all the input streams are first co-grouped by their common keys.
Cascading must create a "bag" of data for every grouping in the
input streams, consisting of all the [classname]+Tuple+
instances associated with that grouping.

image:images/cogrouped-values.svg[align="center"]

It's already been mentioned that joins in Cascading are
analogous to joins in SQL. The most commonly-used type of join is
the inner join, the default in [classname]+CoGroup+. An
inner join tries to match _each_
Tuple on the "lhs" with _every_
Tuple on the "rhs", based on matching field values. With an inner
join, if either side has no tuples for a given value, no tuples are
joined. An outer join, conversely, allows for either side to be
empty and simply substitutes a [classname]+Tuple+
containing [code]+null+ values for the non-existent
tuple.

This sample data is used in the discussion below to explain
and compare the different types of join:


----
LHS = [0,a] [1,b] [2,c]
RHS = [0,A] [2,C] [3,D]
----

In each join type below, the values
are joined on the first tuple position (the join key), a numeric
value. Note that, when Cascading joins tuples, the resulting
[classname]+Tuple+ contains all the incoming values from
in incoming tuple streams, and does not discard the duplicate key
fields. As mentioned above, on outer joins where there is no
equivalent key in the alternate stream, [code]+null+ values are
used.

For example using the data above, the result Tuple of an
"inner" join with join key value of [code]+2+ would be
[code]+[2,c,2,C]+. The result Tuple of an "outer" join with
join key value of [code]+1+ would be
[code]+[1,b,null,null]+.

InnerJoin::
An inner join only returns a joined
[classname]+Tuple+ if neither bag for the join key
is empty.
+
----
[0,a,0,A] [2,c,2,C]
----



OuterJoin::
An outer join performs a join if one bag (left or
right) for the join key is empty, or if neither bag is
empty.
----
[0,a,0,A] [1,b,null,null] [2,c,2,C] [null,null,3,D]
----




LeftJoin::
A left join can also be stated as a left inner and
right outer join, where it is acceptable for the right bag
to be empty (but not the left).



+
----
[0,a,0,A] [1,b,null,null] [2,c,2,C]
----

RightJoin::
A right join can also be stated as a left outer and
right inner join, where it is acceptable for the left bag to
be empty (but not the right).
----
[0,a,0,A] [2,c,2,C] [null,null,3,D]
----




MixedJoin::
A mixed join is where 3 or more tuple streams are
joined, using a small Boolean array to specify each of the
join types to use. For more information, see the
[classname]+cascading.pipe.cogroup.MixedJoin+
class in the Javadoc.


_Custom_::
Developers can subclass the
[classname]+cascading.pipe.cogroup.Joiner+ class
to create custom join operations.






===== Scaling

[classname]+CoGroup+ attempts to store the entire
current unique keys tuple "bag" from the right-hand stream in memory
for rapid joining to the left-hand stream. If the bag is very large,
it may exceed a configurable threshold and be spilled to disk,
reducing performance and potentially causing a memory error (if the
threshold value is too large). Thus it's usually best to put the
stream with the largest groupings on the left-hand side and, if
necessary, adjust the spill threshold as described in the
Javadoc.



==== HashJoin

[classname]+HashJoin+ performs a join (similar to a
SQL join) on two or more streams, and emits a stream of tuples that
contain fields from all of the input streams. With a join, the tuples
in the different input streams do not typically contain the same set
of fields.

As with [classname]+ CoGroup+, the field names must
all be unique, including the names of the key fields, to avoid
duplicate field names in the emitted [classname]+Tuple+. If
necessary, use the [parameter]+declaredFields+ argument to
specify unique field names for the output.

An inner join is performed by default, but you can choose inner,
outer, left, right, or mixed (three or more streams). Self-joins are
permitted. Developers can also create custom Joiners if desired. For
more information on types of joins, refer to <<joiner-class>> or the Javadoc.

.Joining Two Tuple Streams
====
include::simple-join.adoc[]
====

The example above performs an inner join on two streams ("lhs"
and "rhs"), based on common values in two fields. The field names that
are specified in [classname]+lhsFields+ and
[classname]+rhsFields+ are among the field names previously
declared for the two input streams.



===== Scaling

For joins that do not require grouping,
[classname]+HashJoin+ provides faster execution than
[classname]+CoGroup+, but it operates within stricter
limitations. It is optimized for joining one or more small streams
to no more than one large stream.

Unlike [classname]+CoGroup+,
[classname]+HashJoin+ attempts to keep the entire
right-hand stream in memory for rapid comparison (not just the
current grouping, as no grouping is performed for a
[classname]++HashJoin++). Thus a very large tuple stream in
the right-hand stream may exceed a configurable spill-to-disk
threshold, reducing performance and potentially causing a memory
error. For this reason, it's advisable to use the smaller stream on
the right-hand side. Additionally, it may be helpful to adjust the
spill threshold as described in the Javadoc.

Due to the potential difficulties of using
[classname]+HashJoin+ (as compared to the slower but much
more reliable [classname]++CoGroup++), developers should
thoroughly understand this class before attempting to use it.

Frequently the [classname]+HashJoin+ is fed a
filtered down stream of Tuples from what was originally a very large
file. To prevent the large file from being replicated throughout a
cluster, when running in Hadoop mode, use a
[classname]+Checkpoint+ pipe at the point where the data
has been filtered down to its smallest before it is streamed into a
[classname]+HashJoin+. This will force the Tuple stream to
be persisted to disk and new [classname]+FlowStep+
(MapReduce job) to be created to read the smaller data size more
efficiently.



==== Setting Custom Pipe Properties

By default, the properties passed to a FlowConnector subclass
become the defaults for every Flow instance created by that
FlowConnector. In the past, if some of the Flow instances needed
different properties, it was necessary to create additional
FlowConnectors to set those properties. However, it is now possible to
set properties at the Pipe scope and at the process FlowStep
scope.

Setting properties at the Pipe scope lets you set a property
that is only visible to a given Pipe instance (and its child
Operation). This allows Operations such as custom Functions to be
dynamically configured.

More importantly, setting properties at the process FlowStep
scope allows you to set properties on a Pipe that are inherited by the
underlying process during runtime. When running on the Apache Hadoop
platform (i.e., when using the HadoopFlowConnector), a FlowStep is the
current MapReduce job. Thus a Hadoop-specific property can be set on a
Pipe, such as a CoGroup. During runtime, the FlowStep (MapReduce job)
that the CoGroup executes in is configured with the given property -
for example, a spill threshold, or the number of reducer tasks for
Hadoop to deploy.

The following code samples demonstrates the basic form for both
the Pipe scope and the process FlowStep scope.

.Pipe Scope
====
include::properties-pipe.adoc[]
====

.Step Scope
====
include::properties-step.adoc[]
====

As of Cascading 2.2, SubAssemblies can now be configured via the
ConfigDef method.



[[platforms]]
=== Platforms

Cascading supports pluggable planners that
allow it to execute on differing platforms. Planners are invoked by an
associated [classname]+FlowConnector+ subclass. Currently,
only two planners are provided, as described below:

LocalFlowConnector::
The
[classname]+cascading.flow.local.LocalFlowConnector+
provides a "local" mode planner for running Cascading completely
in memory on the current computer. This allows for fast
execution of Flows against local files or any other compatible
custom [classname]+Tap+ and
[classname]+Scheme+ classes.


+
The local mode planner and platform were not designed to
scale beyond available memory, CPU, or disk on the current
machine. Thus any memory-intensive processes that use
[classname]+GroupBy+, [classname]+CoGroup+,
or [classname]+HashJoin+ are likely to fail against
moderately large files.


+
Local mode is useful for development, testing, and
interactive data exploration against sample sets.


HadoopFlowConnector::
The
[classname]+cascading.flow.hadoop.HadoopFlowConnector+
provides a planner for running Cascading on an Apache Hadoop 1
cluster. This allows Cascading to execute against extremely
large data sets over a cluster of computing nodes.


Hadoop2MR1FlowConnector::
The
[classname]+cascading.flow.hadoop2.Hadoop2MR1FlowConnector+
provides a planner for running Cascading on an Apache Hadoop 2
cluster. This class is roughly equivalent to the above
[classname]+HadoopFlowConnector+ except it uses Hadoop
2 specific properties and is compiled against Hadoop 2 API
binaries.




Cascading's support for pluggable planners allows a
pipe assembly to be executed on an arbitrary platform, using
platform-specific Tap and Scheme classes that hide the platform-related
I/O details from the developer. For example, Hadoop uses
[classname]+org.apache.hadoop.mapred.InputFormat+ to read
data, but local mode is happy with a
[classname]+java.io.FileInputStream+. This detail is hidden
from developers unless they are creating custom Tap and Scheme
classes.



[[source-sink]]
=== Source and Sink Taps

All input data comes in from, and all output data goes out to,
some instance of [classname]+cascading.tap.Tap+. A tap
represents a data resource - such as a file on the local file system, on
a Hadoop distributed file system, or on Amazon S3. A tap can be read
from, which makes it a _source_, or
written to, which makes it a _sink_.
Or, more commonly, taps act as both sinks and sources when shared
between flows.

The platform on which your application is running (Cascading local
or Hadoop) determines which specific classes you can use. Details are
provided in the sections below.



==== Schemes

If the Tap is about where the data is and how to access it, the
Scheme is about what the data is and how to read it. Every Tap must
have a Scheme that describes the data. Cascading provides four Scheme
classes:

TextLine::
[classname]+TextLine+ reads and writes raw text
files and returns tuples which, by default, contain two fields
specific to the platform used. The first field is either the
byte offset or line number, and the second field is the actual
line of text. When written to, all Tuple values are converted to
Strings delimited with the TAB character (\t). A TextLine scheme
is provided for both the local and Hadoop modes.


+
By default TextLine uses the UTF-8 character set. This can
be overridden on the appropriate TextLine constructor.


TextDelimited::
[classname]+TextDelimited+ reads and writes
character-delimited files in standard formats such as CSV
(comma-separated variables), TSV (tab-separated variables), and
so on. When written to, all Tuple values are converted to
Strings and joined with the specified character delimiter. This
Scheme can optionally handle quoted values with custom quote
characters. Further, TextDelimited can coerce each value to a
primitive type when reading a text file. A TextDelimited scheme
is provided for both the local and Hadoop modes.


+
By default TextDelimited uses the UTF-8 character set.
This can be overridden on appropriate the TextDelimited
constructor.


SequenceFile::
[classname]+SequenceFile+ is based on the Hadoop
Sequence file, which is a binary format. When written to or read
from, all Tuple values are saved in their native binary form.
This is the most efficient file format - but be aware that the
resulting files are binary and can only be read by Hadoop
applications running on the Hadoop platform.


WritableSequenceFile::
Like the [classname]+SequenceFile+ Scheme,
[classname]+WritableSequenceFile+ is based on the
Hadoop Sequence file, but it was designed to read and write key
and/or value Hadoop [classname]+Writable+ objects
directly. This is very useful if you have sequence files created
by other applications. During writing (sinking), specified key
and/or value fields are serialized directly into the sequence
file. During reading (sourcing), the key and/or value objects
are deserialized and wrapped in a Cascading Tuple object and
passed to the downstream pipe assembly. This class is only
available when running on the Hadoop platform.


There's a key difference between the
[classname]+TextLine+ and
[classname]+SequenceFile+ schemes. With the
[classname]+SequenceFile+ scheme, data is stored as binary
tuples, which can be read without having to be parsed. But with the
[classname]+TextLine+ option, Cascading must parse each line
into a [classname]+Tuple+ before processing it, causing a
performance hit.



===== Platform-specific implementation details

Depending on which platform you use (Cascading local or
Hadoop), the classes you use to specify schemes will vary.
Platform-specific details for each standard scheme are shown
below.

.Platform-specific tap scheme classes

|===============
|*Description*|*Cascading local platform*|*Hadoop platform*
|*Package Name*|[classname]+cascading.scheme.local+|[classname]+cascading.scheme.hadoop+
|Read lines of text|[classname]+TextLine+|[classname]+TextLine+
|Read delimited text (CSV, TSV, etc)|[classname]+TextDelimited+|[classname]+TextDelimited+
|Cascading proprietary efficient binary||[classname]+SequenceFile+
|External Hadoop application binary (custom
[classname]+Writable+ type)||[classname]+WritableSequenceFile+

|===============




===== Sequence File Compression

For best performance when running on the Hadoop platform,
enable Sequence File Compression in the Hadoop property settings -
either block or record-based compression. Refer to the Hadoop
documentation for the available properties and compression
types.



==== Taps

The following sample code creates a new Hadoop FileSystem Tap
that can read and write raw text files. Since only one field name is
provided, the "offset" field is discarded, resulting in an input tuple
stream with only "line" values.

.Creating a new tap
====
include::simple-tap.adoc[]
====

Here are the most commonly-used tap types:

FileTap::
The [classname]+cascading.tap.local.FileTap+ tap
is used with the Cascading local platform to access files on the
local file system.


Hfs::
The [classname]+cascading.tap.hadoop.Hfs+ tap
uses the current Hadoop default file system, when running on the
Hadoop platform.


+
If Hadoop is configured for "Hadoop local mode" (not to be
confused with Cascading local mode), its default file system is
the local file system. If configured for distributed mode, its
default file system is typically the Hadoop distributed file
system.


+
Note that Hadoop can be forced to use an external file
system by specifying a prefix to the URL passed into a new Hfs
tap. For instance, using "s3://somebucket/path" tells Hadoop to
use the S3 [classname]+FileSystem+ implementation to
access files in an Amazon S3 bucket. More information on this
can be found in the Javadoc.


Also provided are six utility taps:

MultiSourceTap::
The [classname]+cascading.tap.MultiSourceTap+ is
used to tie multiple tap instances into a single tap for use as
an input source. The only restriction is that all the tap
instances passed to a new MultiSourceTap share the same Scheme
classes (not necessarily the same Scheme instance).


MultiSinkTap::
The [classname]+cascading.tap.MultiSinkTap+ is
used to tie multiple tap instances into a single tap for use as
output sinks. At runtime, for every Tuple output by the pipe
assembly, each child tap to the MultiSinkTap will sink the
Tuple.


PartitionTap::
The
[classname]+cascading.tap.hadoop.PartitionTap+ and
[classname]+cascading.tap.local.PartitionTap+ are used
to sink tuples into directory paths based on the values in the
Tuple. More can be read below in <<partition-tap>>. Note the
[classname]+TemplateTap+ has been deprecated in favor
of the [classname]+PartitionTap+.


GlobHfs::
The [classname]+cascading.tap.hadoop.GlobHfs+
tap accepts Hadoop style "file globbing" expression patterns.
This allows for multiple paths to be used as a single source,
where all paths match the given pattern. This tap is only
available when running on the Hadoop platform.


DecoratorTap::
The [classname]+cascading.tap.DecoratorTap+ is a
utility helper for wrapping an existing Tap with new
functionality, via sub-class, and/or adding 'meta-data' to a Tap
instance via the generic [classname]+MetaInfo+
instance field. Further, on the Hadoop platform, planner created
intermediate and [classname]+Checkpoint+ Taps can be
wrapped by a [classname]+DecoratorTap+ implementation
by the Cascading Planner. See
[classname]+cascading.flow.FlowConnectorProps+ for
details.


DistCacheTap::
The
[classname]+cascading.tap.hadoop.DistCacheTap+ is a
sub-class of the
[classname]+cascading.tap.DecoratorTap+ that can wrap
an cascading.tap.hadoop.Hfs instance. It allows for writing to
HDFS, but reading from the Hadoop Distributed Cache under the
write circumstances, specifically if the Tap is being read into
the small side of a
[classname]+cascading.pipe.HashJoin+.




===== Platform-specific implementation details

Depending on which platform you use (Cascading local or
Hadoop), the classes you use to specify file systems will vary.
Platform-specific details for each standard tap type are shown
below.

.Platform-specific details for setting file system

|===============
|*Description*|*Either platform*|*Cascading local platform*|*Hadoop platform*
|*Package Name*|[classname]+cascading.tap+|[classname]+cascading.tap.local+|[classname]+cascading.tap.hadoop+
|File access||[classname]+FileTap+|[classname]+Hfs+
|Multiple Taps as single source|[classname]+MultiSourceTap+||
|Multiple Taps as single sink|[classname]+MultiSinkTap+||
|Bin/Partition data into multiple files||[classname]+PartitionTap+|[classname]+PartitionTap+
|Pattern match multiple files/dirs|||[classname]+GlobHfs+
|Wrapping a Tap with MetaData / Decorating intra-Flow
Taps|[classname]+DecoratorTap+||
|Reading from the Hadoop Distributed Cache|||[classname]+DistCacheTap+

|===============




=== Sink modes

.Overwriting An Existing Resource
====
include::simple-replace-tap.adoc[]
====



All applications created with Cascading read data from one or more
sources, process it, then write data to one or more sinks. This is done
via the various [classname]+Tap+ classes, where each class
abstracts different types of back-end systems that store data as files,
tables, blobs, and so on. But in order to sink data, some systems
require that the resource (e.g., a file) not exist before processing
thus must be removed (deleted) before the processing can begin. Other
systems may allow for appending or updating of a resource (typical with
database tables).

When creating a new [classname]+Tap+ instance, a
[classname]+SinkMode+ may be provided so that the Tap will
know how to handle any existing resources. Note that not all Taps
support all [classname]+SinkMode+ values - for example, Hadoop
does not support appends (updates) from a MapReduce job.

The available SinkModes are:

[classname]+SinkMode.KEEP+::
This is the default behavior. If the resource exists,
attempting to write over it will fail.


[classname]+SinkMode.REPLACE+::
This allows Cascading to delete the file immediately after
the Flow is started.


[classname]+SinkMode.UPDATE+::
Allows for new tap types that can update or append - for
example, to update or add records in a database. Each tap may
implement this functionality in its own way. Cascading recognizes
this update mode, and if a resource exists, will not fail or
attempt to delete it.


Note that Cascading itself only uses
these labels internally to know when to automatically call
[methodname]+deleteResource()+ on the
[classname]+Tap+ or to leave the Tap alone. It is up the the
[classname]+Tap+ implementation to actually perform a write or
update when processing starts. Thus, when
[methodname]+start()+ or [methodname]+complete()+
is called on a [classname]+Flow+, any sink
[classname]+Tap+ labeled
[classname]+SinkMode.REPLACE+ will have its
[methodname]+deleteResource()+ method called.

Conversely, if a
[classname]+Flow+ is in a [classname]+Cascade+ and
the [classname]+Tap+ is set to
[classname]+SinkMode.KEEP+ or
[classname]+SinkMode.REPLACE+,
[methodname]+deleteResource()+ will be called if and only if
the sink is stale (i.e., older than the source). This allows a
[classname]+Cascade+ to behave like a "make" or "ant" build
file, only running Flows that should be run. For more information, see
<<skipping-flows>>.

It's also important to understand how Hadoop deals with
directories. By default, Hadoop cannot source data from directories with
nested sub-directories, and it cannot write to directories that already
exist. However, the good news is that you can simply point the
[classname]+Hfs+ tap to a directory of data files, and they
are all used as input - there's no need to enumerate each individual
file into a [classname]+MultiSourceTap+. If there are nested
directories, use [classname]+GlobHfs+.



[[field-algebra]]
=== Fields Sets

Cascading applications can perform complex manipulation or "field
algebra" on the fields stored in tuples, using _Fields sets_, a feature of the
[classname]+Fields+ class that provides a sort of wildcard
tool for referencing sets of field values.

These predefined Fields sets are constant values on the
[classname]+Fields+ class. They can be used in many places
where the [classname]+Fields+ class is expected. They are:
Fields.ALL::
include::algebra-all.adoc[]
The [classname]+cascading.tuple.Fields.ALL+
constant is a wildcard that represents all the current available
fields.



Fields.RESULTS::
include::algebra-results.adoc[]
The [classname]+cascading.tuple.Fields.RESULTS+
constant is used to represent the field names of the current
operations return values. This Fields set may only be used as an
output selector on a pipe, causing the pipe to output a tuple
containing the operation results.



Fields.REPLACE::
include::algebra-replace.adoc[]
The [classname]+cascading.tuple.Fields.REPLACE+
constant is used as an output selector to inline-replace values
in the incoming tuple with the results of an operation. This
convenient Fields set allows operations to overwrite the value
stored in the specified field. The current operation must either
specify the identical argument selector field names used by the
pipe, or use the [classname]+ARGS+ Fields set.



Fields.SWAP::
include::algebra-swap.adoc[]
The [classname]+cascading.tuple.Fields.SWAP+
constant is used as an output selector to swap the operation
arguments with its results. Neither the argument and result
field names, nor the size, need to be the same. This is useful
for when the operation arguments are no longer necessary and the
result Fields and values should be appended to the remainder of
the input field names and Tuple.



Fields.ARGS::
include::algebra-replace.adoc[]
The [classname]+cascading.tuple.Fields.ARGS+
constant is used to let a given operation inherit the field
names of its argument Tuple. This Fields set is a convenience
and is typically used when the Pipe output selector is
[classname]+RESULTS+ or
[classname]+REPLACE+. It is specifically used by the
Identity Function when coercing values from Strings to primitive
types.



Fields.GROUP::
include::algebra-group.adoc[]
The [classname]+cascading.tuple.Fields.GROUP+
constant represents all the fields used as grouping key in the
most recent grouping. If no previous grouping exists in the pipe
assembly, [classname]+GROUP+ represents all the
current field names.



Fields.VALUES::
include::algebra-values.adoc[]
The [classname]+cascading.tuple.Fields.VALUES+
constant represents all the fields not used as grouping fields
in a previous Group. That is, if you have fields "a", "b", and
"c", and group on "a", [classname]+Fields.VALUES+ will
resolve to "b" and "c".



Fields.UNKNOWN::
include::algebra-unknown.adoc[]
The [classname]+cascading.tuple.Fields.UNKNOWN+
constant is used when Fields must be declared, but it's not
known how many fields or what their names are. This allows for
processing tuples of arbitrary length from an input source or
some operation. Use this Fields set with caution.



Fields.NONE::
include::algebra-none.adoc[]
The [classname]+cascading.tuple.Fields.NONE+
constant is used to specify no fields. Typically used as an
argument selector for Operations that do not process any Tuples,
like [classname]+cascading.operation.Insert+.



 The chart below shows common ways to merge input and
result fields for the desired output fields. A few minutes with this
chart may help clarify the discussion of fields, tuples, and pipes. Also
see <<each-every>> for details on the different columns
and their relationships to the [classname]+Each+ and
[classname]+Every+ pipes and Functions, Aggregators, and
Buffers.

image:images/field-algebra.svg[align="center"]



[[flows]]
=== Flows

When pipe assemblies are bound to source and sink taps, a
[classname]+Flow+ is created. Flows are executable in the
sense that, once they are created, they can be started and will execute
on the specified platform. If the Hadoop platform is specified, the Flow
will execute on a Hadoop cluster.

A Flow is essentially a data processing pipeline that reads data
from sources, processes the data as defined by the pipe assembly, and
writes data to the sinks. Input source data does not need to exist at
the time the Flow is created, but it must exist by the time the Flow is
executed (unless it is executed as part of a Cascade - see <<cascades>> for more on this).

The most common pattern is to create a Flow from an existing pipe
assembly. But there are cases where a MapReduce job (if running on
Hadoop) has already been created, and it makes sense to encapsulate it
in a Flow class so that it may participate in a
[classname]+Cascade+ and be scheduled with other
[classname]+Flow+ instances. Alternatively, via the <<,Riffle >>
annotations, third-party applications can participate in a
[classname]+Cascade+, and complex algorithms that result in
iterative Flow executions can be encapsulated as a single Flow. All
patterns are covered here.



==== Creating Flows from Pipe Assemblies

.Creating a new Flow
====
include::simple-flow.adoc[]
====

To create a Flow, it must be planned though one of the
FlowConnector subclass objects. In Cascading, each platform (i.e.,
local and Hadoop) has its own connectors. The [code]+connect()+
method is used to create new Flow instances based on a set of sink
taps, source taps, and a pipe assembly. Above is a trivial example
that uses the Hadoop mode connector.

.Binding taps in a Flow
====
include::complex-flow.adoc[]
====

The example above expands on our previous pipe assembly example
by creating multiple source and sink taps and planning a Flow. Note
there are two branches in the pipe assembly - one named "lhs" and the
other named "rhs". Internally Cascading uses those names to bind the
source taps to the pipe assembly. New in 2.0, a FlowDef can be created
to manage the names and taps that must be passed to a
FlowConnector.



[[configuring-flows]]
==== Configuring Flows

The FlowConnector constructor accepts the
[classname]+java.util.Property+ object so that default
Cascading and any platform-specific properties can be passed down
through the planner to the platform at runtime. In the case of Hadoop,
any relevant Hadoop [code]+*-default.adoc+ properties may be
added. For instance, it's very common to add
[code]+mapred.map.tasks.speculative.execution+,
[code]+mapred.reduce.tasks.speculative.execution+, or
[code]+mapred.child.java.opts+.

One of the two properties that must always be set for production
applications is the application Jar class or Jar path.

.Configuring the Application Jar
====
include::flow-properties.adoc[]
====

More information on packaging production applications can be
found in <<executing-processes>>.

Since the [classname]+FlowConnector+ can be reused,
any properties passed on the constructor will be handed to all the
Flows it is used to create. If Flows need to be created with different
default properties, a new FlowConnector will need to be instantiated
with those properties, or properties will need to be set on a given
[classname]+Pipe+ or [classname]+Tap+ instance
directly - via the [methodname]+getConfigDef()+ or
[methodname]+getStepConfigDef()+ methods.



[[skipping-flows]]
==== Skipping Flows

When a [classname]+Flow+ participates in a
[classname]+Cascade+, the
[classname]+Flow.isSkipFlow()+ method is consulted before
calling [classname]+Flow.start()+ on the flow. The result is
based on the Flow's _skip strategy_.
By default, [methodname]+isSkipFlow()+ returns true if any
of the sinks are stale - i.e., the sinks don't exist or the resources
are older than the sources. However, the strategy can be changed via
the [classname]+Flow.setFlowSkipStrategy()+ and
[classname]+Cascade.setFlowSkipStrategy()+ method, which can
be called before or after a particular [classname]+Flow+
instance has been created.

Cascading provides a choice of two standard skip
strategies:

FlowSkipIfSinkNotStale::
This strategy -
[classname]+cascading.flow.FlowSkipIfSinkNotStale+ -
is the default. Sinks are treated as stale if they don't exist
or the sink resources are older than the sources. If the
SinkMode for the sink tap is REPLACE, then the tap is treated as
stale.


FlowSkipIfSinkExists::
The
[classname]+cascading.flow.FlowSkipIfSinkExists+
strategy skips the Flow if the sink tap exists, regardless of
age. If the [classname]+SinkMode+ for the sink tap is
[code]+REPLACE+, then the tap is treated as stale.


Additionally, you can implement custom skip strategies by using
the interface
[classname]+cascading.flow.FlowSkipStrategy+.

Note that [classname]+Flow.start()+ does not consult
the [methodname]+isSkipFlow()+ method, and consequently
always tries to start the Flow if called. It is up to the user code to
call [classname]+isSkipFlow()+ to determine whether the
current strategy indicates that the Flow should be skipped.



==== Creating Flows from a JobConf

If a MapReduce job already exists and needs to be managed by a
Cascade, then the
[classname]+cascading.flow.hadoop.MapReduceFlow+ class
should be used. To do this, after creating a Hadoop
[classname]+JobConf+ instance simply pass it into the
[classname]+MapReduceFlow+ constructor. The resulting
[classname]+Flow+ instance can be used like any other
Flow.



==== Creating Custom Flows

Any custom Class can be treated as a Flow if given the correct
<<,Riffle>>
annotations. Riffle is a set of Java annotations that identify
specific methods on a class as providing specific life-cycle and
dependency functionality. For more information, see the Riffle
documentation and examples. To use with Cascading, a Riffle-annotated
instance must be passed to the
[classname]+cascading.flow.hadoop.ProcessFlow+ constructor
method. The resulting [classname]+ProcessFlow+ instance can
be used like any other Flow instance.

Since many algorithms need to perform multiple passes over a
given data set, a Riffle-annotated Class can be written that
internally creates Cascading Flows and executes them until no more
passes are needed. This is like nesting Flows or Cascades in a parent
Flow, which in turn can participate in a Cascade.



[[cascades]]
=== Cascades

image:images/cascade.svg[align="center"]

A Cascade allows multiple Flow instances to be executed as a
single logical unit. If there are dependencies between the Flows, they
are executed in the correct order. Further, Cascades act like Ant builds
or Unix make files - that is, a Cascade only executes Flows that have
stale sinks (i.e., output data that is older than the input data). For
more on this, see <<skipping-flows>>.

.Creating a new Cascade
====
include::simple-cascade.adoc[]
====

When passing Flows to the CascadeConnector, order is not
important. The CascadeConnector automatically identifies the
dependencies between the given Flows and creates a scheduler that starts
each Flow as its data sources become available. If two or more Flow
instances have no interdependencies, they are submitted together so that
they can execute in parallel.

For more information, see the section on <<cascade-scheduler>>.

If an instance of
[classname]+cascading.flow.FlowSkipStrategy+ is given to a
[classname]+Cascade+ instance (via the
[classname]+Cascade.setFlowSkipStrategy()+ method), it is
consulted for every Flow instance managed by that Cascade, and all skip
strategies on those Flow instances are ignored. For more information on
skip strategies, see <<skipping-flows>>.

