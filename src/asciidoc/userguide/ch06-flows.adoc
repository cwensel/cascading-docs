:toc2:
:doctitle: {_doctitle} - Flows

= Flows

== Creating Flows from Pipe Assemblies

.Creating a new Flow
====
include::simple-flow.adoc[]
====

To create a Flow, it must be planned though one of the
FlowConnector subclass objects. In Cascading, each platform (i.e.,
local and Hadoop) has its own connectors. The [code]+connect()+
method is used to create new Flow instances based on a set of sink
taps, source taps, and a pipe assembly. Above is a trivial example
that uses the Hadoop mode connector.

.Binding taps in a Flow
====
include::complex-flow.adoc[]
====

The example above expands on our previous pipe assembly example
by creating multiple source and sink taps and planning a Flow. Note
there are two branches in the pipe assembly - one named "lhs" and the
other named "rhs". Internally Cascading uses those names to bind the
source taps to the pipe assembly. New in 2.0, a FlowDef can be created
to manage the names and taps that must be passed to a
FlowConnector.



[[configuring-flows]]
== Configuring Flows

The FlowConnector constructor accepts the
[classname]+java.util.Property+ object so that default
Cascading and any platform-specific properties can be passed down
through the planner to the platform at runtime. In the case of Hadoop,
any relevant Hadoop [code]+*-default.adoc+ properties may be
added. For instance, it's very common to add
[code]+mapred.map.tasks.speculative.execution+,
[code]+mapred.reduce.tasks.speculative.execution+, or
[code]+mapred.child.java.opts+.

One of the two properties that must always be set for production
applications is the application Jar class or Jar path.

.Configuring the Application Jar
====
include::flow-properties.adoc[]
====

More information on packaging production applications can be
found in <<executing-processes>>.

Since the [classname]+FlowConnector+ can be reused,
any properties passed on the constructor will be handed to all the
Flows it is used to create. If Flows need to be created with different
default properties, a new FlowConnector will need to be instantiated
with those properties, or properties will need to be set on a given
[classname]+Pipe+ or [classname]+Tap+ instance
directly - via the [methodname]+getConfigDef()+ or
[methodname]+getStepConfigDef()+ methods.



[[skipping-flows]]
== Skipping Flows

When a [classname]+Flow+ participates in a
[classname]+Cascade+, the
[classname]+Flow.isSkipFlow()+ method is consulted before
calling [classname]+Flow.start()+ on the flow. The result is
based on the Flow's _skip strategy_.
By default, [methodname]+isSkipFlow()+ returns true if any
of the sinks are stale - i.e., the sinks don't exist or the resources
are older than the sources. However, the strategy can be changed via
the [classname]+Flow.setFlowSkipStrategy()+ and
[classname]+Cascade.setFlowSkipStrategy()+ method, which can
be called before or after a particular [classname]+Flow+
instance has been created.

Cascading provides a choice of two standard skip
strategies:

FlowSkipIfSinkNotStale::
This strategy -
[classname]+cascading.flow.FlowSkipIfSinkNotStale+ -
is the default. Sinks are treated as stale if they don't exist
or the sink resources are older than the sources. If the
SinkMode for the sink tap is REPLACE, then the tap is treated as
stale.


FlowSkipIfSinkExists::
The
[classname]+cascading.flow.FlowSkipIfSinkExists+
strategy skips the Flow if the sink tap exists, regardless of
age. If the [classname]+SinkMode+ for the sink tap is
[code]+REPLACE+, then the tap is treated as stale.


Additionally, you can implement custom skip strategies by using
the interface
[classname]+cascading.flow.FlowSkipStrategy+.

Note that [classname]+Flow.start()+ does not consult
the [methodname]+isSkipFlow()+ method, and consequently
always tries to start the Flow if called. It is up to the user code to
call [classname]+isSkipFlow()+ to determine whether the
current strategy indicates that the Flow should be skipped.



== Creating Flows from a JobConf

If a MapReduce job already exists and needs to be managed by a
Cascade, then the
[classname]+cascading.flow.hadoop.MapReduceFlow+ class
should be used. To do this, after creating a Hadoop
[classname]+JobConf+ instance simply pass it into the
[classname]+MapReduceFlow+ constructor. The resulting
[classname]+Flow+ instance can be used like any other
Flow.



== Creating Custom Flows

Any custom Class can be treated as a Flow if given the correct
<<,Riffle>>
annotations. Riffle is a set of Java annotations that identify
specific methods on a class as providing specific life-cycle and
dependency functionality. For more information, see the Riffle
documentation and examples. To use with Cascading, a Riffle-annotated
instance must be passed to the
[classname]+cascading.flow.hadoop.ProcessFlow+ constructor
method. The resulting [classname]+ProcessFlow+ instance can
be used like any other Flow instance.

Since many algorithms need to perform multiple passes over a
given data set, a Riffle-annotated Class can be written that
internally creates Cascading Flows and executes them until no more
passes are needed. This is like nesting Flows or Cascades in a parent
Flow, which in turn can participate in a Cascade.



