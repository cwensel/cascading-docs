:toc2:
:doctitle: {_doctitle} - Flows

== Flows

=== Creating Flows from Pipe Assemblies

.Creating a new Flow
====
include::simple-flow.adoc[]
====

To create a [classname]+Flow+, it must be planned though one of the
[classname]+FlowConnector+ subclass objects. In Cascading, each platform (i.e.,
local and Hadoop) has its own connectors. The [code]+connect()+ method is used
to create new flow instances based on a set of sink taps, source taps, and a
pipe assembly. Above is a trivial example that uses the local-mode connector.

.Binding taps in a Flow
====
include::complex-flow.adoc[]

<1>The [classname]+FlowDef+ class is a fluent API for passing required and
optional metadata to the [classname]+FlowConnector+.

====

The example above expands on our previous pipe assembly example by creating
multiple source and sink taps and planning a [classname]+Flow+. Note there are
two branches in the pipe assembly -- one named "lhs" and the other named "rhs."
Internally Cascading uses those names to bind the source taps to the pipe
assembly.

[[configuring-flows]]
=== Configuring Flows

The FlowConnector constructor accepts the [classname]+java.util.Property+ object
so that default Cascading and any platform-specific properties can be passed
down through the planner to the platform at runtime.

NOTE: In the case of Hadoop, any relevant Hadoop configuration properties may be
added. For instance, it's very common to add [code]+mapreduce.job.reduces+ to
set the number of reducers.

When running on a cluster, one of the two properties that must always be set for
production applications is the application JAR class or JAR path.

.Configuring the application JAR
====
include::flow-properties.adoc[]
====

Since the [classname]+FlowConnector+ can be reused, any properties passed on the
constructor are handed to all the flows it is used to create. If flows need to
be created with different default properties, a new FlowConnector must be
instantiated with those properties. Alternatively, properties can be set on a
given [classname]+Pipe+ or [classname]+Tap+ instance directly -- with the
[methodname]+getConfigDef()+, [methodname]+getNodeConfigDef()+ or
[methodname]+getStepConfigDef()+ methods.

[[skipping-flows]]
=== Skipping Flows

When a [classname]+Flow+ participates in a [classname]+Cascade+, the
[classname]+Flow.isSkipFlow()+ method is checked before calling
[classname]+Flow.start()+ on the [classname]+Flow+. The result is based on the
_skip strategy_ of the [classname]+Flow+.

By default, [methodname]+isSkipFlow()+ returns true if any of the sinks are
_stale_ -- i.e., the sinks do not exist or the resources are older than the
sources. However, the strategy can be changed with either the
[classname]+Flow.setFlowSkipStrategy()+ method or the
[classname]+Cascade.setFlowSkipStrategy()+ method.

Cascading provides a choice of two standard skip strategies:

FlowSkipIfSinkNotStale::

This strategy -- [classname]+cascading.flow.FlowSkipIfSinkNotStale+ -- is the
default. Sinks are treated as _stale_ if they do not exist or the sink resources
are older than the sources. If the SinkMode for the sink tap is REPLACE, then
the tap is treated as stale.

FlowSkipIfSinkExists::

The [classname]+cascading.flow.FlowSkipIfSinkExists+ strategy skips the
[classname]+Flow+ if the sink tap exists, regardless of age. If the
[classname]+SinkMode+ for the sink tap is [code]+REPLACE+, then the tap is
treated as stale.

Additionally, you can implement custom skip strategies by implementing the
interface [classname]+cascading.flow.FlowSkipStrategy+.

Note that [classname]+Flow.start()+ does not check the
[methodname]+isSkipFlow()+ method, and consequently always tries to start the
[classname]+Flow+ if called. The user code determines whether or not to call
[classname]+isSkipFlow()+ for assessing if the programming logic indicates  that
the [classname]+Flow+ should be skipped.

[[custom-flows]]
=== Creating Custom Flows

Custom classes can be treated as flows if given the correct
https://github.com/Cascading/riffle[Riffle] annotations. Riffle is a set of Java
annotations that identify specific methods on a class as providing specific
life-cycle and dependency functionality. For more information, see the Riffle
documentation and examples. To use with Cascading, a Riffle-annotated instance
must be passed to the [classname]+cascading.flow.process.ProcessFlow+
constructor method. The resulting [classname]+ProcessFlow+ instance can be used
like any other flow instance.

Since many algorithms need to perform multiple passes over a given data set, a
Riffle-annotated class can be written that internally creates Cascading flows
and executes them until no more passes are needed. This is like nesting flows or
Cascades in a parent [classname]+Flow+, which in turn can participate in a
Cascade.

[[process-levels]]
=== Process Levels in the Flow Hierarchy

A [classname]+Flow+ is a parent of other process levels, which have other
specific roles.

[classname]+Flow+::

The prior sections provide a detailed description of flows, but at its essence
flows are business-oriented units-of-work. The consumed and produced data  has
some level of durability. A flow satisfies a business or  architectural need.

[classname]+FlowStep+::

A FlowStep represents a unit of platform-managed work.

+

Hadoop MapReduce term: _job_ +
Apache Tez term: _DAG_

[classname]+FlowNode+::

A FlowNode represents the complete unit-of-work that conceptually fits in a
single JVM and what becomes parallelized by handling subsets of the input data
source.

+

The FlowNode may represent multiple data paths, where one path is selected at
runtime depending on which input data set is actually being processed.

+

Hadoop MapReduce terms: _mapper_  and _reducer_ +
Apache Tez term: _vertex_


[classname]+FlowSlice+::

The FlowSlice is the smallest unit of parallelization. At runtime, a FlowSlice
represents the actual JVM-executing Cascading code and the data path(s) being
executed in that JVM.

+

If the FlowNode being parallelized has FlowPipelines, one of those pipelines is
represented here.

+

Hadoop MapReduce and Apache Tez terms: _task attempt_

=== Runtime Metrics

Developers can retrieve runtime metrics (counters) of each process level in a
platform-independent way.
