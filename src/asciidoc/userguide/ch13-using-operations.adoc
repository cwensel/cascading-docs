:doctitle: {_doctitle} - Using and Developing Operations

include::_toc.adoc[]

== Using and Developing Operations

=== Introduction

Previous sections of this guide covered setting up sources and sinks, shaping
the data streams, referencing the data fields, and so on. Within this Pipe
framework, Operations are used to act upon the data -- e.g., alter it, filter
it, analyze it, or transform it. You can use the standard Operations in the
Cascading library to create powerful and robust applications by combining them
in chains (much like UNIX operations, such as _sed_, _grep_, _sort_, _uniq_, and
__awk__). And if you want to go further, it's also very simple to develop custom
Operations in Cascading.

There are four kinds of Operations:

* <<functions,[classname]+Function+>>
* <<filters,[classname]+Filter+>>
* <<aggregators,[classname]+Aggregator+>>
* <<buffers,[classname]+Buffer+>>

image:images/operations.svg[align="center"]

Operations typically require a _multivalued_ Tuple as an input value. And all
Operations can return *_zero or more_* _multivalued_ Tuple results -- except
[classname]+Filter+, which simply returns a Boolean indicating whether to
discard the current Tuple.

A [classname]+Function+, for instance, can parse a string passed by an argument
Tuple and return a new Tuple for every value parsed (i.e., one Tuple for each
"word"), or it may create a single Tuple with every parsed value included as an
element in one Tuple object (e.g., one Tuple with "first-name" and "last-name"
fields).

In theory, a [classname]+Function+ can be used as a [classname]+Filter+ by not
emitting a Tuple result. However, the [classname]+Filter+ type is optimized for
filtering, and can be combined with logical Operations such as [classname]+Not+,
[classname]+And+, [classname]+Or+, etc.

During runtime, Operations actually receive arguments as one or more instances
of the [classname]+TupleEntry+ object. The TupleEntry object holds the current
[classname]+Tuple+, and a [classname]+Fields+ object that defines field names
for positions within the Tuple.

Except for [classname]+Filter+, all Operations must declare result Fields. If
the actual output does not match the declaration, the process fails. For
example, consider a [classname]+Function+ written to parse words out of a String
and return a new Tuple for each word. If it declares that its intended output is
a Tuple with a single field named "word," and then returns more values in the
Tuple beyond that single "word," processing halts. However, Operations designed
to return arbitrary numbers of values in a result Tuple may declare
[code]+Fields.UNKNOWN+.

The Cascading planner always attempts to "fail fast" where possible by checking
the field name dependencies between Pipes and Operations, but there may be some
cases the planner cannot assess.

All Operations must be wrapped by either an [classname]+Each+ or an
[classname]+Every+ pipe instance. The pipe is responsible for passing in an
argument Tuple and accepting the resulting output Tuple from the Operation. In
addition, the pipe merges or replaces the incoming Tuple values with the results
of the Operation.

Operations by default are assumed by the Cascading planner to be "safe." A safe
Operation is idempotent; it can safely execute multiple times on the exact same
record or Tuple; it has no side-effects. If a custom Operation is not
idempotent, the [code]+isSafe()+ method must return [code]+false+. This value
influences how the Cascading planner renders the Flow under certain
circumstances.

[[functions]]
=== Functions

A [classname]+Function+ expects a stream of individual argument Tuples, and
returns zero or more result Tuples for each of them. Like a [classname]+Filter+,
a [classname]+Function+ is used with an [classname]+Each+ pipe, which may follow
any pipe type.

To create a custom [classname]+Function+, subclass the
[code]+cascading.operation.BaseOperation+ class and implement the
[code]+cascading.operation.Function+ interface. Since the [code]+BaseOperation+
has been subclassed, the [code]+operate+ method, as defined on the
[code]+Function+ interface, is the only method that must be implemented.

.Custom Function
====
include::_custom-function.adoc[]
====

Whenever possible, functions should declare both the number of argument values
they expect and the field names of the Tuple they return. However, these
declarations are optional, as explained below.

For input, functions must accept one or more values in a Tuple as arguments. If
not specified, the default is to accept any number of values
([code]+Operation.ANY+). Cascading verifies during planning that the number of
arguments selected matches the number of arguments expected.

For output, it is a good practice to declare the field names that a function
returns. If not specified, the default is [code]+Fields.UNKNOWN+, meaning that
an unknown number of fields are returned in each Tuple.

Both declarations -- the number of input arguments and declared result fields --
must be done on the constructor, either by passing default values to the super
constructor or by accepting the values from the user via a constructor
implementation.

.AddValuesFunction
====
include::_sum-function.adoc[]
====

The example above implements a [classname]+Function+ that accepts two values in
the argument Tuple, adds them together, and returns the result in a new Tuple.

The first constructor above assumes a default field name for the field that this
[classname]+Function+ returns. In practice, it's good to give the user the
option of overriding the declared field names, allowing them to prevent possible
field name collisions that might cause the planner to fail.

This line is especially important:
[source,java]
----
int sum = arguments.getInteger( 0 ) + arguments.getInteger( 1 );
----

Note that ordinal numbers, not field names, are used here to get argument
values. If field names are used, the [classname]+AddValuesFunction+ is coupled
to the incoming stream.

.AddValuesFunction and Context
====
include::_efficient-sum-function.adoc[]
====

This example, a minor variation on the previous one, introduces the use of a
"context" object and [methodname]+prepare()+ and [methodname]+cleanup()+
methods.

All Operations allow for a context object, simply a user-defined object that can
hold user state information between calls to the [methodname]+operate()+ method.
This allows for a given instance of the Operation to be thread safe on a
platform that may use multiple threads of execution versus multiple processes.
It also allows deferring initialization of complex resources until the Operation
is engaged.

The [methodname]+prepare()+ and [methodname]+cleanup()+ methods are invoked once
per thread of execution. In a clustered-platform environment, the methods are
invoked only on the cluster side and never on the client.

In Example 3, a [classname]+Tuple+ is used as the context; a more complex type
is not necessary. Also note that the Tuple is not storing state, but is reused
to reduce the number of new Object instances. In Cascading, it is perfectly safe
to output the same Tuple instance from [methodname]+operate()+. The method
[code]+functionCall.getOutputCollector().add( result )+ does not return until
the resulting [classname]+Tuple+ has been processed, copied, or persisted
downstream.

[[filters]]
=== Filters

A [classname]+Filter+ expects a stream of individual argument Tuples and returns
a boolean value for each one, stating whether it should be discarded. Like a
[classname]+Function+, a [classname]+Filter+ is used with an [classname]+Each+
pipe, which may follow any pipe type.

To create a custom [classname]+Filter+, subclass the class
[code]+cascading.operation.BaseOperation+ and implement the interface
[code]+cascading.operation.Filter+. Because [code]+BaseOperation+ has been
subclassed, the [code]+isRemove+ method, as defined on the [code]+Filter+
interface, is the only method that must be implemented.

.Custom Filter
====
include::_custom-filter.adoc[]
====

Filters must accept one or more values in a Tuple as arguments. When coding a
Filter, declare the number of argument values that you want the Filter to
accept. If not specified, the default is to accept any number of values
([code]+Operation.ANY+). Cascading verifies during planning that the number of
arguments selected matches the number of arguments expected.

The number of arguments declaration must be done on the constructor, either by
passing a default value to the super constructor or by accepting the value from
the user via a constructor implementation.

.StringLengthFilter
====
include::_stringlength-filter.adoc[]
====

The example above implements a [classname]+Filter+ that accepts two arguments
and filters out the current Tuple if the first argument, String length, is
greater than the integer value of the second argument.

[[aggregators]]
=== Aggregators

An [classname]+Aggregator+ expects a stream of tuple groups (the output of a
[classname]+GroupBy+ or [classname]+CoGroup+ pipe), and returns zero or more
result tuples for every group.

An [classname]+Aggregator+ may only be used with an [classname]+Every+ pipe --
an [classname]+Every+ may follow a [classname]+GroupBy+, a [classname]+CoGroup+,
or another [classname]+Every+ pipe, but not an [classname]+Each+.

To create a custom [classname]+Aggregator+, subclass the class
[code]+cascading.operation.BaseOperation+ and implement the interface
[code]+cascading.operation.Aggregator+. Because [code]+BaseOperation+ has been
subclassed, the [code]+start+, [code]+aggregate+, and [code]+complete+ methods,
as defined on the [code]+Aggregator+ interface, are the only methods that must
be implemented.

.Custom Aggregator
====
include::_custom-aggregator.adoc[]
====

Whenever possible, Aggregators should declare both the number of argument values
they expect and the field names of the Tuple they return. However, these
declarations are optional, as explained below.

For input, Aggregators must accept one or more values in a Tuple as arguments.
If not specified, the default is to accept any number of values
([code]+Operation.ANY+). Cascading verifies during planning that the number of
arguments selected is the same as the number of arguments expected.

For output, it is best practice to code an Aggregator so that it declares the
fields that are returned. If not specified, the default is
[code]+Fields.UNKNOWN+, meaning that an unknown number of fields are returned in
each Tuple.

Both declarations -- the number of input arguments and declared result fields --
must be done on the constructor, either by passing default values to the super
constructor or by accepting the values from the user via a constructor
implementation.

.AddTuplesAggregator
====
include::_sum-aggregator.adoc[]
====

The example above implements an [classname]+Aggregator+ that accepts a value in
the argument Tuple, adds all the argument tuples in the current grouping, and
returns the result as a new Tuple.

The first constructor above assumes a default field name that this
[classname]+Aggregator+ returns. In practice, it's good to give the user the
option of overriding the declared field names, allowing them to prevent possible
field name collisions that might cause the planner to fail.

There are several constraints on the use of Aggregators that may not be
self-evident. These constraints are detailed in the Javadoc.

[[buffers]]
=== Buffers

A [classname]+Buffer+ expects a set of argument tuples in the same grouping, and
may return zero or more result tuples.

A [classname]+Buffer+ is very similar to an [classname]+Aggregator+, except that
it receives the current grouping Tuple, and an [classname]+Iterator+ of all the
arguments it expects, for every value Tuple in the current grouping -- all on
the same method call.

[classname]+Buffer+s are similar to the typical _Reducer_ interface in
MapReduce, and is best used for operations that need visibility to the previous
and next elements in the stream for the current group -- such as smoothing a
series of time stamps where there are missing values or creating a running
average.

A [classname]+Buffer+ may only be used with an [classname]+Every+ pipe, and it
may only follow a [classname]+GroupBy+ or [classname]+CoGroup+ pipe type.

To create a custom [classname]+Buffer+, subclass the class
[code]+cascading.operation.BaseOperation+ and implement the interface
[code]+cascading.operation.Buffer+. Because [code]+BaseOperation+ has been
subclassed, the [code]+operate+ method, as defined on the [code]+Buffer+
interface, is the only method that must be implemented.

.Custom Buffer
====
include::_custom-buffer.adoc[]
====

Buffers should declare both the number of argument values they expect and the
field names of the Tuple they return.

For input, Buffers must accept one or more values in a Tuple as arguments. If
not specified, the default is to accept any number of values
([code]+Operation.ANY+). During the planning phase, Cascading verifies that the
number of arguments selected is the same as the number of arguments expected.

For output, it's good practice for Buffers to declare the field names they
return. If not specified, the default is [code]+Fields.UNKNOWN+, meaning that an
unknown number of fields are returned in each Tuple.

Both declarations -- the number of input arguments and declared result fields --
must be done on the constructor, either by passing default values to the super
constructor or by accepting the values from the user via a constructor
implementation.

.Average Buffer
====
include::_average-buffer.adoc[]
====

The example above implements a buffer that accepts a value in the argument
Tuple, adds all these argument tuples in the current grouping, and returns the
result divided by the number of argument tuples counted in a new Tuple.

The first constructor above assumes a default field name for the field that this
[classname]+Buffer+ returns. In practice, it's good to give the user the option
of overriding the declared field names, allowing them to prevent possible field
name collisions that might cause the planner to fail.

Note that this example is somewhat artificial. In actual practice, an
[classname]+Aggregator+ would be a better way to compute averages for an entire
dataset. A [classname]+Buffer+ is better suited for calculating running averages
across very large spans, for example.

There are several constraints on the use of Buffers that may not be
self-evident. These constraints are detailed in the Javadoc.

As with the [classname]+Function+ example above, a [classname]+Buffer+ may
define a custom context object and implement the [methodname]+prepare()+ and
[methodname]+cleanup()+ methods to maintain shared state, or re-use outgoing
[classname]+Tuple+ instances for efficiency.

=== Operation and BaseOperation

In all of the above sections, the [classname]+cascading.operation.BaseOperation+
class was subclassed. This class is an implementation of the
[classname]+cascading.operation.Operation+ interface, and provides a few default
method implementations. It is not strictly required to extend
[classname]+BaseOperation+ when implementing this interface, but it is very
convenient to do so.

When developing custom operations, the developer may need to initialize and
destroy a resource. For example, when doing pattern matching, you might need to
initialize a [classname]+java.util.regex.Matcher+ and use it in a thread-safe
way. Or you might need to open, and eventually close, a remote connection. But
for performance reasons, the operation should not create or destroy the
connection for each Tuple or every Tuple group that passes through. Nor should
user code store any state values in a Class instance or static field.

For this reason, the interface [interfacename]+Operation+ declares two methods:
[methodname]+prepare()+ and [methodname]+cleanup()+.

In the case of a clustered platform, the [methodname]+prepare()+ and
[methodname]+cleanup()+ methods are called once per cluster-side unit of
processing (typically a JVM).

The [methodname]+prepare()+ method is called before any argument Tuple is
passed. The [methodname]+cleanup()+ method is called after all Tuple arguments
are processed in the operation.

Within each of these methods, the developer can initialize or destroy a
"context" object that can hold an open-socket connection or [classname]+Matcher+
instance. This context is user defined, and is the same mechanism used by the
[classname]+Aggregator+ operation -- except that the [classname]+Aggregator+ is
also given the opportunity to initialize and destroy its context, via the
[classname]+start()+ and [classname]+complete()+ methods.

Note that if a "context" object is used, its type should be declared in the
subclass class declaration using the Java Generics notation.
