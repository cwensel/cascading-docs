<?xml version="1.0" encoding="UTF-8"?>
<chapter version="5.0" xml:lang="en" xmlns="http://docbook.org/ns/docbook"
         xmlns:xlink="http://www.w3.org/1999/xlink"
         xmlns:xi="http://www.w3.org/2001/XInclude"
         xmlns:ns5="http://www.w3.org/1998/Math/MathML"
         xmlns:ns4="http://www.w3.org/2000/svg"
         xmlns:ns3="http://www.w3.org/1999/xhtml"
         xmlns:ns="http://docbook.org/ns/docbook">
  <info>
    <title>Cascading</title>
  </info>

  <para>Cascading is an open source Java library and application programming
  interface (API) which provides an abstraction layer for MapReduce. It allows
  developers to build complex, mission-critical, data processing applications
  that run on Hadoop clusters.</para>

  <para>The Cascading project began in the summer of 2007 with its first
  public release, Version 0.1, launched in January 2008. Version 1.0 was
  released in January 2009. Binaries, source code, and add-on modules can be
  downloaded from the project website, http://www.cascading.org/.</para>

  <para>"Map" and "Reduce" operations offer powerful primitives. However, they
  tend to be at the wrong level of granularity for creating sophisticated,
  highly composable code which can be shared among different developers.
  Moreover, many developers find it difficult to "think" in terms of MapReduce
  when faced with real-world applications.</para>

  <para>To address the first issue, Cascading substitutes the "keys" and
  "values" used in MapReduce with simple field names and a data tuple model --
  where a tuple is simply a list of values. For the second issue, Cascading
  departs from "Map" and "Reduce" operations directly by introducing higher
  level abstractions as alternatives; Functions, Filters, Aggregators, and
  Buffers.</para>

  <para>Other alternatives began to emerge at about the same time as the
  project's initial public release, but Cascading was designed to complement
  them. Consider that most of these alternative frameworks impose pre- and
  post-conditions, or other expectations.</para>

  <para>For example, in several other MapReduce tools you must pre-format,
  filter, or import your data into the Hadoop File System (HDFS) prior to
  running the application. That step of preparing the data must be performed
  outside of the programming abstraction. In contrast, Cascading provides
  means to prepare and manage your data using Sources, Sinks, Taps, etc., all
  as integral parts of the programming abstraction.</para>

  <para>This case study begins with an introduction to the main concepts of
  Cascading, then finishes with an overview of how ShareThis (<link
  xlink:href="http://www.sharethis.com/">http://www.sharethis.com</link>) uses
  Cascading in their infrastructure.</para>

  <para>Please see the <emphasis>Cascading User Guide</emphasis> on the
  project website for a more in-depth presentation of the Cascading processing
  model.</para>

  <section>
    <info>
      <title>Fields, Tuples, and Pipes</title>
    </info>

    <para>The MapReduce model uses keys and values to link input data to the
    Map function, the Map function to the Reduce function, and the Reduce
    function to the output data.</para>

    <para>But as we know, real-world Hadoop applications are usually more than
    one MapReduce job chained together. Consider the canonical word count
    example implemented in MapReduce. If you needed to sort the numeric counts
    in descending order, not an unlikely requirement, it would need to be done
    in a second MapReduce job.</para>

    <figure xml:id="fig.mr-count-sort">
      <title>Counting and Sorting in MapReduce</title>

      <mediaobject>
        <imageobject role="fo">
          <imagedata align="center" contentwidth="5in"
                     fileref="images/mr-count-sort.svg"></imagedata>
        </imageobject>

        <imageobject role="html">
          <imagedata align="center" contentwidth="5in"
                     fileref="images/mr-count-sort.png"></imagedata>
        </imageobject>
      </mediaobject>
    </figure>

    <para>So, in the abstract, keys and values not only bind Map to Reduce,
    but Reduce to the next Map, and then to the next Reduce, and so on. That
    is, key/value pairs are sourced from input files and stream through chains
    of Map and Reduce operations and finally rest in an output file. When you
    implement enough of these chained MapReduce applications, you start to see
    a well defined set of key/value manipulations used over and over again to
    modify the key/value data stream.</para>

    <para>Cascading simplifies this by abstracting away keys and values and
    replacing them with tuples that have corresponding field names, similar in
    concept to tables and column names in a Relational Database. And during
    processing, streams of these Fields and Tuples are then manipulated as
    they pass through user defined operations linked together by Pipes.</para>

    <figure xml:id="fig.chained-pipes">
      <title>Pipes linked by Fields and Tuples</title>

      <mediaobject>
        <imageobject role="fo">
          <imagedata align="center" contentwidth="5in"
                     fileref="images/chained-pipes.svg"></imagedata>
        </imageobject>

        <imageobject role="html">
          <imagedata align="center" contentwidth="5in"
                     fileref="images/chained-pipes.png"></imagedata>
        </imageobject>
      </mediaobject>
    </figure>

    <para>So, MapReduce keys and values are reduced to:</para>

    <para><variablelist>
        <varlistentry>
          <term xreflabel="Fields" xml:id="fields">Fields</term>

          <listitem>
            <para>Fields are a collection of either
            <classname>String</classname> names (like "first_name"), numeric
            positions (like 2, or -1, for the third and last position,
            respectively), or a combination of both. Very much like column
            names. So Fields are used to declare the names of values in a
            Tuple, and to select values by name from a Tuple. The later is
            like a SQL <code>select</code> call.</para>
          </listitem>
        </varlistentry>

        <varlistentry>
          <term xreflabel="Tuple" xml:id="tuple">Tuple</term>

          <listitem>
            <para>A Tuple is simply an array of
            <classname>java.lang.Comparable</classname> objects. A Tuple is
            very much like a database row or record.</para>
          </listitem>
        </varlistentry>
      </variablelist></para>

    <para>And the Map and Reduce operations are abstracted behind one or more
    Pipe instances.</para>

    <figure xml:id="fig.pipes">
      <title>Pipe Types</title>

      <mediaobject>
        <imageobject role="fo">
          <imagedata align="center" contentwidth="3in"
                     fileref="images/pipes.svg"></imagedata>
        </imageobject>

        <imageobject role="html">
          <imagedata align="center" contentwidth="3in"
                     fileref="images/pipes.png"></imagedata>
        </imageobject>
      </mediaobject>
    </figure>

    <para><variablelist>
        <varlistentry>
          <term xreflabel="Each" xml:id="each">Each</term>

          <listitem>
            <para>The <code>Each</code> pipe processes a single input Tuple at
            a time. It may apply either a Function or a Filter operation
            (described below) to the input Tuple.</para>
          </listitem>
        </varlistentry>

        <varlistentry>
          <term xreflabel="GroupBy" xml:id="groupby">GroupBy</term>

          <listitem>
            <para>The <code>GroupBy</code> pipe groups Tuples on grouping
            fields. It behaves just like the SQL <code>group by</code>
            statement. It can also merge multiple input tuple streams into a
            single stream, if they all share the same field names.</para>
          </listitem>
        </varlistentry>

        <varlistentry>
          <term xreflabel="CoGroup" xml:id="cogroup">CoGroup</term>

          <listitem>
            <para>The <code>CoGroup</code> pipe both joins multiple tuple
            streams together by common field names, and it also groups the
            Tuples by the common grouping fields. All standard join types
            (inner, outer, etc) and custom joins can be used across two or
            more tuple streams.</para>
          </listitem>
        </varlistentry>

        <varlistentry>
          <term xreflabel="Every" xml:id="every">Every</term>

          <listitem>
            <para>The <code>Every</code> pipe processes a single grouping of
            Tuples at a time, where the group was grouped by a
            <code>GroupBy</code> or <code>CoGroup</code> pipe. The
            <code>Every</code> pipe may apply either an Aggregator or a Buffer
            operation to the grouping.</para>
          </listitem>
        </varlistentry>

        <varlistentry>
          <term xreflabel="SubAssembly"
          xml:id="subassembly">SubAssembly</term>

          <listitem>
            <para>The <code>SubAssembly</code> pipe allows for nesting of
            assemblies inside a single pipe, which can in turn be nested in
            more complex assemblies.</para>
          </listitem>
        </varlistentry>
      </variablelist></para>

    <para>All these pipes are chained together by the developer into "pipe
    assemblies". Where each assembly can have many input Tuple streams
    (sources) and many output Tuple streams (sinks).</para>

    <figure xml:id="fig.pipe-assembly">
      <title>A Simple Pipe Assembly</title>

      <mediaobject>
        <imageobject role="fo">
          <imagedata align="center" contentwidth="6in"
                     fileref="images/pipe-assembly.svg"></imagedata>
        </imageobject>

        <imageobject role="html">
          <imagedata align="center" contentwidth="6in"
                     fileref="images/pipe-assembly.png"></imagedata>
        </imageobject>
      </mediaobject>
    </figure>

    <para>On the surface this might seem more complex than the traditional
    MapReduce model. And admittedly there are more concepts here than Map,
    Reduce, Key, and Value. But in practice there are many more concepts that
    must all work in tandem to provide different behaviors.</para>

    <para>For example, if a developer wanted to provide a "secondary sorting"
    of reducer values, they would need to implement Map, Reduce, a "composite"
    Key (two Keys nested in a parent Key), Value, Partitioner, an "output
    value grouping" Comparator, and an "output key" Comparator. All of which
    would be coupled to one another in varying ways, and very likely
    non-reusable in subsequent applications.</para>

    <para>In Cascading, this would be one line of code:<code> new GroupBy(
    &lt;previous&gt;, &lt;grouping fields&gt;, &lt;secondary sorting
    fields&gt;)</code>. Where <code>previous</code> is the pipe that came
    before.</para>
  </section>

  <section>
    <info>
      <title>Operations</title>
    </info>

    <para>As mentioned above, Cascading departs from MapReduce by introducing
    alternative operations that either are applied to individual Tuples or
    groups of Tuples.</para>

    <figure xml:id="fig.operations">
      <title>Operation Types</title>

      <mediaobject>
        <imageobject role="fo">
          <imagedata align="center" contentwidth="2.5in"
                     fileref="images/operations.svg"></imagedata>
        </imageobject>

        <imageobject role="html">
          <imagedata align="center" contentwidth="2.5in"
                     fileref="images/operations.png"></imagedata>
        </imageobject>
      </mediaobject>
    </figure>

    <para><variablelist>
        <varlistentry>
          <term xreflabel="Function" xml:id="function">Function</term>

          <listitem>
            <para>A <code>Function</code> operates on individual input Tuples
            and may return zero (0) or more output Tuples for every one input.
            Functions are applied by the <code>Each</code> pipe.</para>
          </listitem>
        </varlistentry>

        <varlistentry>
          <term xreflabel="Filter" xml:id="filter">Filter</term>

          <listitem>
            <para>A <code>Filter</code> is a special kind of Function that
            returns a boolean value if the current input Tuple should be
            removed from the Tuple stream. A <code>Function</code> could serve
            this purpose, but the <code>Filter</code> is optimized for this
            case, and many filters can be grouped by "logical" filters like
            <code>And</code>, <code>Or</code>, <code>Xor</code>, and
            <code>Not</code>, rapidly creating more complex filtering
            operations.</para>
          </listitem>
        </varlistentry>

        <varlistentry>
          <term xreflabel="Aggregator" xml:id="aggregator">Aggregator</term>

          <listitem>
            <para>An <code>Aggregator</code> performs some operation against a
            group of Tuples, where the grouped Tuples are grouped by a common
            set of field values. For example, all Tuples having the same
            "last-name" value. Common <code>Aggregator</code> implementations
            would be <code>Sum</code>, <code>Count</code>,
            <code>Average</code>, <code>Max</code>, and
            <code>Min</code>.</para>
          </listitem>
        </varlistentry>

        <varlistentry>
          <term xreflabel="Buffer" xml:id="buffer">Buffer</term>

          <listitem>
            <para>A <code>Buffer</code> is similar to the
            <code>Aggregator</code>, except it is optimized to act as a
            "sliding window" across all the Tuples in a unique grouping. This
            is useful when the developer needs to efficiently insert missing
            values in an ordered set of Tuples (like a missing date or
            duration), or create a running average. Usually
            <code>Aggregator</code> is the operation of choice when working
            with groups of Tuples since many <code>Aggregators</code> can be
            chained together very efficiently, but sometimes a
            <code>Buffer</code> is the best tool for the job.</para>
          </listitem>
        </varlistentry>
      </variablelist></para>

    <para>Operations are bound to pipes when the pipe assembly is
    created.</para>

    <figure xml:id="fig.pipe-assembly-operations">
      <title>An Assembly of Operations</title>

      <mediaobject>
        <imageobject role="fo">
          <imagedata align="center" contentwidth="6in"
                     fileref="images/pipe-assembly-operations.svg"></imagedata>
        </imageobject>

        <imageobject role="html">
          <imagedata align="center" contentwidth="6in"
                     fileref="images/pipe-assembly-operations.png"></imagedata>
        </imageobject>
      </mediaobject>
    </figure>

    <para>The <code>Each</code> and <code>Every</code> pipe provides a simple
    mechanism for selecting some or all values out of a input Tuple before
    being passed to its child operation. And there is a simple mechanism for
    merging the operation results with the original input Tuple to create the
    output Tuple. Without going into great detail, this allows for each
    operation to only care about argument Tuple values and fields, not the
    whole set of fields in the current input Tuple. Subsequently, operations
    can be reusable across applications the same way Java methods can be
    reusable.</para>

    <para>For example, in Java, a method declared as <code>concatenate( String
    first, String second )</code> is more abstract than<code> concatenate(
    Person person )</code>. In the second case, the <code>concatenate()</code>
    function must "know" about the <code>Person</code> object, in the first
    case, it is agnostic to where the data came from. Cascading operations
    exhibit this same quality.</para>
  </section>

  <section>
    <info>
      <title>Taps, Schemes, and Flows</title>
    </info>

    <para>In many of the above diagrams, there are references to "sources" and
    "sinks". In Cascading all data is read from or written to
    <classname>Tap</classname> instances, but is converted to and from Tuple
    instances via <classname>Scheme</classname> objects.</para>

    <para><variablelist>
        <varlistentry>
          <term xreflabel="Tap" xml:id="tap">Tap</term>

          <listitem>
            <para>A <code>Tap</code> is responsible for the "how" and "where"
            parts of accessing data. For example, is the data on HDFS or the
            "local" file system. In Amazon S3 or over HTTP.</para>
          </listitem>
        </varlistentry>

        <varlistentry>
          <term xreflabel="Scheme" xml:id="scheme">Scheme</term>

          <listitem>
            <para>A <code>Scheme</code> is responsible for reading raw data
            and converting it to a Tuple and/or writing a Tuple out into raw
            data. Where this "raw" data can be lines of text, Hadoop binary
            "sequence" files, or some proprietary format.</para>
          </listitem>
        </varlistentry>
      </variablelist>Note that Taps are not part of a pipe assembly, and so
    they are not a type of<classname> Pipe</classname>.</para>

    <para>But they are connected with pipe assemblies when they are made
    cluster executable. When a pipe assembly is connected with the necessary
    number of source and sink Tap instances, we get a
    <code>Flow</code>.</para>

    <para><variablelist>
        <varlistentry>
          <term xreflabel="Flow" xml:id="flow">Flow</term>

          <listitem>
            <para>A <code>Flow</code> is created when a pipe assembly is
            connected with its required number of source and sink taps, and
            the Taps either emit or capture the field names the pipe assembly
            expects. That is, if a <code>Tap</code> emits a Tuple with the
            field name "line" (by reading data from a file on HDFS), the head
            of the pipe assembly must be expecting a "line" value as well.
            Otherwise the process that connects the pipe assembly with the
            Taps will immediately fail with an error.</para>
          </listitem>
        </varlistentry>
      </variablelist>So pipe assemblies are really data process definitions,
    and are not "executable" on their own. They must be connected to source
    and sink Tap instances before they can run on a cluster. This separation
    between Taps and pipe assemblies is part of what makes Cascading so
    powerful.</para>

    <figure xml:id="fig.flow">
      <title>A Flow</title>

      <mediaobject>
        <imageobject role="fo">
          <imagedata align="center" contentwidth="6in"
                     fileref="images/flow.svg"></imagedata>
        </imageobject>

        <imageobject role="html">
          <imagedata align="center" contentwidth="6in"
                     fileref="images/flow.png"></imagedata>
        </imageobject>
      </mediaobject>
    </figure>

    <para>If you think of pipe assemblies to be like a Java Class, then a Flow
    is like a Java Object instance. That is, the same pipe assembly can be
    "instantiated" many times into new Flows, in the same application, without
    fear of any interference between them. This allows pipe assemblies to be
    created and shared like standard Java libraries.</para>
  </section>

  <section>
    <info>
      <title>Cascading in Practice</title>
    </info>

    <para>Now that we know what Cascading is and have a good idea how it
    works, what does an application written in Cascading look like?</para>

    <example>
      <title>Word Count and Sort</title>

      <xi:include href="word-count-sort.xml" />

      <calloutlist>
        <callout arearefs="ex.wcs.source.scheme">
          <para>We create a new <xref linkend="scheme" /> that can read simple
          text files, and emits a new <xref linkend="tuple" /> for each line
          in a field named "line", as declared by the <xref
          linkend="fields" /> instance.</para>
        </callout>

        <callout arearefs="ex.wcs.sink.scheme">
          <para>We create a new <classname>Scheme</classname> that can write
          simple text files, and expects a <classname>Tuple</classname> with
          any number of fields/values. If more than one value, they will be
          TAB delimited in the output file.</para>
        </callout>

        <callout arearefs="ex.wcs.source ex.wcs.sink">
          <para>We create source and sink <xref linkend="tap" /> instances
          that reference the input file and output directory, respectively.
          The sink <classname>Tap</classname> will overwrite any file that may
          already exist.</para>
        </callout>

        <callout arearefs="ex.wcs.pipe">
          <para>We construct the head of our pipe assembly, and name it
          "wordcount". This name is used to bind the source and sink taps to
          the assembly. Multiple heads or tails would require unique
          names.</para>
        </callout>

        <callout arearefs="ex.wcs.each">
          <para>We construct an Each pipe with a <xref linkend="function" />
          that will parse the "line" field into a new
          <classname>Tuple</classname> for each word encountered.</para>
        </callout>

        <callout arearefs="ex.wcs.group.word">
          <para>We construct a GroupBy pipe that will create a new
          <classname>Tuple</classname> grouping for each unique value in the
          field "word".</para>
        </callout>

        <callout arearefs="ex.wcs.every">
          <para>We construct an Every pipe with an <xref
          linkend="aggregator" /> that will count the number of Tuples in
          every unique word group. The result is stored in a field named
          "count".</para>
        </callout>

        <callout arearefs="ex.wcs.group.count">
          <para>We construct a <classname>GroupBy</classname> pipe that will
          create a new <classname>Tuple</classname> grouping for each unique
          value in the field "count", and secondary sort each value in the
          field "word". The result will be a list of "count" and "word" values
          with "count" sorted in increasing order.</para>
        </callout>

        <callout arearefs="ex.wcs.connect ex.wcs.run">
          <para>We connect the pipe assembly to its sources and sinks into a
          <xref linkend="flow" />, and then execute the Flow on the
          cluster.</para>
        </callout>
      </calloutlist>
    </example>

    <para>In the example above, we count the words encountered in the input
    document, and we sort the counts in their natural order (ascending). And
    if some words have the same "count" value, these words are sorted in their
    natural order (alphabetical).</para>

    <para>One obvious problem with this example is that some words might have
    upper-case letters, for example, "the" and "The" when the word becomes at
    the beginning of a sentence. So we might decide to insert a new operation
    to force all the words to lower case, but we realize that all future
    applications that need to parse words from documents should have the same
    behavior, so we decide to create a reusable pipe sub-assembly, just like
    we would by creating a sub-routine in a traditional application.</para>

    <example>
      <title>Creating a SubAssembly</title>

      <xi:include href="word-count-sort-subassembly.xml" />

      <calloutlist>
        <callout arearefs="ex.wcs.subclass">
          <para>We subclass the SubAssembly class, which is itself a kind of
          Pipe.</para>
        </callout>

        <callout arearefs="ex.wcs.expression">
          <para>We create a Java expression function that will call
          <methodname>toLowerCase()</methodname> on the
          <classname>String</classname> value in the field named "word". We
          must also pass in the Java type the expression expects "word" to be,
          in this case,<classname>String</classname>. ( <link
          xlink:href="http://www.janino.net/">http://www.janino.net/ </link>
          is used under the covers)</para>
        </callout>

        <callout arearefs="ex.wcs.tails">
          <para>We must tell the <classname>SubAssembly</classname> superclass
          who the tail ends of our pipe sub-assembly are.</para>
        </callout>
      </calloutlist>
    </example>

    <para>First, we create a SubAssembly pipe to hold our "parse words" pipe
    assembly. Since this is a Java class, it can be reused in any other
    application, as long as there is an incoming field named "word". Note
    there are ways to make this function even more generic, but they are
    covered in the Cascading User Guide.</para>

    <example>
      <title>Extending Word Count and Sort with a SubAssembly</title>

      <xi:include href="word-count-sort-sub.xml" />

      <calloutlist>
        <callout arearefs="ex.wcs.subassembly">
          <para>We replace the <classname>Each</classname> from the previous
          example with our <classname>ParseWordsAssembly</classname>
          pipe.</para>
        </callout>
      </calloutlist>
    </example>

    <para>Finally, we just substitute in our new SubAssembly right where the
    previous Every and word parser function was used in the previous example.
    This nesting can continue as deep as necessary.</para>
  </section>

  <section>
    <info>
      <title>Flexibility</title>
    </info>

    <para>Now take a step back and see what this new model has given us. Or
    better yet, what it has taken away.</para>

    <para>You see, we no longer think in terms of MapReduce jobs, or Mapper
    and Reducer interface implementations. And how to bind or link subsequent
    MapReduce jobs to the ones that precede them. During runtime the Cascading
    "planner" figures out the optimal way to partition the pipe assembly into
    MapReduce jobs, and manages the linkages between them.</para>

    <figure xml:id="fig.pipe-assembly-mapreduce">
      <title>How a Flow Translates to Chained MapReduce Jobs</title>

      <mediaobject>
        <imageobject role="fo">
          <imagedata align="center" contentwidth="6in"
                     fileref="images/pipe-assembly-mapreduce.svg"></imagedata>
        </imageobject>

        <imageobject role="html">
          <imagedata align="center" contentwidth="6in"
                     fileref="images/pipe-assembly-mapreduce.png"></imagedata>
        </imageobject>
      </mediaobject>
    </figure>

    <para>Because of this, developers can build applications of arbitrary
    granularity. They can start with a small application that just filters a
    log file, but then can iteratively build up more features into the
    application as needed.</para>

    <para>Since Cascading is an API and not a syntax like strings of SQL, it
    is more flexible. First off, developers can create Domain Specific
    Languages (DSLs) using their favorite language, like Groovy, JRuby,
    Jython, Scala, and others (see the project site for examples). Second,
    developers can extend various parts of Cascading, like allowing custom
    Thrift or JSON objects to be read and written to and allowing them to be
    passed through the Tuple stream.</para>
  </section>

  <section>
    <info>
      <title>Hadoop and Cascading at ShareThis</title>
    </info>

    <para>ShareThis is a sharing network that makes it simple to share any
    online content. With the click of a button on a web page or browser
    plug-in, ShareThis allows users to seamlessly access their contacts and
    networks from anywhere online and share the content through email, IM,
    Facebook, Digg, mobile SMS, etc. without ever leaving the current page.
    Publishers can deploy the ShareThis button to tap the service’s universal
    sharing capabilities to drive traffic, stimulate viral activity and track
    the sharing of online content. ShareThis also simplifies social media
    services by reducing clutter on web pages and providing instant
    distribution of content across social networks, affiliate groups, and
    communities.</para>

    <para>As ShareThis users share pages and information through the online
    widgets, a continuous stream of events enter the ShareThis network. These
    events are first filtered and processed, and then handed to various
    back-end systems including AsterData, Hypertable, and Katta.</para>

    <para>The volume of these event can be huge, too large to process with
    traditional systems. This data can also be very "dirty" thanks to
    "injection attacks" from rogue systems, browser bugs, or faulty widgets.
    For this reason ShareThis chose to deploy Hadoop as the pre-processing and
    orchestration front-end to their back-end systems. They also chose to use
    Amazon Web Services to host their servers, on the Elastic Computing Cloud
    (EC2), and provide long term storage, on the Simple Storage Service (S3).
    With an eye towards leveraging Elastic MapReduce (EMR).</para>

    <figure xml:id="fig.sharethis-logprocessing">
      <title>The ShareThis Log Processing Pipeline</title>

      <mediaobject>
        <imageobject role="fo">
          <imagedata align="center" contentwidth="6in"
                     fileref="images/sharethis-logprocess.svg"></imagedata>
        </imageobject>

        <imageobject role="html">
          <imagedata align="center" contentwidth="6in"
                     fileref="images/sharethis-logprocess.png"></imagedata>
        </imageobject>
      </mediaobject>
    </figure>

    <para>In this overview we will focus on the "log processing pipeline". The
    log processing pipeline simply takes data stored in an S3 bucket,
    processes it (described below), and stores the results back into another
    bucket. Simple Queue Service (SQS) is used to coordinate the events that
    mark the start and completion of data processing runs. Downstream other
    processes pull data that load AsterData, pull URL lists from Hypertable to
    source a web crawl, or to pull crawled page data to create Lucene indexes
    for use by Katta. Note Hadoop is central to the ShareThis architecture. It
    is used to coordinate the processing and movement of data between
    architectural components.</para>

    <para>With Hadoop as the front-end, all the event logs can be parsed,
    filtered, cleaned, and organized by a set of rules before ever being
    loaded into the AsterData cluster or used by any other component.
    AsterData is a clustered data-warehouse that can support large data-sets
    and allow for complex ad-hoc queries using a standard SQL syntax.
    ShareThis chose to clean and prepare the incoming data-sets on the Hadoop
    cluster and then to load that data into the AsterData cluster for ad-hoc
    analysis and reporting. Though possible with AsterData, it made much sense
    to use Hadoop as the first stage in the processing pipeline to offset load
    on the main data-warehouse.</para>

    <para>Cascading was chosen as the primary data processing API to simplify
    the development process, codify the coordination of data between
    architectural components, and as the developer facing interface to those
    components. This is a departure from the traditional Hadoop use-case of
    simply querying stored data and is one where Hadoop and Cascading bring
    the most value to users.</para>

    <para>For developers, Cascading made it easy to start with a simple
    unit-test (by sub-classing cascading.ClusterTestCase) that did simple text
    parsing and then to layer in more processing rules while keeping the
    application logically organized for maintenance. Cascading aided this
    organization in a couple ways. First, stand-alone operations (Functions,
    Filter, etc) could be written and tested independently. Second, the
    application was segmented into stages, one for parsing, one for rules, and
    a final stage for binning/collating the data, all via the SubAssembly base
    class described above.</para>

    <para>The data coming from the ShareThis loggers looks a lot like Apache
    logs with date/timestamps, share URL's, referrer URL's, and a bit of
    meta-data. To use the data for analysis downstream the URLs needed to be
    un-packed (parsing query-string data, domain names, etc). So a top level
    SubAssembly was created to encapsulate the parsing, and child
    SubAssemblies were nested inside to handle specific fields if they were
    sufficiently complex to parse.</para>

    <para>The same was done for applying rules. As every Tuple passed through
    the rules SubAssembly, it was marked as "bad" if any of the rules were
    triggered. Along with the "bad" tag, a description of why the record was
    bad was added to the Tuple for later review.</para>

    <para>Finally a splitter SubAssembly was created to do two things. First,
    to allow for the tuple stream to split into two, one stream for "good"
    data and one for "bad" data. Second, the splitter binned the data into
    intervals, for example, every hour. To do this, only two operations were
    necessary. The first to create the interval from the 'timestamp' value
    already present in the stream, and the second to use the 'interval' and
    'good/bad' meta-data to create a directory path, for example, "05/good/"
    where "05" is 5am and "good" means the tuple passed all the rules. This
    path would then be used by the Cascading TemplateTap, a special Tap that
    can dynamically output tuple streams to different locations based on
    values in the Tuple. In this case, the TemplateTap used the "path" value
    to create the final output path.</para>

    <para>The developers also created a fourth SubAssembly, this one to apply
    Cascading Assertions during unit-testing. These assertions double checked
    that rules and parsing SubAssemblies did their job.</para>

    <para>In the unit test below we see the splitter isn't being tested, it is
    added in another integration test not shown.</para>

    <example>
      <title>Unit Testing a Flow</title>

      <xi:include href="sharethis-unittest.xml" />
    </example>

    <para>For integration and deployment, many of the features built into
    Cascading allowed for easier integration with external systems and for
    greater process tolerance.</para>

    <para>In production, all the SubAssemblies are joined and planned into a
    Flow, but instead of just source and sink Taps, trap Taps were planned in.
    Normally when a operation throws an exception from a remote Mapper or
    Reducer task, the Flow will fail and kill all its managed MapReduce jobs.
    When a Flow has traps, any exceptions are caught and the data causing the
    exception is saved to the Tap associated with the current trap. Then the
    next Tuple is processed without stopping the Flow. Sometimes you want your
    Flows to fail on errors, but in this case, the ShareThis developers knew
    they could go back and look at the "failed" data and update their unit
    tests while the production system kept running. Losing a few hours of
    processing time was worse than losing a couple bad records.</para>

    <para>Using Cascading's event listeners, Amazon SQS could be integrated.
    When a Flow finished, a message is sent to notify other systems there is
    data ready to be picked up from Amazon S3. On failure, a different message
    is sent alerting other processes.</para>

    <figure xml:id="fig.sharethis-logprocessing-flow">
      <title>The ShareThis Log Processing Flow</title>

      <mediaobject>
        <imageobject role="fo">
          <imagedata align="center" contentwidth="6in"
                     fileref="images/sharethis-logprocess-flow.svg"></imagedata>
        </imageobject>

        <imageobject role="html">
          <imagedata align="center" contentwidth="6in"
                     fileref="images/sharethis-logprocess-flow.png"></imagedata>
        </imageobject>
      </mediaobject>
    </figure>

    <para>The remaining downstream processes pick up where the log processing
    pipeline leaves off on different independent clusters. The log processing
    pipeline today runs once a day, so there is no need to keep a 100 node
    cluster sitting around for the 23 hours it has nothing to do. So it is
    decommissioned and recommissioned 24 hours later.</para>

    <para>In the future, it would be trivial to increase this interval on
    smaller clusters to every 6 hours, or 1 hour, as the business demands.
    Independently other clusters are booting and shutting down at different
    intervals based on the needs of the business unit responsible for that
    component. For example, the web crawler component (using Bixo, a Cascading
    based web-crawler toolkit developed by EMI and ShareThis) may run
    continuously on a small cluster with a companion Hypertable cluster. This
    on-demand model works very well with Hadoop where each cluster can be
    tuned for the kind of workload it is expected to handle.</para>
  </section>

  <section>
    <info>
      <title>Summary</title>
    </info>

    <para>Hadoop is a very powerful platform for processing and coordinating
    the movement of data across various architectural components. Its only
    drawback is that the primary computing model is MapReduce.</para>

    <para>Cascading aims to help developers build powerful applications
    quickly and simply, through a well reasoned API, without needing to think
    in MapReduce. While leaving the heavy lifting of data distribution and
    replication, and distributed process management and liveness to
    Hadoop.</para>

    <para>Read more about Cascading, join the online community, and download
    sample applications by visiting project website,
    http://www.cascading.org/.</para>
  </section>
</chapter>
