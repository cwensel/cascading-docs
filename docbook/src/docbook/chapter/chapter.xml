<?xml version="1.0" encoding="UTF-8"?>
<chapter version="5.0" xml:lang="en" xmlns="http://docbook.org/ns/docbook"
         xmlns:xlink="http://www.w3.org/1999/xlink"
         xmlns:xi="http://www.w3.org/2001/XInclude"
         xmlns:ns5="http://www.w3.org/1998/Math/MathML"
         xmlns:ns4="http://www.w3.org/2000/svg"
         xmlns:ns3="http://www.w3.org/1999/xhtml"
         xmlns:ns="http://docbook.org/ns/docbook">
  <info>
    <title>Cascading - Leveraging Scalability</title>
  </info>

  <para>Cascading is Java library and application programming interface (API)
  that allows developers to build increasingly complex, production quality,
  data processing applications for execution in Hadoop clusters.</para>

  <para>The Cascading project was started during the late Summer of 2007, with
  its first public release, Version 0.1.0, in January 2008. By the time this
  chapter is published, it will have reached Version 1.0, after over a year of
  continuous development, testing, and refinement. Binaries and source code
  can be downloaded from the project website, <link
  xlink:href="http://www.cascading.org/">http://www.cascading.org/</link>.</para>

  <para>The Cascading project was started because it was immediately obvious
  that the MapReduce model does not scale organizationally. MapReduce is
  difficult for developers to "think" in when faced with real-world
  applications. And it is at the wrong granularity for creating efficient,
  composable applications that can be shared between users.</para>

  <para>To solve the first criticism, Cascading traded in MapReduce keys and
  values for a simple field names and data tuple model. For the second,
  Cascading departs from the Map and Reduce operations by introducing some
  alternatives; Functions, Filters, Aggregators, and Buffers.</para>

  <para>Though many alternatives to Cascading began to emerge before and after
  the initial public release of Cascading, Cascading was designed to be
  complimentary to all of them. All these alternative frameworks have some pre
  and post conditions or expectations. For example, data must be
  pre-formatted, filtered, and/or imported into the Hadoop File System (HDFS).
  It is quite a simple endeavor to have Cascading manage, and prepare or
  import, data into these alternative frameworks as necessary.</para>

  <para>This chapter represents a short introduction and summary of the main
  concepts behind Cascading, please see the Cascading User Guide, on the
  project website, for a more in depth presentation of the Cascading
  processing model.</para>

  <section>
    <info>
      <title>Fields, Tuples, and Pipes</title>
    </info>

    <para>The MapReduce model uses keys and values to link input data to the
    Map function, the Map function to the Reduce function, and the Reduce
    function to the output data.</para>

    <para>But as we know, real-world Hadoop applications are usually more than
    one MapReduce job chained together. Consider the canonical word count
    example implemented in MapReduce. If you needed to sort the numeric counts
    in descending order, not an unlikely requirement, it would need to be done
    in a second MapReduce job.</para>

    <mediaobject>
      <imageobject role="fo">
        <imagedata align="center" contentwidth="5in"
                   fileref="images/mr-count-sort.svg"></imagedata>
      </imageobject>

      <imageobject role="html">
        <imagedata align="center" contentwidth="5in"
                   fileref="images/mr-count-sort.png"></imagedata>
      </imageobject>
    </mediaobject>

    <para>So, in the abstract, keys and values not only bind Map to Reduce,
    but Reduce to the next Map, and then to the next Reduce, and so on. That
    is, key/value pairs are sourced from input files and stream through chains
    of Map and Reduce operations and finally rest in an output file. When you
    implement enough of these chained MapReduce applications, you start to see
    a well defined set of key/value manipulations used over and over again to
    manipulate the key/value data stream.</para>

    <para>Cascading both leverages these emergent patterns and simultaneously
    abstracts their complexity away.</para>

    <para>In part this is accomplished by abstracting away keys and values and
    replacing them with tuples that have corresponding field names, similar in
    concept to tables and column names in a Relational Database. And during
    processing, streams of these Fields and Tuples are then manipulated by
    Pipes.</para>

    <mediaobject>
      <imageobject role="fo">
        <imagedata align="center" contentwidth="5in"
                   fileref="images/chained-pipes.svg"></imagedata>
      </imageobject>

      <imageobject role="html">
        <imagedata align="center" contentwidth="5in"
                   fileref="images/chained-pipes.png"></imagedata>
      </imageobject>
    </mediaobject>

    <para>So, MapReduce keys and values are reduced to:</para>

    <mediaobject>
      <imageobject role="fo">
        <imagedata align="center" contentwidth="1.5in"
                   fileref="images/fields-tuples.svg"></imagedata>
      </imageobject>

      <imageobject role="html">
        <imagedata align="center" contentwidth="1.5in"
                   fileref="images/fields-tuples.png"></imagedata>
      </imageobject>
    </mediaobject>

    <para><variablelist>
        <varlistentry>
          <term>Fields</term>

          <listitem>
            <para>Fields are a collection of either
            <classname>String</classname> names (like "first_name"), numeric
            positions (like 2, or -1, for the third and last position,
            respectively), or a combination of both. Very much like column
            names. So Fields are used to declare the names of values in a
            Tuple, and to select values by name from a Tuple. The later is
            like a SQL <code>select</code> call.</para>
          </listitem>
        </varlistentry>

        <varlistentry>
          <term>Tuple</term>

          <listitem>
            <para>A Tuple is simply an array of
            <classname>java.lang.Comparable</classname> objects. A Tuple is
            very much like a database row or record.</para>
          </listitem>
        </varlistentry>
      </variablelist></para>

    <para>And the Map and Reduce operations are abstracted behind one or more
    Pipe instances.</para>

    <mediaobject>
      <imageobject role="fo">
        <imagedata align="center" contentwidth="3in"
                   fileref="images/pipes.svg"></imagedata>
      </imageobject>

      <imageobject role="html">
        <imagedata align="center" contentwidth="3in"
                   fileref="images/pipes.png"></imagedata>
      </imageobject>
    </mediaobject>

    <para><variablelist>
        <varlistentry>
          <term>Each</term>

          <listitem>
            <para>The Each pipe processes a single input Tuple at a time, but
            may return zero (0) or more result Tuples for every input Tuple.
            The Each pipe may apply either a Function or a Filter to the input
            Tuple.</para>
          </listitem>
        </varlistentry>

        <varlistentry>
          <term>GroupBy</term>

          <listitem>
            <para>The GroupBy pipe groups Tuples based on the values of a
            given set of field names. It behaves just like the SQL <code>group
            by</code> statement. The GroupBy can also merge multiple inpust
            tuple streams, if they all share the same field names.</para>
          </listitem>
        </varlistentry>

        <varlistentry>
          <term>CoGroup</term>

          <listitem>
            <para>The CoGroup pipe both joins multiple tuple streams by the
            values of a given set of field names, it also groups the Tuples by
            those values. The all join types and custom joins are supported.
            And more than two streams may be joined simultaneously.</para>
          </listitem>
        </varlistentry>

        <varlistentry>
          <term>Every</term>

          <listitem>
            <para>The Every pipe processes a single group of Tuples at a time,
            where the group share a common sub-set of Tuple values. And it may
            return zero (0) or more result Tuples for that grouping for every
            input grouping. The Every pipe may apply either an Aggregator or a
            Buffer to the input Tuple grouping.</para>
          </listitem>
        </varlistentry>

        <varlistentry>
          <term>SubAssembly</term>

          <listitem>
            <para>The SubAssembly pipe allows for nesting of sub assemblies inside a single pipe, which
            are in turn nested in more complex assemblies.</para>
          </listitem>
        </varlistentry>
      </variablelist></para>

    <para>All these pipes are chained together by the developer into "pipe
    assemblies". Where each assembly can have many input Tuple streams
    (sources) and many output Tuple streams (sinks).</para>

    <mediaobject>
      <imageobject role="fo">
        <imagedata align="center" contentwidth="6in"
                   fileref="images/pipe-assembly.svg"></imagedata>
      </imageobject>

      <imageobject role="html">
        <imagedata align="center" contentwidth="6in"
                   fileref="images/pipe-assembly.png"></imagedata>
      </imageobject>
    </mediaobject>

    <para>On the surface this might seem more complex than the traditional
    MapReduce model. And admitedly there are more concepts here than Map,
    Reduce, Key, and Value. But in practice, as implemented in Hadoop, there
    are many more concepts that must all work in tandem to affect different
    behaviors.</para>

    <para>For example, if a developer wanted to provide a "secondary sorting"
    of reduce values, they would need to implement Map, Reduce, a composite
    Key (two Keys nested in a parent Key), Value, Partitioner, an "output
    value grouping" Comparator, and an "output key" Comparator. All of which
    would be coupled to one another in varying ways, and very likely
    non-reusable in subsequent jobs.</para>

    <para>In Cascading, this would be one line of code: <code>new GroupBy(
    &lt;previous&gt;, &lt;grouping fields&gt;, &lt;secondary sorting
    fields&gt;)</code>. Where <code>previous</code> is the pipe that came
    before.</para>
  </section>

  <section>
    <info>
      <title>Operations</title>
    </info>

    <para>As mentioned above, Cascading departs from MapReduce by introducing
    alternative operations that either are applied to individual Tuples or
    groups of Tuples.</para>

    <mediaobject>
      <imageobject role="fo">
        <imagedata align="center" contentwidth="3in"
                   fileref="images/operations.svg"></imagedata>
      </imageobject>

      <imageobject role="html">
        <imagedata align="center" contentwidth="3in"
                   fileref="images/operations.png"></imagedata>
      </imageobject>
    </mediaobject>

    <para><variablelist>
        <varlistentry>
          <term>Function</term>

          <listitem>
            <para>A Function operates on individual input Tuples and may
            return zero (0) or more output Tuples for every one input.
            Functions are applied by the Each pipe.</para>
          </listitem>
        </varlistentry>

        <varlistentry>
          <term>Filter</term>

          <listitem>
            <para>A Filter is a special kind of Function that returns a
            boolean value if the current input Tuple should be removed from
            the Tuple stream. A Function could serve this purpose, but the
            Filter is optimized for this case, and many filters can be grouped
            by "logical" filters like And, Or, Xor, and Not, rapidly creating
            more complex filtering operations.</para>
          </listitem>
        </varlistentry>

        <varlistentry>
          <term>Aggregator</term>

          <listitem>
            <para>An Aggregator performs some function against a group of
            Tuples, where the grouped Tuples are grouped by a common set of
            field values. For example, all Tuples have the same "last-name"
            field value. Common Aggregator functions would be Sum, Count,
            Average, Max, and Min.</para>
          </listitem>
        </varlistentry>

        <varlistentry>
          <term>Buffer</term>

          <listitem>
            <para>A Buffer is similar to the Aggregator, except it is
            optimized to act as a "sliding window" across a group of Tuples.
            This is useful when the developer needs to efficiently insert
            missing values in an ordered set of Tuples (like a missing date or
            duration). Usually Aggregator is the operation of choice when
            working with groups of Tuples since many Aggregators can be
            chained together very efficiently, but sometimes a Buffer is the
            best tool for the job.</para>
          </listitem>
        </varlistentry>
      </variablelist></para>

    <para>Operations are bound to pipes when the pipe assembly is
    created.</para>

    <mediaobject>
      <imageobject role="fo">
        <imagedata align="center" contentwidth="6in"
                   fileref="images/pipe-assembly-operations.svg"></imagedata>
      </imageobject>

      <imageobject role="html">
        <imagedata align="center" contentwidth="6in"
                   fileref="images/pipe-assembly-operations.png"></imagedata>
      </imageobject>
    </mediaobject>

    <para>The Each and Every pipe provides a simple mechanism for selecting
    some or all values out of a input Tuple before being passed to its child
    operation. And there is a simple mechanism for merging the operation
    results with the original input Tuple to create the output Tuple. Without
    going into great detail, this allows for each operation to only care about
    argument Tuple values and fields, not the whole set of fields in the
    current input Tuple. Subsequently, operations can be reusable across
    applicaitions the same way Java methods can be reusable.</para>

    <para>For example, in Java, a method declared as <code>join( String first,
    String second )</code> is more abstract than <code>join( Person person
    )</code>. In the second case, the <code>join</code> function must know
    about the <code>Person</code> object, in the first case, it is agnostic to
    where the data came from. Cascading operations exhibit this same
    quality.</para>
  </section>

  <section>
    <info>
      <title>Absorbing Complexity</title>
    </info>

    <para>Now take a step back and see what this new model has given us. Or
    better yet, what it has taken away.</para>

    <para>You see, we no longer think in terms of MapReduce jobs, or Mapper
    and Reducer interface implementations. And how to bind or link subsequent
    MapReduce jobs to the ones that preceede them. Cascading, during runtime
    figures out the optimal way to partition the pipe assembly into MapReduce
    jobs, and manages the linkages between them.</para>

    <mediaobject>
      <imageobject role="fo">
        <imagedata align="center" contentwidth="6in"
                   fileref="images/pipe-assembly-mapreduce.svg"></imagedata>
      </imageobject>

      <imageobject role="html">
        <imagedata align="center" contentwidth="6in"
                   fileref="images/pipe-assembly-mapreduce.png"></imagedata>
      </imageobject>
    </mediaobject>

    <para>Because of this, developers can build applications of arbitrary
    granularity. They can start with a small application that just filters a
    log file, but then can iteratively build up more features into the
    application as needed.</para>

    <para>Since Cascading is an API and not a syntax like SQL, it is more
    flexible. First off, developers can create Domain Specific Languages
    (DSLs) using their favorite language, like Groovy, JRuby, Jython, Scala,
    and others. Second, developers can extend various parts of Cascading, like
    allowing custom Thrift or JSON objects to be read and written to, and
    allowing them to be passed through the Tuple stream.</para>
  </section>

  <section>
    <info>
      <title>Production Quality</title>
    </info>

    <para>No doubt Cascading can be used for ad-hoc, single use, applications.
    But it really shines when large complex processes need to be created that
    need a higher level of robustness.</para>

    <para>Here are just a few of these features:</para>

    <para><variablelist>
        <varlistentry>
          <term>Stream Assertions</term>

          <listitem>
            <para>Stream assertions are just like Java assertions and unit
            tests all rolled up. A pipe assembly can be assebled with
            assertions built into them. These assertions can test for null
            values in an argument Tuple, or if the values match some regular
            expression pattern. If the assertion fails, the process can
            optionally fail, or it can save the bad input Tuple to a new file
            on HDFS. If assertions aren't wanted, Cascading can remove them at
            runtime so they don't slow things down on your production system,
            for example.</para>
          </listitem>
        </varlistentry>

        <varlistentry>
          <term>Failure Traps</term>

          <listitem>
            <para>Failure traps are like Java Exception handlers. If an
            operation fails unexpectedly, the input Tuple that caused the
            failure can be saved off to a file for later debugging. This
            features works great with stream assertions.</para>
          </listitem>
        </varlistentry>

        <varlistentry>
          <term>Events</term>

          <listitem>
            <para>Events allow for external code and/or systems to be notified
            when a new process starts, completes, or fails.</para>
          </listitem>
        </varlistentry>

        <varlistentry>
          <term>Topological Scheduling</term>

          <listitem>
            <para>In Cascading, many pipe assemblies can be made into an
            executable data process simultaneously. If one process depends on
            a previous one, Cascading offers up an utility called a Cascade
            that allows for all these processes to be scheduled in dependency
            order.</para>
          </listitem>
        </varlistentry>
      </variablelist></para>
  </section>

  <section>
    <info>
      <title>Cascading in Practice</title>
    </info>

    <para>Now that we know what Cascading is and have a good idea how it
    works, what does an application written in Cascading look like, and is
    really any simpler.</para>

    <example>
      <title>Word Count and Sort</title>

      <xi:include href="word-count-sort.xml" />

      <calloutlist>
        <callout arearefs="ex.wcs.source ex.wcs.sink">
          <para>We create source and sink Taps that reference the input file
          and output directory, respectively.</para>
        </callout>

        <callout arearefs="ex.wcs.each">
          <para>We construct an Each pipe with a Function that will parse the
          "line" field into a new Tuple for each word encountered.</para>
        </callout>

        <callout arearefs="ex.wcs.group.word">
          <para>We construct a GroupBy pipe that will create a new Tuple
          grouping for each unique value in the field "word".</para>
        </callout>

        <callout arearefs="ex.wcs.every">
          <para>We construct an Every pipe that will count the number of
          Tuples in every unique word group. The result is stored in a field
          named "count".</para>
        </callout>

        <callout arearefs="ex.wcs.group.count">
          <para>We construct a GroupBy pipe that will create a new Tuple
          grouping for each unique value in the field "count", and secondary
          sort each value in the field "word". The result will be a list of
          "count" and "word" values with "count" sorted in increasing
          order.</para>
        </callout>

        <callout arearefs="ex.wcs.connect ex.wcs.run">
          <para>We connect the pipe assembly to its sources and sinks, and
          then execute it on the cluster.</para>
        </callout>
      </calloutlist>
    </example>

    <para>In the example above, we count the words encounterd in the input
    document, and we sort the counts in their natural order (ascending). And
    if some words have the same "count" value, these words are sorted in their
    natural order (alphabetical).</para>

    <para>One obvious problem with this example is that some words might have
    upper-case letters, for example, "the" and "The" when the word becomes at
    the beginning of a sentence. So we might decide to insert a new operation
    to force all the words to lower case, but we also decide that all future
    applications that need to parse words from documents should have the same
    behavior, so we decide to create a reusable pipe sub-assembly, just like
    we would by creating a sub-routine in a traditional application.</para>

    <example>
      <title>Creating a SubAssembly</title>

      <xi:include href="word-count-sort-subassembly.xml" />

      <calloutlist>
        <callout arearefs="ex.wcs.subclass">
          <para>We subclass the SubAssembly class, which is itself a kind of Pipe.</para>
        </callout>
        <callout arearefs="ex.wcs.expression">
          <para>We create a Java expression function that will call toLowerCase() on the "word" value.</para>
        </callout>
        <callout arearefs="ex.wcs.tails">
          <para>We must tell the SubAssembly superclass who the tail ends of our pipe sub-assembly are.</para>
        </callout>
      </calloutlist>
    </example>

    <para>First, we create a SubAssembly pipe to hold our "parse words" pipe assembly. Since this is a
    Java class, it can be reused in any other application, as long as there is an incoming field named "word". Note
    there are ways to make this function even more generic, but they are out of scope here.</para>

    <example>
      <title>Extending Word Count and Sort</title>
      <xi:include href="word-count-sort-sub.xml" />

      <calloutlist>
        <callout arearefs="ex.wcs.subassembly">
          <para>We replace the Each from the previous example with our ParseWordsAssembly pipe.</para>
        </callout>
      </calloutlist>
    </example>

    <para>Finall, we just substitute in our new SubAssembly right where the previous Every and word parser function
    was used in the previous example. This nesting can continue as deep as necessary, in the same way Objects and be
    subclassed and methods can call other methods in an application.</para>
  </section>
</chapter>
