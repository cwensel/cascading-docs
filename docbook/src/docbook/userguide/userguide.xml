<?xml version="1.0" encoding="UTF-8"?>
<!--
  ~ Copyright (c) 2007-2012 Concurrent, Inc. All Rights Reserved.
  ~
  ~ Project and contact information: http://www.concurrentinc.com/
  -->
<book version="5.0" xml:lang="en" xmlns="http://docbook.org/ns/docbook"
      xmlns:xlink="http://www.w3.org/1999/xlink"
      xmlns:xi="http://www.w3.org/2001/XInclude"
      xmlns:ns5="http://www.w3.org/1998/Math/MathML"
      xmlns:ns4="http://www.w3.org/1999/xhtml"
      xmlns:ns3="http://www.w3.org/2000/svg"
      xmlns:ns="http://docbook.org/ns/docbook">
  <info>
    <title>Cascading 2 - User Guide</title>

    <pubdate>March, 2012</pubdate>

    <copyright>
      <year>2007-2012</year>

      <holder>Concurrent, Inc.</holder>
    </copyright>

    <releaseinfo>V 2.01</releaseinfo>

    <productname>Cascading</productname>

    <authorgroup>
      <author>
        <orgname>Concurrent, Inc.</orgname>
      </author>
    </authorgroup>

    <mediaobject>
      <imageobject role="fo">
        <imagedata contentwidth="1in" fileref="images/cascading-logo.svg"/>
      </imageobject>

      <imageobject role="html">
        <imagedata contentwidth="1in" fileref="images/cascading-logo.png"/>
      </imageobject>
    </mediaobject>
  </info>

  <toc/>

  <chapter>
    <info>
      <title>About Cascading</title>
    </info>

    <section>
      <title>What is Cascading?</title>

      <para>Cascading is a data processing API and processing query planner
      used for defining, sharing, and executing data-processing workflows on a
      single computing node or distributed computing cluster. On a single
      node, Cascading's "local mode" can be used to efficiently test code and
      process local files before being deployed on a cluster. On a distributed
      computing cluster using Apache Hadoop platform, Cascading adds an
      abstraction layer over the Hadoop API, greatly simplifying Hadoop
      application development, job creation, and job scheduling.</para>
    </section>

    <section>
      <title>Usage Scenarios</title>

      <section>
        <title>Why use Cascading?</title>

        <para>Cascading was developed to allow organizations to rapidly
        develop complex data processing applications with Hadoop. The need for
        Cascading is typically driven by one of two cases:</para>

        <para><emphasis role="bold">Increasing data size</emphasis> exceeds
        the processing capacity of a single computing system. In response,
        developers may adopt Apache Hadoop as the base computing
        infrastructure, but discover that developing useful applications on
        Hadoop is not trivial. Cascading eases the burden on these developers
        and allows them to rapidly create, refactor, test, and execute complex
        applications that scale linearly across a cluster of computers.</para>

        <para><emphasis role="bold">Increasing process complexity in data
        centers</emphasis> results in one-off data-processing applications
        sprawling haphazardly onto any available disk space or CPU. Apache
        Hadoop solves the problem with its Global Namespace file system, which
        provides a single reliable storage framework. In this scenario,
        Cascading eases the learning curve for developers as they convert
        their existing applications for execution on a Hadoop cluster for its
        reliability and scalability. In addition, it lets developers create
        reusable libraries and applications for use by analysts, who use them
        to extract data from the Hadoop file system.</para>

        <para>Since Cascading's creation, a number of Domain Specific
        Languages (DSL's) have emerged as query languages that wrap the
        Cascading API's, allowing developers and analysts to create ad-hoc
        queries for data mining and exploration. These DSL's coupled with
        Cascading local-mode allow users to rapidly query and analyze
        reasonably large datasets on their local systems before executing them
        at scale in a production environment. See the section on DSL's for
        references.</para>
      </section>

      <section>
        <title>Who are the users?</title>

        <para>Cascading users typically fall into three roles:</para>

        <para><emphasis role="bold">The application Executor</emphasis> is a
        person (e.g., a developer or analyst) or process (e.g., a cron job)
        that runs a data processing application on a given cluster. This is
        typically done via the command line, using a pre-packaged Java Jar
        file compiled against the Apache Hadoop and Cascading libraries. The
        application may accept command-line parameters to customize it for a
        given execution, and generally outputs a data set to be exported from
        the Hadoop file system for some specific purpose.</para>

        <para><emphasis role="bold">The process Assembler</emphasis> is a
        person who assembles data processing workflows into unique
        applications. This work is generally a development task that involves
        chaining together operations to act on one or more input data sets,
        producing one or more output data sets. This can be done with the raw
        Java Cascading API, or with a scripting language such as Scale,
        Clojure, Groovy, JRuby, or Jython (or by one of the DSL's implemented
        in these languages).</para>

        <para><emphasis role="bold">The operation Developer</emphasis> is a
        person who writes individual functions or operations (typically in
        Java) or reusable sub-assemblies that act on the data that passes
        through the data processing workflow. A simple example would be a
        parser that takes a string and converts it to an Integer. Operations
        are equivalent to Java functions in the sense that they take input
        arguments and return data. And they can execute at any granularity,
        from simply parsing a string to performing complex procedures on the
        argument data using third-party libraries.</para>

        <para>All three roles can be filled by a developer, but because
        Cascading supports a clean separation of these responsibilities, some
        organizations may choose to use non-developers to run ad-hoc
        applications or build production processes on a Hadoop cluster.</para>
      </section>
    </section>

    <section>
      <title>What is Apache Hadoop?</title>

      <para>From the Hadoop website, it <quote>is a software platform that
      lets one easily write and run applications that process vast amounts of
      data</quote>. Hadoop does this by providing a storage layer that holds
      vast amounts of data, and an execution layer that runs an application in
      parallel across the cluster, using coordinated subsets of the stored
      data.</para>

      <para>The storage layer, called the Hadoop File System (HDFS), looks
      like a single storage volume that has been optimized for many concurrent
      serialized reads of large data files - where "large" might be measured
      in gigabytes or petabytes. However, it does have limitations. For
      example, random access to the data is not really possible in an
      efficient manner. And Hadoop only supports a single writer for output.
      But this limit helps make Hadoop very performant and reliable, in part
      because it allows for the data to be replicated across the cluster,
      reducing the chance of data loss.</para>

      <para>The execution layer, called MapReduce, relies on a
      divide-and-conquer strategy to manage massive data sets and computing
      processes. Explaining MapReduce is beyond the scope of this document,
      but its complexity, and the difficulty of creating real-world
      applications against it, are the chief driving force behind the creation
      of Cascading.</para>

      <para>Hadoop, according to its documentation, can be configured to run
      in three modes: standalone mode (i.e., on the local computer, useful for
      testing and debugging), pseudo-distributed mode (i.e., on an emulated
      "cluster" of one computer, useful for testing), and fully-distributed
      mode (on a full cluster, for production purposes). The
      pseudo-distributed mode does not add value for most purposes, and will
      not be discussed further. Cascading itself supports a local-mode and a
      Hadoop mode, where the Hadoop mode can be in standalone or distributed.
      The primary difference is that in local mode, Cascading makes no use of
      Hadoop API's and performs all of its work in memory, allowing it to be
      very fast - but consequently is not as robust or scaleable as Cascading
      in Hadoop mode, which inherits Hadoop's scalability.</para>

      <para>Apache Hadoop is an Open Source Apache project and is freely
      available. It can be downloaded from the Hadoop website:<link
      xlink:href="http://hadoop.apache.org/core/">http://hadoop.apache.org/core/</link>.</para>
    </section>
  </chapter>

  <chapter>
    <info>
      <title>Diving In</title>
    </info>

    <para>The most common example presented to new Hadoop (and MapReduce)
    developers is an application that counts words. It is the Hadoop
    equivalent to a "Hello World" application.</para>

    <para>In the word-counting application, a document is parsed into
    individual words and the frequency of each word is counted. In the last
    paragraph, for example, "is" appears twice and "equivalent" appears
    once.</para>

    <para>The following code example uses Cascading to read each line of text
    from our document file, parse it into words, then count the number of
    times each word appears.</para>

    <example>
      <title>Word Counting</title>

      <xi:include href="basic-word-count.xml"/>
    </example>

    <para>Several features of this example are worth highlighting.</para>

    <para>First, notice that the pipe assembly is not coupled to the data
    (i.e., the <classname>Tap</classname> instances) until the last moment
    before execution. File paths or references are not embedded in the pipe
    assembly; instead, the pipe assembly is specified independent of data
    inputs and outputs. The only dependency is the data scheme, i.e., the
    field names. In Cascading, every input or output file has field names
    associated with it, and every processing element of the pipe assembly
    either expects the specified fields or creates them. This allows
    developers to easily self-document their code, and allows the Cascading
    planner to "fail fast" if an expected dependency between elements isn't
    satisfied - for instance, if a needed field name is missing or incorrect.
    (If more information is desired on the planner, see <xref
    linkend="job-planner"/> .)</para>

    <para>Also notice that pipe assemblies are assembled through constructor
    chaining. This may seem odd, but it is done for two reasons. First, it
    keeps the code more concise. Second, it prevents developers from creating
    "cycles" (i.e., recursive loops) in the resulting pipe assembly. Pipe
    assemblies are intended to be Directed Acyclic Graphs (DAG's), and in
    keeping with this, the Cascading planner is not designed to handle
    processes that feed themselves. (If desired, there are safer approaches to
    achieving this result.</para>

    <para>Finally, notice that the very first <code>Pipe</code> instance has a
    name. That instance is the <emphasis role="italic">head</emphasis> of this
    particular pipe assembly. Pipe assemblies can have any number of heads,
    and any number of <emphasis role="italic">tails</emphasis>. Although the
    tail in this example does not have a name, in a more complex assembly it
    would. In general, heads and tails of pipe assemblies are assigned names
    to disambiguate them. One reason is that names are used to bind sources
    and sinks to pipes during planning. (The example above is an exception,
    because there is only one head and one tail - and consequently only one
    source and one sink - so the binding is unmistakable.) Another reason is
    that the naming of pipes contributes to self-documention of pipe
    assemblies, especially where there are splits, joins, and merges in the
    assembly.</para>

    <para>To sum up, the example word-counting application will:</para>

    <itemizedlist>
      <listitem>
        <para>Read each line of text from a file and give it the field name
        "line"</para>
      </listitem>

      <listitem>
        <para>parse each "line" into words with the
        <code>RegexGenerator</code> object, which returns each word in the
        field named "word"</para>
      </listitem>

      <listitem>
        <para>sort and group all the tuples on the "word" field, using the
        <code>GroupBy</code> object</para>
      </listitem>

      <listitem>
        <para>count the number of elements in each group, using the
        <code>Count</code> object, and store this value in the "count"
        field</para>
      </listitem>

      <listitem>
        <para>and write out the "word" and "count" fields.</para>
      </listitem>
    </itemizedlist>
  </chapter>

  <chapter>
    <info>
      <title>Data Processing</title>
    </info>

    <section>
      <title>Terminology</title>

      <para>The Cascading processing model is based on a metaphor of pipes
      (data streams) and filters (data operations). Thus the Cascading API
      allows the developer to assemble pipe assemblies that split, merge,
      group, or join streams of data while applying operations to each data
      record or groups of records.</para>

      <para>In Cascading, we call a data record a <emphasis
      role="italic">tuple</emphasis>, a simple chain of pipes without forks or
      merges a <emphasis role="italic">branch</emphasis>, an interconnected
      set of pipe branches a <emphasis>pipe assembly</emphasis>, and a series
      of tuples passing through a pipe branch or assembly a <emphasis
      role="italic">tuple stream</emphasis>.</para>

      <para>Pipe assemblies are specified independently of the data source
      they are to process. So before a pipe assembly can be executed, it must
      be bound to <emphasis role="italic">taps</emphasis>, i.e., data sources
      and sinks. The result of binding one or more pipe assemblies to taps is
      a <emphasis role="italic">flow</emphasis>, which is executed on a
      computer or cluster using the Hadoop framework.</para>

      <para>Multiple flows can be grouped together and executed as a single
      process. In this context, if one flow depends on the output of another,
      it is not executed until all of its data dependencies are satisfied.
      Such a collection of flows is called a <emphasis
      role="italic">cascade</emphasis>.</para>
    </section>

    <section>
      <title>Pipe Assemblies</title>

      <para>Pipe assemblies define what work should be done against tuple
      streams, which are read from tap <emphasis role="italic">sources
      </emphasis> and written to tap <emphasis role="italic">sinks</emphasis>.
      The work performed on the data stream may include actions such as
      filtering, transforming, organizing, and calculating. Pipe assemblies
      may use multiple sources and multiple sinks, and may define splits,
      merges, and joins to manipulate the tuple streams.</para>

      <section>
        <title>Pipe Assembly Workflow</title>

        <para>Pipe assemblies are created by chaining
        <classname>cascading.pipe.Pipe</classname> classes and subclasses
        together. Chaining is accomplished by passing the names of previous
        <classname>Pipe</classname> instances to the constructor of the next
        <classname>Pipe</classname> instance.</para>

        <para>The following example demonstrates this type of chaining. It
        creates two pipes - a "left-hand side" (lhs) and a "right-hand side"
        (rhs) - and performs some processing on them both, using the Each
        pipe. Then it joins the two pipes into one, using the CoGroup pipe,
        and performs several operations on the joined pipe using Every and
        GroupBy. The specific operations performed are not important in the
        example; the point is to show the general flow of the data streams.
        The diagram after the example gives a visual representation of the
        workflow.</para>

        <example>
          <title xreflabel="Chaining Pipes" xml:id="chaining-pipes">Chaining
          Pipes</title>

          <xi:include href="simple-pipe-assembly.xml"/>
        </example>

        <para>The following diagram is a visual representation of the example
        above.</para>

        <mediaobject>
          <imageobject role="fo">
            <imagedata align="center" contentwidth="5.4in"
                       fileref="images/simple-pipe-assembly.svg"/>
          </imageobject>

          <imageobject role="html">
            <imagedata align="center" contentwidth="5.4in"
                       fileref="images/simple-pipe-assembly.png"/>
          </imageobject>
        </mediaobject>
      </section>

      <section>
        <title>Common Stream Patterns</title>

        <para>As data moves through the pipe, streams may be separated or
        combined for various purposes. Here are the three basic
        patterns:</para>

        <variablelist>
          <varlistentry>
            <term>Split</term>

            <listitem>
              <para>A split takes a single stream and sends it down multiple
              paths - that is, it feeds a single <classname>Pipe</classname>
              instance into two or more subsequent separate
              <classname>Pipe</classname> instances. This is done by using the
              <classname>Each</classname> class, or by using named instances
              of the <classname>Pipe</classname> class. (The branch names are
              useful for binding<xref linkend="failure-traps"/>.)</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>Merge</term>

            <listitem>
              <para>A merge combines two or more streams that have identical
              fields into a single stream. This is done by passing two or more
              <classname>Pipe</classname> instances to a
              <classname>Merge</classname> or <classname>GroupBy</classname>
              pipe.</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>Join</term>

            <listitem>
              <para>A join combines data from two or more streams that have
              different fields, based on common field values (analogous to a
              SQL join.) This is done by passing two or more
              <classname>Pipe</classname> instances to a
              <classname>Join</classname> or <classname>CoGroup</classname>
              pipe. The code sequence and diagram above give an
              example.</para>
            </listitem>
          </varlistentry>
        </variablelist>
      </section>

      <section>
        <title>Data Processing</title>

        <para>In addition to directing the tuple streams - using splits,
        merges, and joins - pipe assemblies can examine, filter, organize, and
        transform the tuple data as the streams move through the pipe
        assemblies. To facilitate this, the values in the tuple are typically
        given field names, just as database columns are given names, so that
        they may be referenced or selected. The following terminology is
        used:</para>

        <variablelist>
          <varlistentry>
            <term>Operation</term>

            <listitem>
              <para>Operations
              (<classname>cascading.operation.Operation</classname>) accept an
              input argument Tuple, and output zero or more result tuples.
              There are a few sub-types of operations defined below. Cascading
              has a number of generic Operations that can be used, or
              developers can create their own custom Operations.</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>Tuple</term>

            <listitem>
              <para>In Cascading, data is processed as a stream of Tuples
              (<classname>cascading.tuple.Tuple</classname>), which are
              composed of fields, much like a database record or row. A Tuple
              is effectively an array of (field) values, where each value can
              be any <classname>java.lang.Object</classname> Java type (or
              <code>byte[]</code> array). For information on supporting
              non-primitive types, see <xref linkend="custom-types"/>.</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>Fields</term>

            <listitem>
              <para>Fields (<classname>cascading.tuple.Fields</classname>) are
              used either to declare the field names for fields in a Tuple, or
              reference field values in a Tuple. They can either be strings
              (such as "firstname" or "birthdate"), integers (for the field
              position, starting at<code>0</code>, and using <code>-1</code>
              for the last position), or one of eight predefined Fields "sets"
              (such as<code>Fields.ALL</code>, which selects all values in the
              Tuple, like an asterisk in SQL). For more on Fields sets, see
              <xref linkend="field-algebra"/>).</para>
            </listitem>
          </varlistentry>
        </variablelist>
      </section>
    </section>

    <section>
      <title>Pipes</title>

      <para>The code for the sample pipe assembly above, <xref
      linkend="chaining-pipes"/>, consists almost entirely of a series of
      <classname>Pipe</classname> constructors, which are the subject of this
      section. The base class <classname>cascading.pipe.Pipe</classname> and
      its subclasses, the subject of this section, are shown in the diagram
      below.</para>

      <mediaobject>
        <imageobject role="fo">
          <imagedata align="center" contentwidth="5in"
                     fileref="images/pipes.svg"/>
        </imageobject>

        <imageobject role="html">
          <imagedata align="center" contentwidth="5in"
                     fileref="images/pipes.png"/>
        </imageobject>
      </mediaobject>

      <section>
        <title>Types of Pipes</title>

        <variablelist>
          <para><emphasis role="bold">The <classname>Pipe</classname> class
          </emphasis> is used to instantiate and name a pipe. Pipe names are
          used by the planner to bind taps to the pipe as sources or sinks. (A
          third option is to bind a tap to the pipe branch as a <emphasis
          role="italic">trap</emphasis>, discussed elsewhere as an advanced
          topic.)</para>

          <para><emphasis role="bold">The <classname>SubAssembly</classname>
          subclass </emphasis> is a special type of pipe. It is used to nest
          reusable pipe assemblies within a <classname>Pipe</classname> class
          for inclusion in a larger pipe assembly. For more information on
          this, see the section on <xref linkend="subassemblies"/>.</para>

          <para><emphasis role="bold">The other six types of pipes</emphasis>
          are used to perform operations on the tuple streams as they pass
          through the pipe assemblies. This may involve operating on the
          individual tuples, on groups of related tuples, or on entire
          streams. These six pipe types are introduced here, then explored in
          detail further below.</para>

          <varlistentry>
            <term><classname>Each</classname> and
            <classname>Every</classname></term>

            <listitem>
              <para>These pipes perform operations based on the data contents
              of tuples - analyze, transform, or filter. The
              <classname>Each</classname> pipe operates on individual tuples
              in the stream, applying functions or filters such as search and
              replace, removing tuples that have values outside a target
              range, etc. The <classname>Every</classname> pipe operates on
              groups of tuples that share a common value (such as all the
              tuples for a particular date, or zipcode), applying aggregator
              or buffer operations such as counting, totaling, or averaging
              field values within each group.</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term><classname>Merge</classname> and
            <classname>Join</classname></term>

            <listitem>
              <para>These pipes combine two or more tuple streams into
              one.</para>

              <para>A <emphasis role="italic">Merge</emphasis> accepts two or
              more streams that have identical fields, and outputs a single
              stream of tuples (in arbitrary order) that contains all the
              tuples from all the specified input streams. Thus a Merge is
              just a mingling of all the tuples from the input streams, as if
              shuffling multiple card decks into one. A sample use case might
              be to merge the Apache access logs from an entire server farm
              onto a single directory in the Hadoop file system.</para>

              <para>A <emphasis role="italic">Join</emphasis> is a logical
              joining of tuples from two or more streams into a stream of
              hybrid tuples based on matching field values. It is analogous to
              a SQL join. With a join, the tuples in the different source
              streams typically do not contain the same field names, but at
              least one field must be used from each stream for the purpose of
              joining, regardless of the field name. (By default, the fields
              must be mutually comparable for matching purposes.) One sample
              use case would be to combine the values from a customer file, a
              sales order file, and a shipped orders file into a single tuple
              stream for analysis.</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term><classname>GroupBy</classname> and
            <classname>CoGroup</classname></term>

            <listitem>
              <para>These pipes extend the functionality of Merge and Join,
              adding the ability to sort the tuples on field values and
              express them via the Group interface (described in the Javadoc).
              This makes the output suitable for the Every pipe, which
              performs aggregator and buffer operations, such as averaging
              values in a group of tuples that have the same field
              value.</para>

              <para><emphasis role="italic">GroupBy</emphasis> sorts a stream
              and expresses the results via the Group interface. If passed
              multiple streams as inputs, it performs a Merge before the sort.
              (Recall that Merge requires that input streams share the same
              field structure.)</para>

              <para><emphasis role="italic">CoGroup</emphasis> performs a Join
              and sorts the results, expressing the resulting stream via the
              Group interface. As with a Join, the resulting output tuples
              will contain fields from all the input streams.</para>
            </listitem>
          </varlistentry>
        </variablelist>

        <para>The following table summarizes the different types of
        pipes.</para>

        <table colsep="1" rowsep="1">
          <title>Comparison of pipe types</title>

          <tgroup cols="4">
            <colspec colname="c1" colnum="1" colwidth="90px"/>

            <colspec colname="c2" colnum="2" colwidth="1.08*"/>

            <colspec colname="c3" colnum="3" colwidth="1*"/>

            <colspec colname="c4" colnum="4" colwidth="2.36*"/>

            <tbody>
              <row>
                <entry><emphasis role="bold"> <emphasis role="underline">Pipe
                type </emphasis> </emphasis></entry>

                <entry><emphasis role="underline"> <emphasis
                role="bold">Purpose </emphasis> </emphasis></entry>

                <entry><emphasis role="underline"> <emphasis role="bold">Input
                </emphasis> </emphasis></entry>

                <entry><emphasis role="underline"> <emphasis
                role="bold">Output </emphasis> </emphasis></entry>
              </row>

              <row>
                <entry><classname>Pipe</classname></entry>

                <entry>instantiate a pipe; name a pipe</entry>

                <entry>name</entry>

                <entry>a (named) pipe</entry>
              </row>

              <row>
                <entry><classname>SubAssembly</classname></entry>

                <entry>create nested subassemblies</entry>

                <entry/>

                <entry/>
              </row>

              <row>
                <entry><classname>Each</classname></entry>

                <entry>apply a filter or function; create a split</entry>

                <entry>tuple stream (grouped or not)</entry>

                <entry>the tuple stream, filtered or transformed; field list
                may be altered</entry>
              </row>

              <row>
                <entry><classname>Every</classname></entry>

                <entry>apply aggregator or buffer</entry>

                <entry>grouped stream</entry>

                <entry>the tuple stream (field list may be altered) plus new
                fields with operation results</entry>
              </row>

              <row>
                <entry><classname>Merge</classname></entry>

                <entry>merge 2 or more streams with identical fields</entry>

                <entry>2 or more tuple streams, any order</entry>

                <entry>tuple stream, unsorted;</entry>
              </row>

              <row>
                <entry><classname>Join</classname></entry>

                <entry>join 2 streams on a matching field value</entry>

                <entry>2 or more tuple streams (ordered?)</entry>

                <entry>tuple stream, unsorted; field list may be
                altered</entry>
              </row>

              <row>
                <entry><classname>GroupBy</classname></entry>

                <entry>sort/group on field values; optionally merge 2 or more
                streams with identical fields</entry>

                <entry>1 or more tuple streams</entry>

                <entry>sorted/grouped tuples; optional secondary sort</entry>
              </row>

              <row>
                <entry><classname>CoGroup</classname></entry>

                <entry>join 2 streams on a matching field value</entry>

                <entry>2 or more tuple streams</entry>

                <entry>joined, sorted/grouped tuples</entry>
              </row>
            </tbody>
          </tgroup>
        </table>
      </section>

      <section>
        <title xreflabel="Each and Every Pipes" xml:id="each-every">Each and
        Every Pipes</title>

        <para>The <classname>Each</classname> and <classname>Every</classname>
        pipes perform operations on tuple data - for instance, perform a
        search-and-replace on tuple contents, filter out some of the tuples
        based on their contents, or count the number of tuples in a stream
        that share a common field value.</para>

        <para>Here is the syntax for these pipes:</para>

        <para><programlisting>new Each( previousPipe, argumentSelector, operation, outputSelector )</programlisting></para>

        <para><programlisting>new Every( previousPipe, argumentSelector, operation, outputSelector )</programlisting></para>

        <para>Both types take four arguments:</para>

        <para><itemizedlist>
            <listitem>
              <para>a Pipe instance</para>
            </listitem>

            <listitem>
              <para>an argument selector</para>
            </listitem>

            <listitem>
              <para>an Operation instance</para>
            </listitem>

            <listitem>
              <para>an output selector on the constructor (selectors here are
              Fields instances)</para>
            </listitem>
          </itemizedlist></para>

        <para>The key difference between Each and Every is that one operates
        on individual tuples, and the other operates on sorted groups of
        tuples expressed through the Group interface (described in the
        Javadoc). This affects the kind of operations that these two pipes can
        perform, and the kind of output they produce as a result.</para>

        <para>The <emphasis role="italic">Each</emphasis> pipe acts on one
        tuple at a time, applying operations that are subclasses of
        <classname>Functions</classname> and <classname>Filters</classname>
        (described in the Javadoc). For example, using Each pipes you can
        split a line from a logfile into multiple fields, filter out
        everything but the HTTP GET requests, convert timestring fields into
        date and time fields, and keep only the dates. The Each pipe outputs
        the stream of resulting tuples after the filter or function has been
        applied.</para>

        <para>Similarly, since the <emphasis role="italic">Every</emphasis>
        pipe works on groups of tuples, it applies operations that are
        subclasses of <classname>Aggregators</classname> and
        <classname>Buffers</classname>. These operate on groups of tuples -
        i.e., the output of a <classname>GroupBy</classname> or
        <classname>CoGroup</classname> pipe - one group at a time. For
        example, an Every pipe could accept the output of the above Each pipes
        and count the page requests by date. In this example, the pipe would
        output the operation results as the date and the count for each group,
        appended to each of the input tuples that are in that group.</para>

        <mediaobject>
          <imageobject role="fo">
            <imagedata align="center" contentwidth="6in"
                       fileref="images/pipe-operation-relationship.svg"/>
          </imageobject>

          <imageobject role="html">
            <imagedata align="center" contentwidth="6in"
                       fileref="images/pipe-operation-relationship.png"/>
          </imageobject>
        </mediaobject>

        <para>In the syntax shown at the start of this section, the <emphasis
        role="italic">argument selector </emphasis> specifies fields from the
        input Tuple to use as input values. If the argument selector is not
        specified, the whole input Tuple (<code>Fields.ALL</code>) is passed
        to the Operation as a set of argument values.</para>

        <para>Most Operations declare result fields (shown as "declared
        fields" in the diagram). The <emphasis role="italic">output
        selector</emphasis> specifies the fields of the output Tuple from the
        fields of the input Tuple and the Operation result. This new output
        Tuple becomes the input Tuple to the next pipe in the pipe assembly.
        If the output selector is <code>Fields.ALL</code>, the output Tuple is
        the input Tuple plus the Operation result, merged into a single
        Tuple.</para>

        <para>Note that it's possible for a <classname>Function</classname> or
        <classname>Aggregator</classname> to return more than one output Tuple
        per input Tuple. In this case, the input Tuple is duplicated as many
        times as necessary to create the necessary output Tuples. This is
        similar to the reiteration of values that happens during a join. If a
        Function is designed to always emit three result Tuples for every
        input Tuple, each of the three outgoing Tuples will consist of the
        selected input Tuple values plus one of the three sets of Function
        result values.</para>

        <mediaobject>
          <imageobject role="fo">
            <imagedata align="center" contentwidth="6in"
                       fileref="images/each-operation-relationship.svg"/>
          </imageobject>

          <imageobject role="html">
            <imagedata align="center" contentwidth="6in"
                       fileref="images/each-operation-relationship.png"/>
          </imageobject>
        </mediaobject>

        <para>If the result selector is not specified for an
        <classname>Each</classname> pipe performing a
        <classname>Functions</classname> operation, the Operation results are
        returned by default (<code>Fields.RESULTS</code>), discarding the
        input Tuple values in the tuple stream. (This is not true of
        <classname>Filters</classname> , which either discard the input Tuple
        or return it intact, and thus do not use an output selector.)</para>

        <mediaobject>
          <imageobject role="fo">
            <imagedata align="center" contentwidth="6in"
                       fileref="images/every-operation-relationship.svg"/>
          </imageobject>

          <imageobject role="html">
            <imagedata align="center" contentwidth="6in"
                       fileref="images/every-operation-relationship.png"/>
          </imageobject>
        </mediaobject>

        <para>For the <classname>Every</classname> pipe, the Aggregator
        results are appended to the input Tuple (<code>Fields.ALL</code>) by
        default.</para>

        <para>Note that the <classname>Every</classname> pipe associates
        Aggregator results with the current group Tuple (they unique keys
        currently being grouped on). For example, if you are grouping on the
        field "department" and counting the number of "names" grouped by that
        department, the resulting output Fields will be
        ["department","num_employees"].</para>

        <para>If you are also adding up the salaries associated with each
        "name" in each "department", the output Fields will be
        ["department","num_employees","total_salaries"]. This is only true for
        chains of <classname>Aggregator</classname> Operations - you are not
        allowed to chain <classname>Buffer</classname> Operations.</para>

        <mediaobject>
          <imageobject role="fo">
            <imagedata align="center" contentwidth="6in"
                       fileref="images/buffer-operation-relationship.svg"/>
          </imageobject>

          <imageobject role="html">
            <imagedata align="center" contentwidth="6in"
                       fileref="images/buffer-operation-relationship.png"/>
          </imageobject>
        </mediaobject>

        <para>When the <classname>Every</classname> pipe is used with a
        <classname>Buffer</classname>, instead of an
        <classname>Aggregator</classname>, the behavior is different.</para>

        <para>Instead of being associated with the current grouping Tuple, the
        Buffer results are associated with the current values Tuple. This is
        analogous to how an <classname>Each</classname> pipe works with a
        <classname>Function</classname>. This approach may seem slightly
        unintuitive, but provides much more flexibility. To put it another
        way, the results of the Buffer are not appended to the current keys
        being grouped on. It is up to the buffer to emit them if they are
        relevant. It is also possible for a Buffer to emit more than one
        result Tuple per unique grouping. That is, a Buffer may or may not
        emulate an Aggregator, where an Aggregator is just a special optmized
        case of a Buffer.</para>

        <para>For more information on how operations process fields, see <xref
        linkend="field-processing"/>.</para>
      </section>

      <section>
        <title>Merge and Join Pipes</title>

        <para>The <classname>Merge</classname> and <classname>Join</classname>
        pipes combine two or more tuple streams into one.</para>

        <section>
          <title>The Merge Pipe</title>

          <para>A <emphasis role="italic">Merge</emphasis> accepts two or more
          streams that have identical fields, and outputs a single stream of
          tuples (in arbitrary order) that contains all the tuples from all
          the specified input streams. Thus a Merge is just a mingling of all
          the tuples from the input streams, as if shuffling multiple card
          decks into one. A sample use case might be to merge the Apache
          access logs from an entire server farm onto a single directory in
          the Hadoop file system.</para>

          <example>
            <title>Merging Two Tuple Streams</title>

            <xi:include href="simple-merge.xml"/>
          </example>

          <para>The example above simply combines all the tuples from two
          existing streams into a new tuple stream. The resulting stream is in
          arbitrary sequence.</para>
        </section>

        <section>
          <title>The Join Pipe</title>

          <para>A <emphasis role="italic">Join</emphasis> is a logical joining
          of tuples from two or more streams into a stream of hybrid tuples
          based on matching field values. It is analogous to a SQL join, and
          can be an inner, outer, left, or right join. With a join, the tuples
          in the different source streams do not typically contain the same
          field lists, but must have at least one field in common (for the
          purpose of matching). One sample use case would be to combine the
          values from a customer file, a sales order file, and a shipped
          orders file into a single tuple stream for analysis; this would be
          done by joining the customer and sales streams on the unique
          customer ID, and joining the result with the shipped orders on the
          order number.</para>

          <example>
            <title>Joining Two Tuple Streams</title>

            <xi:include href="simple-join.xml"/>
          </example>

          <para>The example above performs an inner join on two streams ("lhs"
          and "rhs"), based on common values in two fields. The fieldnames
          that are specified in <classname>lhsFields</classname> and
          <classname>rhsFields</classname> are among the field names
          previously declared for the two input streams.</para>

          <section>
            <title>Field Names</title>

            <para>Note that all the field names used in any tuple must be
            unique; duplicates are not allowed. This includes tuples created
            by the Join operation, which leads to a possible problem, shown in
            the following diagram.</para>

            <mediaobject>
              <imageobject role="fo">
                <imagedata align="center" contentwidth="5in"
                           fileref="images/cogrouping-fields-fail.svg"/>
              </imageobject>

              <imageobject role="html">
                <imagedata align="center" contentwidth="5in"
                           fileref="images/cogrouping-fields-fail.png"/>
              </imageobject>
            </mediaobject>

            <para>In this figure, two streams are to be joined on the "url"
            field, resulting in a new Tuple that contains fields from the two
            input tuples. However, the resulting tuple would include two
            fields with the same name ("url"), which is unworkable. To handle
            the conflict, developers can use the <classname>Join</classname>
            pipe's <parameter>declaredFields</parameter> argument (described
            in the Javadoc) to declare unique field names for the output
            tuple.</para>

            <example>
              <title>Joining Two Tuple Streams with Duplicate Field
              Names</title>

              <xi:include href="duplicate-join.xml"/>
            </example>

            <mediaobject>
              <imageobject role="fo">
                <imagedata align="center" contentwidth="5in"
                           fileref="images/cogrouping-fields-pass.svg"/>
              </imageobject>

              <imageobject role="html">
                <imagedata align="center" contentwidth="5in"
                           fileref="images/cogrouping-fields-pass.png"/>
              </imageobject>
            </mediaobject>

            <para>This revised figure demonstrates the use of declared field
            names to prevent a planning failure.</para>

            <para>It might seem preferable for Cascading to automatically
            recognize the duplication and simply merge the identically-named
            fields, saving effort for the developer. However, during an outer
            type join, one field (or set of fields used for the join) for a
            given join side may be <code>null</code>. Discarding one of the
            duplicate fields would lose this information.</para>

            <para>Further, the internal implementation relies on field
            position, not field names, when reading tuples; the field names
            are a device for the developer. This approach is beneficial in
            terms of execution speed, readability, and abstraction of
            subassemblies from specific field names.</para>
          </section>

          <section>
            <title>The Joiner Class</title>

            <para>In the example above, we explicitly specified a Joiner class
            to perform a join on our data. The reason the
            <classname>CoGroup</classname> pipe is named "CoGroup", and not
            "Join", is because the joining of data is done after all the
            parallel streams are co-grouped by their common keys. A detailed
            explanation is beyond the scope of this document. But in brief,
            before a join can be performed, Cascading must create a "bag" of
            data (borrowing from Pig terminology) for every input tuple
            stream, consisting of all the Tuple instances associated with a
            given grouping.</para>

            <mediaobject>
              <imageobject role="fo">
                <imagedata align="center" contentwidth="5in"
                           fileref="images/cogrouped-values.svg"/>
              </imageobject>

              <imageobject role="html">
                <imagedata align="center" contentwidth="5in"
                           fileref="images/cogrouped-values.png"/>
              </imageobject>
            </mediaobject>

            <para>The most commonly-used type of join is the inner join, which
            tries to match <emphasis role="italic">each</emphasis> Tuple on
            the "lhs" with <emphasis role="italic">every</emphasis> Tuple on
            the "rhs", based on field values. This is the default behavior for
            a join in SQL. With an inner join, if one of the bags is empty, no
            tuples are joined. An outer join, conversely, allows for either
            bag to be empty and simply substitutes a Tuple containing
            <code>null</code> values for the non-existent tuple.</para>

            <mediaobject>
              <imageobject role="fo">
                <imagedata align="center" contentwidth="3.5in"
                           fileref="images/joins.svg"/>
              </imageobject>

              <imageobject role="html">
                <imagedata align="center" contentwidth="3.5in"
                           fileref="images/joins.png"/>
              </imageobject>
            </mediaobject>

            <para>The diagram above shows all the supported Joiner
            types.</para>

            <para><programlisting>LHS = [0,a] [1,b] [2,c]
              RHS = [0,A] [2,C] [3,D]</programlisting>Using the above simple
            data sets, we define each join type below, where the values are
            joined on the first position, a numeric value. Note that, when
            Cascading joins tuples, the resulting Tuple contains all the
            incoming values. The duplicate common key(s) is not discarded if
            given. And on outer joins, where there is no equivalent key in the
            alternate stream, <code>null</code> values are used as
            placeholders. <variablelist>
                <varlistentry>
                  <term>InnerJoin</term>

                  <listitem>
                    <para>An inner join only returns a joined Tuple if neither
                    bag is empty. <programlisting>[0,a,0,A] [2,c,2,C] </programlisting></para>
                  </listitem>
                </varlistentry>

                <varlistentry>
                  <term>OuterJoin</term>

                  <listitem>
                    <para>An outer join performs a join if one bag (left or
                    right) is empty, or if neither bag is empty.
                    <programlisting>[0,a,0,A] [1,b,null,null] [2,c,2,C] [null,null,3,D] </programlisting></para>
                  </listitem>
                </varlistentry>

                <varlistentry>
                  <term>LeftJoin</term>

                  <listitem>
                    <para>A left join can also be stated as a left inner and
                    right outer join, where it is acceptable for the right bag
                    to be empty (but not the left).</para>

                    <programlisting>[0,a,0,A] [1,b,null,null] [2,c,2,C]</programlisting>
                  </listitem>
                </varlistentry>

                <varlistentry>
                  <term>RightJoin</term>

                  <listitem>
                    <para>A right join can also be stated as a left outer and
                    right inner join, where it is acceptable for the left bag
                    to be empty (but not the right). <programlisting>[0,a,0,A] [2,c,2,C] [null,null,3,D] </programlisting></para>
                  </listitem>
                </varlistentry>

                <varlistentry>
                  <term>MixedJoin</term>

                  <listitem>
                    <para>A mixed join is where 3 or more tuple streams are
                    joined, using a small Boolean array to specify each of the
                    join types to use. For more information, see the
                    <classname>cascading.pipe.cogroup.MixedJoin</classname>
                    class in the Javadoc.</para>
                  </listitem>
                </varlistentry>

                <varlistentry>
                  <term><emphasis>Custom</emphasis></term>

                  <listitem>
                    <para>Developers can subclass the
                    <classname>cascading.pipe.cogroup.Joiner</classname> class
                    to create custom join operations.</para>
                  </listitem>
                </varlistentry>
              </variablelist></para>
          </section>

          <section>
            <title><?oxy_comment_start author="Jim" timestamp="20120402T014844-0700" comment="Chris - not sure I've got this quite right..."?>Memory
            considerations<?oxy_comment_end ?></title>

            <para>During a join of two streams, the entire current tuple group
            of the right-hand stream is stored in memory for rapid comparison.
            If the group is very large, it may exceed a configurable spill
            threshold and be spilled to disk, reducing performance. For this
            reason, alway use the shorter stream or the stream with shorter
            groups on the right-hand side. If necessary, adjust the spill
            threshold as described in the Javadoc.</para>
          </section>
        </section>
      </section>

      <section>
        <title>GroupBy and CoGroup Pipes</title>

        <section>
          <title>Basic Operation</title>

          <para>The <classname>GroupBy</classname> and
          <classname>CoGroup</classname> pipes extend the functionality of
          <classname>Merge</classname> and<classname>Join</classname>, adding
          the ability to logically group the tuples on field values and
          express them via the Group interface (described in the Javadoc).
          This makes the output suitable for the <classname>Every</classname>
          pipe, which performs <classname>Aggregator</classname> and
          <classname>Buffer</classname> operations, such as averaging values
          in a group of tuples that have the same field value.</para>

          <para><emphasis role="italic">GroupBy</emphasis> sorts an input
          stream and expresses the results via the Group interface. If passed
          multiple input streams, it performs a Merge before the sort. (Recall
          that Merge requires that input streams share the same field
          structure.)</para>

          <para><emphasis role="italic">CoGroup</emphasis> performs a Join and
          sorts the results, expressing the resulting stream via the Group
          interface. As with a Join, the resulting output tuples will contain
          fields from all the input streams</para>

          <para>At the end of a <classname>GroupBy</classname> or
          <classname>CoGroup</classname> operation, the resulting groups of
          tuples are sorted by the designated grouping values, and then are
          passed to an <classname>Aggregator</classname> or
          <classname>Buffer</classname> operation in that sequence. However,
          by default there is no secondary sort, so within each group the
          tuples are in arbitrary order. For instance, when grouping on
          "lastname", the tuples <code>[doe, john]</code> and <code>[doe,
          jane] </code> end up in the same group, but in arbitrary sequence
          (e.g., not necessarily sorted by the "firstname" values).</para>
        </section>

        <section>
          <title>Secondary Sorting</title>

          <para>If multi-level sorting is desired, the names of the sort
          fields on must be specified to the GroupBy instance, as seen below.
          In this example, <code>value1</code> and <code>value2</code> will
          arrive in their natural sort order (assuming they are
          <classname>java.lang.Comparable</classname>).</para>

          <example>
            <title>Secondary Sorting</title>

            <xi:include href="simple-groupby-secondary.xml"/>
          </example>

          <para>If we don't care about the order of<code>value2</code>, we can
          leave it out of the <code>sortFields</code>
          <classname>Fields</classname> constructor.</para>

          <para>In the next example, we reverse the order of
          <code>value1</code> while keeping the natural order of
          <code>value2</code>.</para>

          <example>
            <title>Reversing Secondary Sort Order</title>

            <xi:include href="simple-groupby-secondary-comparator.xml"/>
          </example>

          <para>Whenever there is an implied sort during grouping or secondary
          sorting, a custom <classname>java.util.Comparator</classname> can
          optionally be supplied to the grouping <classname>Fields</classname>
          or secondary sort <classname>Fields</classname>, in order to
          influence the sorting through the
          <code>Fields.setComparator()</code> call.</para>

          <para>To sort or group on non- <classname>Comparable</classname>
          classes, consider creating a custom
          <classname>Comparator</classname>.</para>

          <para>Below is a more practical example, where we group by the "day
          of the year", but want to reverse the order of the tuples within
          that grouping by "time of day".</para>

          <example>
            <title>Reverse Order by Time</title>

            <xi:include href="simple-groupby-secondary-time.xml"/>
          </example>
        </section>
      </section>

      <section>
        <title>Setting Custom Pipe Properties</title>

        <para>The Cascading planner assigns the same properties to all pipes
        in a flow, based on the defaults for that flow. However, it's possible
        to override the default properties by using
        <classname>xxx</classname>.</para>

        <para>The following code sample demonstrates the basic form.</para>

        <para/>

        <para>Below are some examples of properties that you might wish to
        override. For complete lists of flow and pipe properties, consult the
        Javadoc.</para>

        <para><variablelist>
            <varlistentry>
              <term/>

              <listitem>
                <para/>
              </listitem>
            </varlistentry>
          </variablelist> <variablelist>
            <varlistentry>
              <term/>

              <listitem>
                <para/>
              </listitem>
            </varlistentry>
          </variablelist> <variablelist>
            <varlistentry>
              <term/>

              <listitem>
                <para/>
              </listitem>
            </varlistentry>
          </variablelist></para>

        <para>A final cautionary or encouraging note about using this
        feature... For more information, see...</para>
      </section>
    </section>

    <section>
      <title>Source and Sink taps</title>

      <para>All input data comes in from, and all output data goes out to,
      some instance of <classname>cascading.tap.tap</classname>. A tap
      represents a data resource - such as a file on the local file system, on
      a Hadoop distributed file system, or on Amazon S3. A tap can be read
      from, which makes it a <emphasis role="italic">source</emphasis>, or
      written to, which makes it a <emphasis role="italic">sink</emphasis>.
      Or, more commonly, taps act as both sinks and sources when shared
      between Flows.</para>

      <para>Depending on your operating mode (Cascading local mode or Hadoop
      mode), the specific classes you use may vary. Details are provided in
      the sections below.</para>

      <section>
        <title>Schemes</title>

        <para>If the Tap is about where the data is and how to access it, the
        Scheme is about what the data is and how to read it. Every Tap must
        have a Scheme that describes the data. Cascading provides four Scheme
        classes:</para>

        <variablelist>
          <varlistentry>
            <term>TextLine</term>

            <listitem>
              <para>TextLine reads and writes raw text files and returns
              tuples with two field names by default, specfic to the platform
              used. Either the bytes offset or line number, and the actual
              line of text. When written to, all Tuple values are converted to
              Strings delimited with the TAB character (\t).</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>TextDelimited</term>

            <listitem>
              <para>TextDelimited reads and writes character-delimited files
              in standard formats such as CSV (comma-separated variables), TSV
              (tab-separated variables), and so on. When written to, all Tuple
              values are converted to Strings and joined with the specified
              character delimiter. This Scheme can optionally handle quoted
              values with custom quote characters. Further, TextDelimited can
              coerce each value to a primitive type when reading a text
              file.</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>SequenceFile</term>

            <listitem>
              <para>SequenceFile is based on the Hadoop Sequence file, which
              is a binary format. When written to or read from, all Tuple
              values are saved in their native binary form. This is the most
              efficient file format - but be aware that the resulting files
              are binary and can only be read by Hadoop applications in Hadoop
              mode.</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>WritableSequenceFile</term>

            <listitem>
              <para>WritableSequenceFile is based on the Hadoop Sequence file,
              like the SequenceFile Scheme, but was designed to read and write
              key and/or value Hadoop <classname>Writable</classname> objects
              directly. This is very useful if you have sequence files created
              by other applications. During writing (sinking), specified key
              and/or value fields are serialized directly into the sequence
              file. During reading (sourcing), the key and/or value objects
              are deserialized and wrapped in a Cascading Tuple object and
              passed to the downstream pipe assembly. This class is only
              available in Hadoop mode.</para>
            </listitem>
          </varlistentry>
        </variablelist>

        <para>There's a key difference between the
        <classname>TextLine</classname> and
        <classname>SequenceFile</classname> schemes. With the
        <classname>SequenceFile</classname> scheme, data is stored as binary
        tuples, which can be read without having to be parsed. But with the
        <classname>TextLine</classname> option, Cascading must parse each line
        into a <classname>Tuple</classname> before processing it, causing a
        performance hit.</para>

        <section>
          <title>Mode-specific implementation details</title>

          <para>Depending on your operating mode (Cascading local mode or
          Hadoop mode), the classes you use to specify schemes will vary.
          Mode-specific details for each standard scheme are shown
          below.</para>

          <table frame="all">
            <title>Mode-specific tap scheme classes</title>

            <tgroup cols="3">
              <colspec colname="c1" colnum="1" colwidth="1.0*"/>

              <colspec colname="c2" colnum="2" colwidth="1.0*"/>

              <colspec colname="c3" colnum="3" colwidth="1.0*"/>

              <tbody>
                <row>
                  <entry><emphasis role="bold">Description</emphasis></entry>

                  <entry><emphasis role="bold">Local mode </emphasis></entry>

                  <entry><emphasis role="bold">Hadoop mode</emphasis></entry>
                </row>

                <row>
                  <entry>Read lines of text</entry>

                  <entry><classname>cascading.scheme.local.TextLine</classname></entry>

                  <entry><classname>cascading.scheme.hadoop.TextLine</classname></entry>
                </row>

                <row>
                  <entry>Read delimited text (CSV, TSV, etc)</entry>

                  <entry><classname>cascading.scheme.local.TextDelimited</classname></entry>

                  <entry><classname>cascading.scheme.hadoop.TextDelimited</classname></entry>
                </row>

                <row>
                  <entry>Cascading proprietary efficient binary</entry>

                  <entry><classname/></entry>

                  <entry><classname>cascading.scheme.hadoop.SequenceFile</classname></entry>
                </row>

                <row>
                  <entry>External Hadoop application binary (custom
                  <classname>Writable</classname> type)</entry>

                  <entry><classname/></entry>

                  <entry><classname>cascading.scheme.hadoop.WritableSequenceFile</classname></entry>
                </row>
              </tbody>
            </tgroup>
          </table>
        </section>

        <section>
          <title>Sequence File Compression</title>

          <para>For best performance when operating in Hadoop mode, enable
          Sequence File Compression in the Hadoop property settings - either
          block or record-based compression. Refer to the Hadoop documentation
          for the available properties and compression types.</para>
        </section>
      </section>

      <section>
        <title>Taps</title>

        <para>The following sample code creates a new Hadoop FileSystem Tap
        that can read and write raw text files. Since only one field name is
        provided, the "offset" field is discarded, resulting in an input tuple
        stream with only "line" values.</para>

        <example>
          <title>Creating a new tap</title>

          <xi:include href="simple-tap.xml"/>
        </example>

        <para>Here are the most commonly-used tap types:</para>

        <variablelist>
          <varlistentry>
            <term>FileTap</term>

            <listitem>
              <para>The <classname>cascading.tap.local.FileTap</classname> tap
              is used with Cascading local mode to access files on the local
              filesystem.</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>Hfs</term>

            <listitem>
              <para>The <classname>cascading.tap.hadoop.Hfs</classname> tap
              uses the current Hadoop default file system, in Hadoop mode. If
              Hadoop is configured for "local mode" its default file system is
              the local file system. If configured for distributed mode, the
              default file system is typically the Hadoop distributed file
              system. The Hfs is convenient when writing Cascading
              applications that may or may not be run on a cluster.
              <classname>Lhs</classname> and <classname>Dfs</classname>
              subclass the <classname>Hfs</classname> tap. Note, specifing a
              prefix to the URL passed into a new Hfs tap, like
              "s3://somebucket/path" will force Hadoop to use the S3
              <classname>FileSystem</classname> implementation to access files
              in an Amazon S3 bucket.</para>
            </listitem>
          </varlistentry>
        </variablelist>

        <para>Also provided are four utility taps:</para>

        <variablelist>
          <varlistentry>
            <term>MultiSourcetap</term>

            <listitem>
              <para>The <classname>cascading.tap.MultiSourcetap</classname> is
              used to tie multiple tap instances into a single tap for use as
              an input source. The only restriction is that all the tap
              instances passed to a new MultiSourcetap share the same Scheme
              classes (not necessarily the same Scheme instance).</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>MultiSinktap</term>

            <listitem>
              <para>The <classname>cascading.tap.MultiSinktap</classname> is
              used to tie multiple tap instances into a single tap for use as
              output sinks. At runtime, for every Tuple output by the pipe
              assembly, each child tap to the MultiSinktap will sink the
              Tuple.</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>Templatetap</term>

            <listitem>
              <para>The <classname>cascading.tap.Templatetap</classname> is
              used to sink tuples into directory paths based on the values in
              the Tuple. More can be read below in <xref
              linkend="template-tap"/>. Only available in Hadoop mode.</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>GlobHfs</term>

            <listitem>
              <para>The <classname>cascading.tap.GlobHfs</classname> tap
              accepts Hadoop style "file globbing" expression patterns. This
              allows for multiple paths to be used as a single source, where
              all paths match the given pattern. Only available in Hadoop
              mode.</para>
            </listitem>
          </varlistentry>
        </variablelist>

        <section>
          <title>Mode-specific implementation details</title>

          <para>Depending on your operating mode, local or Hadoop mode, the
          classes you use to specify file systems will vary. Mode-specifric
          details for each tap type are shown below.</para>

          <table frame="all">
            <title>Mode-specific details for setting file system</title>

            <tgroup cols="4">
              <colspec colname="c1" colwidth="1*"/>

              <colspec/>

              <colspec colname="c2" colwidth="3.4*"/>

              <tbody>
                <row>
                  <entry><emphasis role="bold">Description</emphasis></entry>

                  <entry><emphasis role="bold">Either mode</emphasis></entry>

                  <entry><emphasis role="bold">Local mode</emphasis></entry>

                  <entry><emphasis role="bold">Hadoop Mode</emphasis></entry>
                </row>

                <row>
                  <entry>File access</entry>

                  <entry/>

                  <entry><classname>cascading.tap.local.FileTap</classname></entry>

                  <entry><classname>cascading.tap.hadoop.Hfs</classname></entry>
                </row>

                <row>
                  <entry>Multiple Taps as single source</entry>

                  <entry><classname>cascading.tap.MultiSourceTap</classname></entry>

                  <entry/>

                  <entry/>
                </row>

                <row>
                  <entry>Multiple Taps as single sink</entry>

                  <entry><classname>cascading.tap.MultiSinkTap</classname></entry>

                  <entry/>

                  <entry/>
                </row>

                <row>
                  <entry>Bin/Partition data into multiple files</entry>

                  <entry/>

                  <entry/>

                  <entry><classname>cascading.tap.hadoop.TemplateTap</classname></entry>
                </row>

                <row>
                  <entry>Pattern match mulitiple files/dirs</entry>

                  <entry/>

                  <entry/>

                  <entry><classname>cascading.tap.hadoop.GlobHfs</classname></entry>
                </row>
              </tbody>
            </tgroup>
          </table>
        </section>
      </section>
    </section>

    <section>
      <title>Sink modes</title>

      <para><example>
          <title>Overwriting An Existing Resource</title>

          <xi:include href="simple-replace-tap.xml"/>
        </example></para>

      <para>All applications created with Cascading read data from one or more
      sources, process it, then write data to one or more sinks. This is done
      via the various <classname>Tap</classname> classes, where each class
      abstracts different types of back-end systems that store data as files,
      tables, blobs, or whatever. But to sink data, some systems require that
      the resource (a file for example) not exist before processing. That is,
      it must be removed (deleted) before the processing can begin. Other
      systems my allow for appending or updating of a resource (typical with
      database tables).</para>

      <para>When creating a new <classname>Tap</classname> instance, a
      <classname>SinkMode</classname> may be provided so that the Tap will
      know how to handle any existing resources. Note not all Taps support all
      <classname>SinkMode</classname> values, for example Hadoop does not
      support appends (update) from a MapReduce job.</para>

      <para>The available SinkModes are:</para>

      <variablelist>
        <varlistentry>
          <term><classname>SinkMode.KEEP</classname></term>

          <listitem>
            <para>This is the default behavior. If the resource exists,
            attempting to write to it will fail.</para>
          </listitem>
        </varlistentry>

        <varlistentry>
          <term><classname>SinkMode.REPLACE</classname></term>

          <listitem>
            <para>This allows Cascading to delete the file immediately after
            the Flow is started.</para>
          </listitem>
        </varlistentry>

        <varlistentry>
          <term><classname>SinkMode.UPDATE</classname></term>

          <listitem>
            <para>Allows for new tap types that can update or append - for
            example, to update or add records in a database. Each tap may
            implement this functionality in its own way. Cascading recognizes
            this update mode, and if a resource exists, will not fail or
            attempt to delete it.</para>
          </listitem>
        </varlistentry>
      </variablelist>

      <para>Note that Cascading itself only uses these labels internally to
      know when to automatically call
      <methodname>deleteResource()</methodname> on the
      <classname>Tap</classname> or to leave the Tap alone. It is up the the
      <classname>Tap</classname> implementation to actually perform a write or
      update when processing starts. Thus, when
      <methodname>start()</methodname> or <methodname>complete()</methodname>
      is called on a <classname>Flow</classname>, any sink
      <classname>Tap</classname> labeled
      <classname>SinkMode.REPLACE</classname> will have its
      <methodname>deleteResource()</methodname> method called. If a
      <classname>Flow</classname> is in a <classname>Cascade</classname>,
      <methodname>deleteResource()</methodname> will be called if the
      <classname>Tap</classname> is set to
      <classname>SinkMode.KEEP</classname> or
      <classname>SinkMode.REPLACE</classname> if and only if the sink is stale
      (older) than the source (newer). This allows a
      <classname>Cascade</classname> to behave like a "make" or "ant" build
      file, only running Flows that should be run.</para>

      <para>It's important to understand how Hadoop deals with directories.
      Hadoop cannot source data from directories with nested sub-directories,
      and it cannot write to directories that already exist. However, the good
      news is that you can simply point the <classname>Hfs</classname> tap to
      a directory of data files, and they are all used as input - there's no
      need to enumerate each individual file into a
      <classname>MultiSourcetap</classname>. But for a
      <classname>Flow</classname> to execute (when not used in a
      <classname>Cascade</classname> object), all of its sinks must be set to
      <classname>SinkMode.REPLACE</classname>.</para>
    </section>

    <section>
      <title xreflabel="Field Algebra" xml:id="field-algebra">Fields
      Sets</title>

      <para>Cascading applications can perform complex manipulation or "field
      algebra" on the fields stored in tuples, using<emphasis
      role="italic">Fields sets</emphasis>, a feature of the
      <classname>Fields</classname> class that provides a sort of wildcard
      tool for referencing sets of field values.</para>

      <para>The eight predefined Fields sets are constant values on the
      <classname>Fields</classname> class. They can be used in many places
      where the <classname>Fields</classname> class is expected. They are:
      <variablelist>
          <varlistentry>
            <term>Fields.ALL</term>

            <listitem>
              <para>The <classname>cascading.tuple.Fields.ALL</classname>
              constant is a wildcard that represents all the current available
              fields.</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>Fields.RESULTS</term>

            <listitem>
              <para>The <classname>cascading.tuple.Fields.RESULTS</classname>
              constant is used to represent the field names of the current
              operations return values. This Fields set may only be used as an
              output selector on a pipe, causing the pipe to output a tuple
              containing the operation results.</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>Fields.REPLACE</term>

            <listitem>
              <para>The <classname>cascading.tuple.Fields.REPLACE</classname>
              constant is used as an output selector to inline-replace values
              in the incoming tuple with the results of an operation. This
              convenient Fields set allows operations to overwrite the value
              stored in the specified field. The current operation must either
              specify the identical field names used by the pipe, or use the
              <classname>ARGS</classname> Fields set.</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>Fields.SWAP</term>

            <listitem>
              <para>The <classname>cascading.tuple.Fields.SWAP</classname>
              constant is used as an output selector to swap the operation
              arguments with its results. Neither the argument and result
              field names, nor the size, need to be the same. This is useful
              for when the operation arguments are no longer necessary and the
              result Fields and values should be appended to the remainder of
              the input field names and Tuple.</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>Fields.ARGS</term>

            <listitem>
              <para>The <classname>cascading.tuple.Fields.ARGS</classname>
              constant is used to let a given operation inherit the field
              names of its argument Tuple. This Fields set is a convenience
              and is typically used when the Pipe output selector is
              <classname>RESULTS</classname> or
              <classname>REPLACE</classname>. It is specifically used by the
              Identity Function when coercing values from Strings to primitive
              types.</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>Fields.GROUP</term>

            <listitem>
              <para>The <classname>cascading.tuple.Fields.GROUP</classname>
              constant represents all the fields used as grouping key in the
              most recent grouping. If no previous grouping exists in the pipe
              assembly, <classname>GROUP</classname> represents all the
              current field names.</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>Fields.VALUES</term>

            <listitem>
              <para>The <classname>cascading.tuple.Fields.VALUES</classname>
              constant represent all the fields not used as grouping fields in
              a previous Group. That is, if you have fields "a", "b", and "c",
              and group on "a", <classname>Fields.VALUES</classname> will
              resolve to "b" and "c".</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>Fields.UNKNOWN</term>

            <listitem>
              <para>The <classname>cascading.tuple.Fields.UNKNOWN</classname>
              constant is used when Fields must be declared, but it's not
              known how many fields or what their names are. This allows for
              processing tuples of arbitrary length from an input source or
              some operation. Use this Fields set with caution.</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>Fields.NONE</term>

            <listitem>
              <para>The <classname>cascading.tuple.Fields.NONE</classname>
              constant is used to specify no fields. Typically used as an
              argument selector for Operations that do not process any Tuples,
              like <classname>cascading.operation.Insert</classname>.</para>
            </listitem>
          </varlistentry>
        </variablelist>The chart below shows common ways to merge input and
      result fields for the desired output fields. A few minutes with this
      chart may help clarify the discussion of fields, tuples, and pipes. Also
      see the section on <xref linkend="each-every"/> for details on the
      different columns and their relationships to the
      <classname>Each</classname> and <classname>Every</classname> pipes and
      Functions, Aggregators, and Buffers.</para>

      <para xml:id="field-processing"><emphasis role="bold">Operations and
      Field-processing</emphasis></para>

      <mediaobject>
        <imageobject role="fo">
          <imagedata align="center" contentwidth="7in"
                     fileref="images/field-algebra.svg"/>
        </imageobject>

        <imageobject role="html">
          <imagedata align="center" contentwidth="7in"
                     fileref="images/field-algebra.png"/>
        </imageobject>
      </mediaobject>
    </section>

    <section>
      <title>Flows</title>

      <para>When pipe assemblies are bound to source and sink taps, a
      <classname>Flow</classname> is created. Flows are executable in the
      sense that, once they are created, they can be started and will execute
      on the configured platform. In the case of Hadoop mode, the Flow will
      execute on a Hadoop cluster.</para>

      <para>A Flow is essentially a data processing pipeline that reads data
      from sources, processes the data as defined by the pipe assembly, and
      writes data to the sinks. Input source data does not need to exist at
      the time the Flow is created, but it must exist by the time the Flow is
      executed (unless it is executed as part of a Cascade - see <xref
      linkend="cascades"/>).</para>

      <para>The most common pattern is to create a Flow from an existing pipe
      assembly. But there are cases where a MapReduce job (for Hadoop mode)
      has already been created and it makes sense to encapsulate it in a Flow
      class, so that it may participate in a <classname>Cascade</classname>
      and be scheduled with other <classname>Flow</classname> instances.
      Alternatively, via the <link
      xlink:href="http://github.com/cwensel/riffle">Riffle </link>
      annotations, third-party applications can participate in a
      <classname>Cascade</classname>, and complex algorithms that result in
      iterative Flow executions can be encapsulated as a single Flow. All
      patterns are covered here.</para>

      <section>
        <title>Creating Flows from Pipe Assemblies</title>

        <example>
          <title>Creating a new Flow</title>

          <xi:include href="simple-flow.xml"/>
        </example>

        <para>To create a Flow, it must be planned though one of the
        FlowConnector sub-class objects. Each platform mode, local mode and
        Hadoop mode, have their own connectors. The <code>connect()</code>
        method is used to create new Flow instances based on a set of sink
        taps, source taps, and a pipe assembly. The example above is quite
        trivial and is using the Hadoop mode connector.</para>

        <example>
          <title>Binding taps in a Flow</title>

          <xi:include href="complex-flow.xml"/>
        </example>

        <para>The example above expands on our previous pipe assembly example
        by creating multiple source and sink taps and planning a Flow. Note
        there are two branches in the pipe assembly - one named "lhs" and the
        other named "rhs". Internally Cascading uses those names to bind the
        source taps to the pipe assembly. New in 2.0, a FlowDef can be created
        to manage the names and taps that must be passed to a
        FlowConnector.</para>
      </section>

      <section>
        <title xreflabel="Configuring Flows"
        xml:id="configuring-flows">Configuring Flows</title>

        <para>The FlowConnector constructor accepts the
        <classname>java.util.Property</classname> object so that default
        Cascading and any platform specific properties can be passed down
        through the planner to the platform runtime. In the case of Hadoop,
        any relevant Hadoop <code>hadoop-default.xml</code> properties may be
        added. For instance, it's very common to add
        <code>mapred.map.tasks.speculative.execution</code>,
        <code>mapred.reduce.tasks.speculative.execution</code>, or
        <code>mapred.child.java.opts</code>.</para>

        <para>One property that must be set for production applications is the
        application Jar class or Jar path.</para>

        <example>
          <title>Configuring the Application Jar</title>

          <xi:include href="flow-properties.xml"/>
        </example>

        <para>More information on packaging production applications can be
        found in <xref linkend="executing-processes"/>.</para>

        <para>Note the pattern of using a static property-setter method
        (<classname>cascading.flow.FlowConnector.setApplicationJarPath</classname>).
        See <classname>cascading.flow.Flow</classname> for other
        properties..</para>

        <para>Since the <classname>FlowConnector</classname> can be reused,
        any properties passed on the constructor will be handed to all the
        Flows it is used to create. If Flows need to be created with different
        default properties, a new FlowConnector will need to be instantiated
        with those properties, or properties will need to be set on a given
        <classname>Pipe</classname> instance directly.</para>
      </section>

      <section>
        <title xreflabel="Skipping Flows" xml:id="skipping-flows">Skipping
        Flows</title>

        <para>When a Flow participates in a Cascade, the
        <classname>Flow.isSkip()</classname> method is consulted before
        calling <classname>Flow.start()</classname> on the flow. The result is
        based on the Flow's "skip strategy". By default,
        <methodname>isSkip()</methodname> returns true if any of the sinks are
        stale - i.e., the sinks don't exist or the resources are older than
        the sources. However, the strategy can be changed via the
        <classname>Cascade.setFlowSkipStrategy()</classname> method, which can
        be called before or after a particular <classname>Flow</classname>
        instance has been created.</para>

        <para>Cascading provides a choice of two standard skip
        strategies:</para>

        <variablelist>
          <varlistentry>
            <term>FlowSkipIfSinkStale</term>

            <listitem>
              <para>This strategy -
              <classname>cascading.flow.FlowSkipIfNotSinkStale</classname> -
              is the default. Sinks are treated as stale if they don't exist
              or the resources are older than the sources. If the SinkMode for
              the sink tap is REPLACE, then the tap is treated as
              stale.</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>FlowSkipIfSinkExists</term>

            <listitem>
              <para>The
              <classname>cascading.flow.FlowSkipIfSinkExists</classname>
              strategy skips the Flow if the sink tap exists, regardless of
              age. If the <classname>SinkMode</classname> for the sink tap is
              <code>REPLACE</code>, then the tap is treated as stale.</para>
            </listitem>
          </varlistentry>
        </variablelist>

        <para>Additionally, you can implement custom skip strategies by using
        the interface
        <classname>cascading.flow.FlowSkipStrategy</classname>.</para>

        <para>Note that <classname>Flow.start()</classname> does not consult
        the <methodname>isSkip()</methodname> method, and consequently always
        tries to start the Flow if called. It is up to the user code to call
        <classname>isSkip()</classname> to determine whether the current
        strategy indicates that the Flow should be skipped.</para>
      </section>

      <section>
        <title>Creating Flows from a JobConf</title>

        <para>If a MapReduce job already exists and needs to be managed by a
        Cascade, then the <classname>cascading.flow.MapReduceFlow</classname>
        class should be used. To do this, after creating a Hadoop
        <classname>JobConf</classname> instance simply pass it into the
        <classname>MapReduceFlow</classname> constructor. The resulting
        <classname>Flow</classname> instance can be used like any other
        Flow.</para>
      </section>

      <section>
        <title>Creating Custom Flows</title>

        <para>Any custom Class can be treated as a Flow if given the correct
        <link xlink:href="http://github.com/cwensel/riffle">Riffle</link>
        annotations. Riffle is a set of Java Annotations that identify
        specific methods on a Class as providing specific life-cycle and
        dependency functionality. For more information, see the Riffle
        documentation and examples. To use with Cascading, a Riffle-annotated
        instance must be passed to the
        <classname>cascading.flow.ProcessFlow</classname> constructor method.
        The resulting <classname>ProcessFlow</classname> instance can be used
        like any other Flow instance.</para>

        <para>Since many algorithms need to perform multiple passes over a
        given data set, a Riffle-annotated Class can be written that
        internally creates Cascading Flows and executes them until no more
        passes are needed. This is like nesting Flows or Cascades in a parent
        Flow, which in turn can participate in a Cascade.</para>
      </section>
    </section>

    <section>
      <title xreflabel="Cascades" xml:id="cascades">Cascades</title>

      <mediaobject>
        <imageobject role="fo">
          <imagedata contentwidth="2in" fileref="images/cascade.svg"/>
        </imageobject>

        <imageobject role="html">
          <imagedata contentwidth="2in" fileref="images/cascade.png"/>
        </imageobject>
      </mediaobject>

      <para>A Cascade allows multiple Flow instances to be executed as a
      single logical unit. If there are dependencies between the Flows, they
      are executed in the correct order. Further, Cascades act like Ant builds
      or Unix make files, that is, a Cascade only executes Flows that have
      stale sinks - i.e., output data that is older than the input data. For
      more on this, see <xref linkend="skipping-flows"/>.</para>

      <example>
        <title>Creating a new Cascade</title>

        <xi:include href="simple-cascade.xml"/>
      </example>

      <para>When passing Flows to the CascadeConnector, order is not
      important. The CascadeConnector automatically identifies the
      dependencies between the given Flows and creates a scheduler that starts
      each Flow as its data sources become available. If two or more Flow
      instances have no interdependencies, they are submitted together so that
      they can execute in parallel.</para>

      <para>For more information, see the section on<xref
      linkend="cascade-scheduler"/>.</para>

      <para>If an instance of
      <classname>cascading.flow.FlowSkipStrategy</classname> is given to a
      <classname>Cascade</classname> instance (via the
      <classname>Cascade.setFlowSkipStrategy()</classname> method), it is
      consulted for every Flow instance managed by that Cascade, and all skip
      strategies on those Flow instances are ignored. For more information on
      skip strategies, see <xref linkend="skipping-flows"/>.</para>
    </section>
  </chapter>

  <chapter>
    <info>
      <title xreflabel="Executing Processes"
      xml:id="executing-processes">Executing Processes</title>
    </info>

    <section>
      <title>Introduction</title>

      <para>Cascading requires Apache Hadoop to be installed and correctly
      configured when using the <classname>HadoopFlowConnector</classname>.
      Hadoop is an Open Source Apache project, freely available for download
      from the Hadoop website,<link
      xlink:href="http://hadoop.apache.org/core/">http://hadoop.apache.org/core/</link>.</para>
    </section>

    <section>
      <title xreflabel="Building Cascading Applications"
      xml:id="building-processes">Building</title>

      <para>Cascading ships with several jars, including the following:</para>

      <variablelist>
        <varlistentry>
          <term>cascading-core-2.0.x.jar</term>

          <listitem>
            <para>This jar contains the Cascading Core class files. It should
            be packaged with<filename> lib/*.jar</filename> when using
            Hadoop.</para>
          </listitem>
        </varlistentry>

        <varlistentry>
          <term>cascading-local-2.0.x.jar</term>

          <listitem>
            <para>This jar contains the Cascading local mode class files. Not
            used with Hadoop.</para>
          </listitem>
        </varlistentry>

        <varlistentry>
          <term>cascading-hadoop-2.0.x.jar</term>

          <listitem>
            <para>This jar contains the Cascading Hadoop specific
            dependencies.It should be packaged with
            <filename>lib/*.jar</filename> when using Hadoop.</para>
          </listitem>
        </varlistentry>

        <varlistentry>
          <term>cascading-xml-1.2.x.jar</term>

          <listitem>
            <para>This jar contains Cascading XML module class files and is
            optional. It should be packaged with
            <filename>lib/xml/*.jar</filename> when using Hadoop.</para>
          </listitem>
        </varlistentry>

        <varlistentry>
          <term>cascading-test-1.2.x.jar</term>

          <listitem>
            <para>This jar contains Cascading unit tests. If writing custom
            modules for cascading, it may be helpful to sub-class
            <classname>cascading.CascadingTestCase</classname>.</para>
          </listitem>
        </varlistentry>
      </variablelist>

      <para>Cascading works with either of the Hadoop processing modes - the
      default local stand-alone mode and the distributed cluster mode. As
      specified in the Hadoop documentation, running in cluster mode requires
      the creation of a Hadoop job jar that includes the Cascading jars, plus
      any dependent third-party jars, in its <filename>lib</filename>
      directory. This is true regardless of whether they are Cascading Hadoop
      mode applications or raw Hadoop MapReduce applications.</para>

      <para>The following Ant snippets can be used in your project to create a
      Hadoop jar for submission on your cluster.</para>

      <example>
        <title>Sample Ant Build - Properties</title>

        <xi:include href="sample-build-properties.xml"/>
      </example>

      <example>
        <title>Sample Ant Build - Target</title>

        <xi:include href="sample-build-target.xml"/>
      </example>

      <para>Note that these Ant snippets are only intended to show how to
      include Cascading libraries. You are still required to compile your
      project into the <property>build.classes</property> path.</para>
    </section>

    <section>
      <title>Configuring</title>

      <para>During runtime, Hadoop must be told which application jar file
      should be pushed to the cluster. Typically, this is done via the Hadoop
      API <classname>JobConf</classname> object.</para>

      <para>Cascading offers a shorthand for configuring this parameter,
      demonstrated here:</para>

      <xi:include href="flow-properties.xml"/>

      <para>Above we see two ways to set the same property - via the
      <methodname>setApplicationJarClass()</methodname> method, and via the
      <methodname>setApplicationJarPath()</methodname> method.
      <?oxy_comment_start author="Jim" timestamp="20120402T205343-0700" comment="Chris, I added two sentences here because it felt odd to only mention the first of the two approaches shown.  I&apos;m not sure I get this part; hopefully this statement is true."?>One
      is based on a Class name, and the other is based on a literal
      path.<?oxy_comment_end ?></para>

      <para>The first method takes a Class object that owns the "main"
      function for this application. The assumption here is that
      <code>Main.class</code> is not located in a Java Jar that is stored in
      the <filename>lib</filename> folder of the application Jar. If it is,
      that Jar is pushed to the cluster, not the parent application
      jar.</para>

      <para><?oxy_comment_start author="Jim" timestamp="20120402T205332-0700" comment="Chris, not sure I get this part; hopefully this statemet is true."?>The
      second method simply sets the path to the parent Class as a
      property.<?oxy_comment_end ?></para>

      <para>In your application, only one of these methods needs to be called,
      but one of them must be called to properly configure Hadoop.</para>
    </section>

    <section>
      <title xml:id="operating-modes"><?oxy_comment_start author="Jim" timestamp="20120402T042700-0700" comment="Chris, you&apos;ll need to either plug in some information here or brief me so I can do it. For now, I just threw in this dummy text to get you started."?>Operating
      Modes<?oxy_comment_end ?></title>

      <para>Although applications that use Cascading are typically run in
      distributed mode on a computer cluster, for development and testing it's
      often more practical to run on in local mode. Cascading provides several
      options for controlling the operating mode:</para>

      <variablelist>
        <varlistentry>
          <term>Cascading local mode</term>

          <listitem>
            <para>This mode causes the application to bypass Hadoop and run
            entirely on the local machine using the local file system.</para>
          </listitem>
        </varlistentry>

        <varlistentry>
          <term>MapReduce mode</term>

          <listitem>
            <para>This mode causes the application to run in the mode that
            Hadoop is currently configured for, and use the Hadoop file
            system.</para>
          </listitem>
        </varlistentry>
      </variablelist>

      <para><?oxy_comment_start author="Jim" timestamp="20120402T205646-0700" comment="One question I have, from what I&apos;ve read so far, is that I think I see how to set the file mode, but how do you tell the MapReduce whether to divide the work up among multiple CPU&apos;s or not...?"?>To
      select a mode for your application... <?oxy_comment_end ?></para>
    </section>

    <section>
      <title>Executing</title>

      <para>Running a Cascading application is the same as running any Hadoop
      application. After packaging your application into a single jar (see
      <xref linkend="building-processes"/>), you must use
      <filename>bin/hadoop</filename> to submit the application to the
      cluster.</para>

      <para>For example, to execute an application stuffed into
      <filename>your-application.jar</filename>, call the Hadoop shell
      script:</para>

      <example>
        <title>Running a Cascading Application</title>

        <para><programlisting>$HADOOP_HOME/bin/hadoop jar your-application.jar [some params]</programlisting></para>
      </example>

      <para>If the configuration scripts in <envar>$HADOOP_CONF_DIR</envar>
      are configured to use a cluster, the Jar is pushed into the cluster for
      execution.</para>

      <para>Cascading does not rely on any environment variables like
      <envar>$HADOOP_HOME</envar> or<envar>$HADOOP_CONF_DIR</envar>, only
      <filename>bin/hadoop</filename> does.</para>

      <para>It should be noted that even though
      <filename>your-application.jar</filename> is passed on the command line
      to<filename>bin/hadoop</filename>, this in no way configures Hadoop to
      push this jar into the cluster. You must still call one of the property
      setters mentioned above to set the proper path to the application jar.
      If misconfigured, it's likely that one of the internal libraries (found
      in the lib folder) will be pushed to the cluster instead, and "Class Not
      Found" exceptions will be thrown.</para>
    </section>
  </chapter>

  <chapter>
    <info>
      <title>Using and Developing Operations</title>
    </info>

    <section>
      <title>Introduction</title>

      <para>So far we've talked about setting up sources and sinks, shaping
      the data streams, referencing the data fields, and so on. Within this
      Pipe framework, Operations are used to act upon the data - alter it,
      filter it, analyze it, and so on. You can use the standard Operations in
      the Cascading library to create powerful and robust applications, and
      you can combine them in chains (much like Unix operations such
      as<command> sed</command>, <command>grep</command>,
      <command>sort</command>, <command>uniq</command>, and
      <command>awk</command>). And if you want to go further, it's also very
      simple to develop custom Operations in Cascading.</para>

      <para>There are four kinds of Operations:
      <classname>Function</classname>,<classname>Filter</classname>,
      <classname>Aggregator</classname>, and
      <classname>Buffer</classname>.</para>

      <mediaobject>
        <imageobject role="fo">
          <imagedata align="center" contentwidth="3in"
                     fileref="images/operations.svg"/>
        </imageobject>

        <imageobject role="html">
          <imagedata align="center" contentwidth="3in"
                     fileref="images/operations.png"/>
        </imageobject>
      </mediaobject>

      <para>Operations may require an input argument Tuple to act on. And all
      Operations can return zero or more Tuple object results - except
      <classname>Filter</classname>, which simply returns a Boolean indicating
      whether to discard the current Tuple. A <classname>Function</classname>,
      for instance, can parse a string passed by an argument Tuple and return
      a new Tuple for every value parsed (i.e., one Tuple for each "word"), or
      it may create a single Tuple with every parsed value included as an
      element in one Tuple object (e.g., one Tuple with "first-name" and
      "last-name" fields).</para>

      <para>In theory, a <classname>Function</classname> can be used as a
      <classname>Filter</classname>. However, the
      <classname>Filter</classname> type is optimized for filtering, and can
      be combined with logical Operations such as <classname>Not</classname>,
      <classname>And</classname>, <classname>Or</classname>, etc.</para>

      <para>During runtime, Operations actually receive arguments as one or
      more instances of the <classname>TupleEntry</classname> object. The
      TupleEntry object holds both an instance of
      <classname>Fields</classname> and the current
      <classname>Tuple</classname> for which the <classname>Fields</classname>
      object defines fields.</para>

      <para>Except for <classname>Filter</classname>, all Operations must
      declare result Fields, and if the actual output does not match the
      declaration, the process will fail. For example, consider a
      <classname>Function</classname> written to parse words out of a String
      and return a new Tuple for each word. If it declares that its intended
      output is a Tuple with a single field named "word", and then returns
      more values in the Tuple beyond that single "word", processing will
      halt. However, Operations designed to return arbitrary numbers of values
      in a result Tuple may declare <code>Fields.UNKNOWN</code>.</para>

      <para>The Cascading planner always attempts to "fail fast" where
      possible by checking the field name dependencies between Pipes and
      Operations, but there may be some cases the planner can't account
      for.</para>

      <para>All Operations must be wrapped by either an
      <classname>Each</classname> or an <classname>Every</classname> pipe
      instance. The pipe is responsible for passing in an argument Tuple and
      accepting the resulting output Tuple.</para>

      <para>Operations by default are assumed by the Cascading planner to be
      "safe". A safe Operation is idempotent; it can safely execute multiple
      times on the exact same record or Tuple; it has no side-effects. If a
      custom Operation is not idempotent, the method <code>isSafe()</code>
      must return <code>false</code>. This value influences how the Cascading
      planner renders the Flow under certain circumstances.</para>
    </section>

    <section>
      <title>Functions</title>

      <para>A <classname>Function</classname> expects a stream of individual
      argument Tuples, and returns zero or more result Tuples for each of
      them. Like a <classname>Filter</classname>, a
      <classname>Function</classname> is used with an
      <classname>Each</classname> pipe, which may follow any pipe type.</para>

      <para>To create a custom <classname>Function</classname>, subclass the
      class <code>cascading.operation.BaseOperation</code> and implement the
      interface <code>cascading.operation.Function</code>. Since the
      <code>BaseOperation</code> has been subclassed, the <code>operate</code>
      method, as defined on the <code>Function</code> interface, is the only
      method that must be implemented.</para>

      <example>
        <title>Custom Function</title>

        <xi:include href="custom-function.xml"/>
      </example>

      <para>Whenever possible, functions should declare both the number of
      argument values they expect and the field names of the Tuple they
      return. However, these declarations are optional, as explained
      below.</para>

      <para>For input, functions must accept one or more values in a Tuple as
      arguments. If not specified, the default is to accept any number of
      values (<code>Operation.ANY</code>). Cascading verifies during planning
      that the number of arguments selected matches the number of arguments
      expected.</para>

      <para>For output, it's a good practice to declare the field names that a
      function returns. If not specified, the default is
      <code>Fields.UNKNOWN</code>, meaning that an unknown number of fields
      are returned in each Tuple.</para>

      <para>Both declarations - input argument and output fields - must be
      done on the constructor, either by passing default values to the
      <code>super</code> constructor, or by accepting the values from the user
      via a constructor implementation.</para>

      <example>
        <title>Add Values Function</title>

        <xi:include href="sum-function.xml"/>
      </example>

      <para>The example above implements a <classname>Function</classname>
      that accepts two values in the argument Tuple, adds them together, and
      returns the result in a new Tuple.</para>

      <para>The first constructor above assumes a default field name for the
      field that this <classname>Function</classname> returns. In practice,
      it's good to give the user the option of overriding the declared field
      names, allowing them to prevent possible field name collisions that
      might cause the planner to fail.</para>

      <example>
        <title>Add Values Function and Context</title>

        <xi:include href="efficient-sum-function.xml"/>
      </example>

      <para>This example is a minor variation on the previous and introduces
      the notion of a "context" object and the
      <methodname>prepare()</methodname> and
      <methodname>cleanup()</methodname> methods.</para>

      <para>All Operations allow for a context object, simply a user defined
      object that holds state between calls of the
      <methodname>operate()</methodname> method. This allows for a given
      instance of the Operation to be multi-thread safe on a platform that may
      use multiple threads of execution versus multiple processes. It also
      allows deferring complex resources until the Operation will be
      engaged.</para>

      <para>The <methodname>prepare()</methodname> and
      <methodname>cleanup()</methodname> methods are invoked once per thread
      of execution, and in the case of the Hadoop platform, only on the
      cluster side, never on the client.</para>

      <para>In the above example, a <classname>Tuple</classname> is used as
      the context a more complex type isn't necessary. Also note the Tuple
      isn't storing state but it is re-used to reduce the number of new Object
      instances created. In Cascading, it is perfectly safe to output the same
      Tuple instance from the <methodname>operate()</methodname>. The method
      <code>functionCall.getOutputCollector().add( result )</code> will not
      return until the result <classname>Tuple</classname> has been processed
      or persisted downstream.</para>
    </section>

    <section>
      <title>Filter</title>

      <para>A <classname>Filter</classname> expects a stream of individual
      argument Tuples and returns a Boolean value for each, stating whether it
      should be discarded. Like a <classname>Function</classname>, a
      <classname>Filter</classname> is used with an
      <classname>Each</classname> pipe, which may follow any pipe type.</para>

      <para>To create a custom <classname>Filter</classname>, subclass the
      class <code>cascading.operation.BaseOperation</code> and implement the
      interface<code>cascading.operation.Filter</code>. Because
      <code>BaseOperation</code> has been subclassed, the
      <code>isRemove</code> method, as defined on the <code>Filter</code>
      interface, is the only method that must be implemented.</para>

      <example>
        <title>Custom Filter</title>

        <xi:include href="custom-filter.xml"/>
      </example>

      <para>Filters must accept one or more values in a Tuple as arguments,
      and should declare the number of argument values they expect. If not
      specified, the default is to accept any number of values
      (<code>Operation.ANY</code>). Cascading verifies during planning that
      the number of arguments selected matches the number of arguments
      expected.</para>

      <para>The number of arguments declaration must be done on the
      constructor, either by passing a default value to the <code>super</code>
      constructor, or by accepting the value from the user via a constructor
      implementation.</para>

      <example>
        <title>String Length Filter</title>

        <xi:include href="stringlength-filter.xml"/>
      </example>

      <para>The example above implements a <classname>Filter</classname> that
      accepts two arguments and filters out the current Tuple if the first
      argument, String length, is greater than the integer value of the second
      argument.</para>
    </section>

    <section>
      <title>Aggregator</title>

      <para>An <classname>Aggregator</classname> expects a stream of tuple
      groups (the output of a GroupBy or CoGroup pipe), and returns zero or
      more result tuples for every group. An <classname>Aggregator</classname>
      may only be used with an <classname>Every</classname> pipe, which may
      follow a<classname>GroupBy</classname>, a
      <classname>CoGroup</classname>, or another <classname>Every</classname>
      pipe, but not an<classname>Each</classname>.</para>

      <para>To create a custom<classname>Aggregator</classname>, subclass the
      class <code>cascading.operation.BaseOperation</code> and implement the
      interface<code>cascading.operation.Aggregator</code>. Because
      <code>BaseOperation</code> has been subclassed, the <code>start</code>,
      <code>aggregate</code>, and <code>complete</code> methods, as defined on
      the <code>Aggregator</code> interface, are the only methods that must be
      implemented.</para>

      <example>
        <title>Custom Aggregator</title>

        <xi:include href="custom-aggregator.xml"/>
      </example>

      <para>Whenever possible, Aggregators should declare both the number of
      argument values they expect and the field names of the Tuple they
      return. However, these declarations are optional, as explained
      below.</para>

      <para>For input, Aggregators must accept one or more values in a Tuple
      as arguments. If not specified, the default is to accept any number of
      values (<code>Operation.ANY</code>). Cascading verifies during planning
      that the number of arguments selected is the same as the number of
      arguments expected.</para>

      <para>For output, it's good practice for Aggregators to declare the
      field names they return. If not specified, the default is
      <code>Fields.UNKNOWN</code>, meaning that an unknown number of fields
      are returned in each Tuple.</para>

      <para>Both declarations - input argument and output fields - must be
      done on the constructor, either by passing default values to the
      <code>super</code> constructor, or by accepting the values from the user
      via a constructor implementation.</para>

      <example>
        <title>Add Tuples Aggregator</title>

        <xi:include href="sum-aggregator.xml"/>
      </example>

      <para>The example above implements an <classname>Aggregator</classname>
      that accepts a value in the argument Tuple, adds all the argument tuples
      in the current grouping, and returns the result as a new Tuple.</para>

      <para>The first constructor above assumes a default field name that this
      <classname>Aggregator</classname> returns. In practice, it's good to
      give the user the option of overriding the declared field names,
      allowing them to prevent possible field name collisions that might cause
      the planner to fail.</para>

      <para>There are several constraints on the use of Aggregators that may
      not be self-evident. These are detailed in the Javadoc</para>
    </section>

    <section>
      <title>Buffer</title>

      <para>A <classname>Buffer</classname> expects set of argument tuples in
      the same grouping, and may return zero or more result tuples.</para>

      <para>A <classname>Buffer</classname> is very similar to an
      <classname>Aggregator</classname>, except that it receives the current
      Grouping Tuple, and an iterator of all the arguments it expects, for
      every value Tuple in the current grouping - all on the same method call.
      This is very similar to the typical Reducer interface in MapReduce, and
      is best used for operations that need high visibility to the previous
      and next elements in the stream - such as smoothing a series of
      time-stamps where there are missing values.</para>

      <para>A <classname>Buffer</classname> may only be used with an
      <classname>Every</classname> pipe, and it may only follow a
      <classname>GroupBy</classname> or <classname>CoGroup</classname> pipe
      type.</para>

      <para>To create a custom <classname>Buffer</classname>, subclass the
      class <code>cascading.operation.BaseOperation</code> and implement the
      interface <code>cascading.operation.Buffer</code>. Because
      <code>BaseOperation</code> has been subclassed, the <code>operate</code>
      method, as defined on the <code>Buffer</code> interface, is the only
      method that must be implemented.</para>

      <example>
        <title>Custom Buffer</title>

        <xi:include href="custom-buffer.xml"/>
      </example>

      <para>Buffers should declare both the number of argument values they
      expect and the field names of the Tuple they return.</para>

      <para>For input, Buffers must accept one or more values in a Tuple as
      arguments. If not specified, the default is to accept any number of
      values (Operation.ANY). During the planning phase, Cascading verifies
      that the number of arguments selected is the same as the number of
      arguments expected.</para>

      <para>For output, it's good practice for Buffers to declare the field
      names they return. If not specified, the default is
      <code>Fields.UNKNOWN</code>, meaning that an unknown number of fields
      are returned in each Tuple.</para>

      <para>Both declarations - input argument and output fields - must be
      done on the constructor, either by passing default values to the
      <code>super</code> constructor, or by accepting the values from the user
      via a constructor implementation.</para>

      <example>
        <title>Average Buffer</title>

        <xi:include href="average-buffer.xml"/>
      </example>

      <para>The example above implements a buffer that accepts a value in the
      argument Tuple, adds all these argument tuples in the current grouping,
      and returns the result divided by the number of argument tuples counted
      in a new Tuple.</para>

      <para>The first constructor above assumes a default field name for the
      field that this <classname>Buffer</classname> returns. In practice, it's
      good to give the user the option of overriding the declared field names,
      allowing them to prevent possible field name collisions that might cause
      the planner to fail</para>

      <para>Note that this example is somewhat artificial. In actual practice,
      an <classname>Aggregator</classname> would be a better way to compute
      averages for an entire dataset. A <classname>Buffer</classname> is
      better suited for calculating running averages across very large spans,
      for example.</para>

      <para>There are several constraints on the use of Buffers that may not
      be self-evident. These are detailed in the Javadoc.</para>

      <para>As with the <classname>Function</classname> example above, a
      <classname>Buffer</classname> may define a custom context object and
      implement the <methodname>prepare()</methodname> and
      <methodname>cleanup()</methodname> methods to maintain state, or re-use
      outgoing <classname>Tuple</classname> instances for efficiency.</para>
    </section>

    <section>
      <title>Operation and BaseOperation</title>

      <para>In all of the above sections, the
      <classname>cascading.operation.BaseOperation</classname> class was
      subclassed. This class is an implementation of the
      <classname>cascading.operation.Operation</classname> interface, and
      provides a few default method implementations. It is not strictly
      required to extend <classname>BaseOperation</classname> when
      implementing this interface, but it is very convenient to do so.</para>

      <para>When developing custom operations, the developer may need to
      initialize and destroy a resource. For example, when doing pattern
      matching, you might need to initialize a
      <classname>java.util.regex.Matcher</classname> and use it in a
      thread-safe way. Or you might need to open, and eventually close, a
      remote connection. But for performance reasons, the operation should not
      create or destroy the connection for each Tuple or every Tuple group
      that passes through.</para>

      <para>For this reason, the interface
      <interfacename>Operation</interfacename> declares two methods:
      <methodname>prepare()</methodname> and
      <methodname>cleanup()</methodname>. In the case of Hadoop and MapReduce,
      the <methodname>prepare()</methodname> and
      <methodname>cleanup()</methodname> methods are called once per Map or
      Reduce task. The <methodname>prepare()</methodname> method is called
      before any argument Tuple is passed in, and the
      <methodname>cleanup()</methodname> method is called after all Tuple
      arguments have been operated on. Within each of these methods, the
      developer can initialize a "context" object that can hold an open socket
      connection or <classname>Matcher</classname> instance. This context is
      user defined, and is the same mechanism used by the
      <classname>Aggregator</classname> operation - except that the
      <classname>Aggregator</classname> is also given the opportunity to
      initialize and destroy its context, via the
      <classname>start()</classname> and <classname>complete()</classname>
      methods.</para>

      <para>Note that if a "context" object is used, its type should be
      declared in the sub-class class declaration using the Java Generics
      notation.</para>
    </section>
  </chapter>

  <chapter>
    <info>
      <title>Advanced Processing</title>
    </info>

    <section>
      <title xreflabel="SubAssemblies"
      xml:id="subassemblies">SubAssemblies</title>

      <para>In Cascading, SubAssemblies are reusable pipe assemblies that are
      linked into larger pipe assemblies. They function much like subroutines
      in a larger program. Subassemblies are a good way to organize complex
      pipe assemblies, and they allow for commonly-used pipe assemblies to be
      packaged into libraries for inclusion in other projects by other
      users.</para>

      <para>To create a SubAssembly, subclass the
      <classname>cascading.pipe.SubAssembly</classname> class.</para>

      <example>
        <title>Creating a SubAssembly</title>

        <xi:include href="custom-subassembly.xml"/>
      </example>

      <para>In the example above, we pass in (as parameters via the
      constructor) the pipes that we wish to continue assembling against, and
      in the last line we register the "join" pipe as a tail. This allows
      SubAssemblies to be nested within larger pipe assemblies or other
      SubAssemblies.</para>

      <example>
        <title>Using a SubAssembly</title>

        <xi:include href="simple-subassembly.xml"/>
      </example>

      <para>The example above demonstrates how to include a SubAssembly into a
      new pipe assembly.</para>

      <para>Note that in a SubAssembly that represents a split - that is, a
      SubAssembly with two or more tails - you can use the
      <methodname>getTails()</methodname> method to access the array of tails
      set internally by the <methodname>setTails()</methodname> method.</para>

      <example>
        <title>Creating a Split SubAssembly</title>

        <xi:include href="split-subassembly.xml"/>
      </example>

      <example>
        <title>Using a Split SubAssembly</title>

        <xi:include href="simple-split-subassembly.xml"/>
      </example>

      <para>To rephrase, if a <classname>SubAssembly</classname> does not
      split the incoming Tuple stream, the SubAssembly instance can be passed
      directly to the next Pipe instance. But, if the
      <classname>SubAssembly</classname> splits the stream into multiple
      branches, handles will be needed to access them. The solution is to pass
      each branch tail to the <methodname>setTails()</methodname> method, and
      call the <methodname>getTails()</methodname> method to get handles for
      the desired branches, which can be passed to subsequent instances of
      <classname>Pipe</classname>.</para>
    </section>

    <section>
      <title xreflabel="Stream Assertions" xml:id="stream-assertions">Stream
      Assertions</title>

      <mediaobject>
        <imageobject role="fo">
          <imagedata align="center" contentwidth="6in"
                     fileref="images/stream-assertions.svg"/>
        </imageobject>

        <imageobject role="html">
          <imagedata align="center" contentwidth="6in"
                     fileref="images/stream-assertions.png"/>
        </imageobject>
      </mediaobject>

      <para>Stream assertions are simply a mechanism for asserting that one or
      more values in a tuple stream meet certain criteria. This is similar to
      the Java language "assert" keyword, or a unit test. An example would be
      "assert not null" or "assert matches".</para>

      <para>Assertions are treated like any other function or aggregator in
      Cascading. They are embedded directly into the pipe assembly by the
      developer. By default, if an assertion fails, the processing fails. As
      an alternative, an assertion failure can be caught by a failure
      Trap.</para>

      <para>Assertions may be more, or less, desirable in different contexts.
      For this reason, stream assertions can be treated as either "strict" or
      "validating". <emphasis role="italic">Strict</emphasis> assertions make
      sense when running tests against regression data - which should be
      small, and should represent many of the edge cases that the processing
      assembly must robustly support. <emphasis role="italic">Validating
      </emphasis> assertions, on the other hand, make more sense when running
      tests in staging, or when using data that may vary in quality due to an
      unmanaged source.</para>

      <para>And of course there are cases where assertions are unnecessary and
      only impede processing, and it would be best to just bypass them
      altogether.</para>

      <para>To handle all three of these situations, Cascading can be
      instructed to <emphasis role="italic">plan out</emphasis> (i.e., omit)
      strict assertions, validation assertions, or both when building the
      Flow. To create optimal performance, Cascading implements this by
      actually leaving the undesired assertions out of the final Flow (not
      merely switching them off).</para>

      <example>
        <title>Adding Assertions</title>

        <xi:include href="simple-assertion.xml"/>
      </example>

      <para>Again, assertions are added to a pipe assembly like any other
      operation, except that the <classname>AssertionLevel</classname> must be
      set to tell the planner how to treat the assertion during
      planning.</para>

      <example>
        <title>Planning Out Assertions</title>

        <xi:include href="simple-assertion-planner.xml"/>
      </example>

      <para>To configure the planner to remove some or all assertions, a
      property can be set via the
      <classname>FlowConnector.setAssertionLevel()</classname> method or
      directly on the <classname>FlowDef</classname> instance, as can be seen
      above. Setting <classname>AssertionLevel.NONE</classname> removes all
      assertions. <classname>AssertionLevel.VALID</classname> keeps
      <code>VALID</code> assertions but removes <code>STRICT</code> ones. And
      <classname>AssertionLevel.STRICT</classname> keeps all assertions - the
      planner default value.</para>
    </section>

    <section>
      <title xreflabel="Failure Traps" xml:id="failure-traps">Failure
      Traps</title>

      <para>The following diagram shows the use of <emphasis
      role="italic">Failure Traps </emphasis> in a pipe assembly.</para>

      <para><inlinemediaobject>
          <imageobject role="fo">
            <imagedata align="center" contentwidth="5in"
                       fileref="images/failure-traps.svg"/>
          </imageobject>

          <imageobject role="html">
            <imagedata align="center" contentwidth="5in"
                       fileref="images/failure-traps.png"/>
          </imageobject>
        </inlinemediaobject></para>

      <para>Failure Traps are very similar to tap sinks (as opposed to tap
      sources) in that they allow data to be stored. Except that Tap sinks are
      bound to a particular tail pipe in a pipe assembly and are the primary
      outlet of a branch in a pipe assembly. Tap traps can be bound to
      intermediate pipe assembly branches - just like Stream Assertions - yet
      they only capture data that causes and Operation to fail (throw an
      Exception).</para>

      <para>Whenever an operation fails and throws an exception, if there is
      an associated trap, the offending Tuple is saved to the resource
      specified by the trap Tap. This allows the job to continue processing
      without any data loss.</para>

      <para>By design, clusters are hardware fault-tolerant - lose a node, and
      the cluster continues working. But fault tolerance for software is a
      little different. Failure Traps provide a means for the processing to
      continue without losing track of the data that caused the fault. For
      high fidelity applications, this may not be very important, you likely
      will want any errors during processing to cause the application to stop.
      But for low fidelity applications (like webpage indexing) this can
      dramatically improve processing reliability, skipping a page out of a
      few million won't wreck things.</para>

      <example>
        <title>Setting Traps</title>

        <xi:include href="simple-traps.xml"/>
      </example>

      <para>The example above binds a trap tap to the pipe assembly segment
      named "assertions". Note how we can name branches and segments by using
      a single <classname>Pipe</classname> instance, and that the naming
      applies to all subsequent <classname>Pipe</classname> instances.</para>

      <para>Traps are for exceptional cases, in the same way that Java
      Exception handling is. Traps are not intended for application flow
      control, and not a means to filter some data into other locations.
      Applications that need to filter out bad data should do so explicitly,
      using filters. For more on this, see<xref
      linkend="handling-bad-data"/>.</para>
    </section>

    <section>
      <title>Event Handling</title>

      <para>Each Flow has the ability to execute callbacks via an event
      listener. This ability is useful when an external application needs to
      be notified that a Flow has started, halted, completed, or has thrown an
      exception.</para>

      <para>For instance, at the completion of a flow that runs on an Amazon
      EC2 Hadoop cluster, an Amazon SQS message can be sent to notify another
      application to fetch the job results from S3 or begin the shutdown of
      the cluster.</para>

      <para>Flows support event listeners through the
      <classname>cascading.flow.FlowListener</classname> interface, which
      supports four events:</para>

      <variablelist>
        <varlistentry>
          <term>onStarting</term>

          <listitem>
            <para>The onStarting event is fired when a Flow instance receives
            the <code>start()</code> message.</para>
          </listitem>
        </varlistentry>

        <varlistentry>
          <term>onStopping</term>

          <listitem>
            <para>The onStopping event is fired when a Flow instance receives
            the <code>stop()</code> message.</para>
          </listitem>
        </varlistentry>

        <varlistentry>
          <term>onCompleted</term>

          <listitem>
            <para>The onCompleted event is fired when a Flow instance has
            completed all work, regardless of success or failure. If an
            exception was thrown, onThrowable will be fired before this
            event.</para>
          </listitem>
        </varlistentry>

        <varlistentry>
          <term>onThrowable</term>

          <listitem>
            <para>The onThrowable event is fired if any internal job client
            throws a Throwable type. This throwable is passed as an argument
            to the event. onThrowable should return true if the given
            throwable was handled, and should not be rethrown from the
            <code>Flow.complete()</code> method.</para>
          </listitem>
        </varlistentry>
      </variablelist>
    </section>

    <section>
      <title xreflabel="Template Taps" xml:id="template-tap">Template
      taps</title>

      <para>The <classname>Templatetap</classname> <classname>tap</classname>
      class provides a simple means to break large datasets into smaller sets
      based on data item values. This is commonly called partioning or
      "binning" the data, where each "bin" of data is named after some data
      value(s) shared by the members of that bin. For example, organizing log
      files by month and year. The <classname>TemplateTap</classname> is only
      available in Hadoop mode.</para>

      <xi:include href="template-tap.xml"/>

      <para>In the example above, we construct a parent
      <classname>Hfs</classname> <classname>tap</classname> and pass it to the
      constructor of a <classname>Templatetap</classname> instance, along with
      a String format "template". This format template is populated in the
      order in which values are declared via the <classname>Scheme</classname>
      class. If more complex path formatting is necessary, you may subclass
      the <classname>Templatetap</classname>.</para>

      <para>Note that you can only create sub-directories to bin data into.
      Hadoop must still write "part" files into each bin directory.</para>

      <para>One last thing to keep in mind is whether "binning" happens during
      the Map phase or the Reduce phase. By doing a
      <classname>GroupBy</classname> on the values used to populate the
      template, binning will happen during the Reduce phase, and will likely
      scale much better in cases where there are a very large number of unique
      values used in the template resulting in a large number of
      directories.</para>
    </section>

    <section>
      <title>Partial Aggregation instead of Combiners</title>

      <para>In Hdoop mode, Cascading does not support MapReduce "Combiners".
      Combiners are a simple optimization allowing some Reduce functions to
      run on the Map side of MapReduce. Combiners are very powerful in that
      they reduce the I/O between the Mappers and Reducers - why send all of
      your Mapper data to Reducers when you can compute some values on the Map
      side and combine them in the Reducer? But Combiners are limited to
      Associative and Commutative functions only, such as "sum" and "max". And
      the process requires that the values output by the Map task must be
      serialized, sorted (which involves deserialization and comparison),
      deserialized again, and operated on - after which the results are again
      serialized and sorted. Combiners trade CPU for gains in I/O.</para>

      <para>Cascading takes a different approach. It provides a mechanism to
      perform partial aggregations on the Map side and combine the results on
      the Reduce side, but trades memory, instead of CPU, for I/O gains by
      caching values (up to a threshold limit). This bypasses the redundant
      serialization, deserialization, and sorting. Also, Cascading allows any
      aggregate function to be implemented - not just Associative and
      Commutative functions.</para>

      <para>Cascading supports a few built-in partial aggregate operations,
      including AverageBy, CountBy, and SumBy. These are actually
      SubAssemblies, not operations, and are sub-classes of the AggregateBy
      SubAssembly. For more on this, see the section on<xref
      linkend="aggregate-by"/>.</para>

      <para>Using partial aggregate operations is quite easy. They are
      actually less verbose than a standard Aggregate operation.</para>

      <example>
        <title>Using a SumBy</title>

        <xi:include href="partials-sumby.xml"/>
      </example>

      <para>For composing multiple partial aggregate operations, things are
      done a little differently.</para>

      <example>
        <title>Composing partials with AggregateBy</title>

        <xi:include href="partials-compose.xml"/>
      </example>

      <para>It's important to note that a <classname>GroupBy</classname> Pipe
      is embedded in the resulting assemblies above. But only one GroupBy is
      performed in the case of the AggregateBy, and all of the partial
      aggregations will be performed simultaneously. It is also important to
      note that, depending on the final pipe assembly, the Map side partial
      aggregate functions may be planned into the previous Reduce operation in
      Hadoop, further improving the performance of the application.</para>
    </section>
  </chapter>

  <chapter>
    <info>
      <title>Built-In Operations</title>
    </info>

    <section>
      <title>Identity Function</title>

      <para>The <classname>cascading.operation.Identity</classname> function
      is used to "shape" a tuple stream. Here are some common patterns that
      should help illustrate how the Cascading "field algebra" works even
      though there are helper sub-assemblies for many of these examples, like
      <classname>Rename</classname> and <classname>Discard</classname>.</para>

      <para><variablelist>
          <varlistentry>
            <term>Discard unused fields</term>

            <listitem>
              <para>Here Identity passes its arguments out as results, thanks
              to the <code>Fields.ARGS</code> field declaration.</para>

              <xi:include href="identity-discard-fields-long.xml"/>

              <para>In practice the field declaration can be left out, as
              <code>Field.ARGS</code> is the default declaration for the
              Identity function. And <code>Fields.RESULTs</code> can be left
              off, as it is the default for the <classname>Every</classname>
              pipe. Thus, simpler code yields the same result:</para>

              <xi:include href="identity-discard-fields.xml"/>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>Rename all fields</term>

            <listitem>
              <para>Here Identity renames the incoming arguments. Since
              Fields.RESULTS is implied, the incoming Tuple is replaced by the
              selected arguments and given new field names as declared on
              Identity.</para>

              <xi:include href="identity-rename-fields-explicit.xml"/>

              <para>In the example above, if there were more fields than "ip"
              and "method", it would work fine - all the extra fields would be
              discarded. But if the same were true for the next example, the
              planner would fail.</para>

              <xi:include href="identity-rename-fields-long.xml"/>

              <para>Since <code>Fields.ALL</code> is the default argument
              selector for the <classname>Each</classname> pipe, it can be
              left out as shown below. Again, the above and below examples
              will fail unless there are exactly two fields in the tuples of
              the incoming stream.</para>

              <xi:include href="identity-rename-fields.xml"/>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>Rename a single field</term>

            <listitem>
              <para>Here we rename a single field and return it, along with an
              input Tuple field, as the result. All other fields are
              dropped.</para>

              <xi:include href="identity-rename-some.xml"/>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>Coerce values to specific primitive types</term>

            <listitem>
              <para>Here we replace the Tuple String values "status" and
              "size" with <classname>int</classname> and
              <classname>long</classname> values, respectively. All other
              fields are dropped.</para>

              <xi:include href="identity-coerce.xml"/>

              <para>Or we can replace just the Tuple String value "status"
              with an<classname>int</classname>, while keeping all the other
              values in the output Tuple.</para>

              <xi:include href="identity-coerce-single.xml"/>
            </listitem>
          </varlistentry>
        </variablelist></para>
    </section>

    <section>
      <title xreflabel="Debug Function" xml:id="debug-function">Debug
      Function</title>

      <para>The <classname>cascading.operation.Debug</classname> function is a
      utility function (actually, it's a <classname>Filter</classname>) that
      prints the current argument Tuple to either <code>stdout</code> or
      <code>stderr</code>. Used with one of the
      <classname>DebugLevel</classname> enum values
      (<classname>NONE</classname>, <classname>DEFAULT</classname>, or
      <classname>VERBOSE</classname>), different debug levels can be embedded
      in a pipe assembly.</para>

      <para>The example below inserts a <classname>Debug</classname> operation
      at the <classname>VERBOSE</classname> level, but configures the planner
      to remove all <classname>Debug</classname> operations from the resulting
      <classname>Flow</classname>.</para>

      <xi:include href="flow-debug.xml"/>

      <para>Note that if the above Flow is run on a cluster, the
      <code>stdout</code> on the cluster nodes will be used. Nothing from the
      debug output will display client side. Debug is only useful when testing
      things in an IDE or if the remote logs are readily available.</para>
    </section>

    <section>
      <title>Sample and Limit Functions</title>

      <para>The Sample and Limit functions are used to limit the number of
      tuples that pass through a pipe assembly.</para>

      <para><variablelist>
          <varlistentry>
            <term>Sample</term>

            <listitem>
              <para>The
              <classname>cascading.operation.filter.Sample</classname> filter
              allows a percentage of tuples to pass.</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>Limit</term>

            <listitem>
              <para>The
              <classname>cascading.operation.filter.Limit</classname> filter
              allows a set number of tuples to pass.</para>
            </listitem>
          </varlistentry>
        </variablelist></para>
    </section>

    <section>
      <title>Insert Function</title>

      <para>The <classname>cascading.operation.Insert</classname> function
      allows for insertion of constant literal values into the tuple
      stream.</para>

      <para>This is most useful when a splitting a tuple stream and one of the
      branches needs some identifying value, or when some missing parameter or
      value, like a date String for the current date, needs to be
      inserted.</para>
    </section>

    <section>
      <title>Text Functions</title>

      <para>Cascading includes a number of text functions in the
      <classname>cascading.operation.text</classname> package.</para>

      <para><variablelist>
          <varlistentry>
            <term> DateFormatter</term>

            <listitem>
              <para>The
              <classname>cascading.operation.text.DateFormatter</classname>
              function is used to convert a date timestamp to a formatted
              String. This function expects a <classname>long</classname>
              value representing the number of milliseconds since January 1,
              1970, 00:00:00 GMT/UTC, and formats the output using
              <classname>java.text.SimpleDateFormat</classname> syntax.</para>

              <xi:include href="text-format-date.xml"/>

              <para>The example above converts a <classname>long</classname>
              timestamp ("ts") to a date String.</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>DateParser</term>

            <listitem>
              <para>The
              <classname>cascading.operation.text.DateParser</classname>
              function is used to convert a text date String to a timestamp,
              using the <classname>java.text.SimpleDateFormat</classname>
              syntax. The timestamp is a <classname>long</classname> value
              representing the number of milliseconds since January 1, 1970,
              00:00:00 GMT/UTC. By default, the output is a field with the
              name "ts" (for timestamp), but this can be overridden by passing
              a declared Fields value.</para>

              <xi:include href="text-create-timestamp.xml"/>

              <para>In the example above, an Apache log-style date-time field
              is converted into a <classname>long</classname> timestamp in
              UTC.</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>FieldJoiner</term>

            <listitem>
              <para>The
              <classname>cascading.operation.text.FieldJoiner</classname>
              function joins all the values in a Tuple with a specified
              delimiter and places the result into a new field. (For the
              opposite effect, see the <classname>RegexSplitter</classname>
              function.)</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>FieldFormatter</term>

            <listitem>
              <para>The
              <classname>cascading.operation.text.FieldFormatter</classname>
              function formats Tuple values with a given String format and
              stuffs the result into a new field. The
              <classname>java.util.Formatter</classname> class is used to
              create a new formatted String.</para>
            </listitem>
          </varlistentry>
        </variablelist></para>
    </section>

    <section>
      <title>Regular Expression Operations</title>

      <para><variablelist>
          <varlistentry>
            <term>RegexSplitter</term>

            <listitem>
              <para>The
              <classname>cascading.operation.regex.RegexSplitter</classname>
              function splits an argument value based on a regex pattern
              String. (For the opposite effect, see the FieldJoiner function.)
              Internally, this function uses
              <classname>java.util.regex.Pattern.split()</classname>, and it
              behaves accordingly. By default, it splits on the TAB character
              ("\t"). If it is known that a determinate number of values will
              emerge from this function, it can declare field names. In this
              case, if the splitter encounters more split values than field
              names, the remaining values are discarded. For more information,
              see<classname>java.util.regex.Pattern.split( input, limit
              )</classname>.</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>RegexParser</term>

            <listitem>
              <para>The
              <classname>cascading.operation.regex.RegexParser</classname>
              function is used to extract a regex-matched value from an
              incoming argument value. If the regular expression is
              sufficiently complex, an <classname>int</classname> array may be
              provided to specify which regex groups should be returned in
              which field names.</para>

              <xi:include href="regex-parser.xml"/>

              <para>In the example above, a line from an Apache access log is
              parsed into its component parts. Note that the
              <classname>int[]</classname> groups array starts at 1, not 0.
              Group 0 is the whole group, so if the first field is included,
              it is a copy of "line" and not "ip".</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>RegexReplace</term>

            <listitem>
              <para>The
              <classname>cascading.operation.regex.RegexReplace</classname>
              function is used to replace a regex-matched value with a
              specified replacement value. It can operate in a "replace all"
              or "replace first" mode. For more information, see the methods
              <classname>java.util.regex.Matcher.replaceAll()</classname> and
              <classname>java.util.regex.Matcher.replaceFirst()</classname>.</para>

              <xi:include href="regex-replace.xml"/>

              <para>In the example above, all adjoined white space characters
              are replaced with a single space character.</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>RegexFilter</term>

            <listitem>
              <para>The
              <classname>cascading.operation.regex.RegexFilter</classname>
              function filters a Tuple stream based on a specified regex
              value. By default, tuples that match the given pattern are kept,
              and tuples that do not match are filtered out. This can be
              reversed by setting "removeMatch" to<code>true</code>. Also, by
              default, the whole Tuple is matched against the given regex
              String (in tab-delimited sections). If "matchEachElement" is set
              to<code>true</code>, the pattern is applied to each Tuple value
              individually. For more information, see the
              <classname>java.util.regex.Matcher.find()</classname>
              method.</para>

              <xi:include href="regex-filter.xml"/>

              <para>The above keeps all lines in which "68." appears at the
              start of the IP address.</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>RegexGenerator</term>

            <listitem>
              <para>The
              <classname>cascading.operation.regex.RegexGenerator</classname>
              function emits a new tuple for every string (found in an input
              tuple) that matches a specified regex pattern.</para>

              <xi:include href="regex-generator.xml"/>

              <para>Above each "line" in a document is parsed into unique
              words and stored in the "word" field of each result
              Tuple.</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>RegexSplitGenerator</term>

            <listitem>
              <para>The
              <classname>cascading.operation.regex.RegexSplitGenerator</classname>
              function emits a new Tuple for every split on the incoming
              argument value delimited by the given pattern String. The
              behavior is similar to the RegexSplitter function, except that
              (assuming multiple matches) RegexSplitter outputs a single tuple
              that may contain multiple values, and RegexSplitGenerator
              outputs multiple tuples that each contain only one value, as
              does <classname>RegexGenerator</classname>.</para>
            </listitem>
          </varlistentry>
        </variablelist></para>
    </section>

    <section>
      <title xreflabel="Expression Operations"
      xml:id="operation-expression">Java Expression Operations</title>

      <para>Cascading provides some support for dynamically-compiled Java
      expressions to be used in either <classname>Functions</classname> or
      <classname>Filters</classname>. This capability is provided by the
      Janino embedded Java compiler, which compiles the expressions into byte
      code for optimal processing speed. Janino is documented in detail on its
      website,<link
      xlink:href="http://www.janino.net/">http://www.janino.net/</link>.</para>

      <para>This capability allows an Operation to evaluate a suitable
      one-line Java expression, such as <code>a + 3 * 2</code> or<code>a &lt;
      7</code>, where the variable values (<code>a</code> and <code>b</code>)
      are passed in as Tuple fields. The result of the Operation thus depends
      on the evaluated result of the expression - in the first example, some
      number, and in the second, a Boolean value.</para>

      <para><variablelist>
          <varlistentry>
            <term>ExpressionFunction</term>

            <listitem>
              <para>The
              <classname>cascading.operation.expression.ExpressionFunction</classname>
              function dynamically composes a string expression when executed
              assigns argument Tuple values to the variables in the
              expression.</para>

              <xi:include href="expression-function.xml"/>

              <para>Above, we create a new String value that contains an
              expression containing values from the current Tuple. Note that
              you must declare the type for every input Tuple field so that
              the expression compiler knows how to treat the variables in the
              expression.</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>ExpressionFilter</term>

            <listitem>
              <para>The
              <classname>cascading.operation.expression.ExpressionFilter</classname>
              filter evaluates a Boolean expression, assigning argument Tuple
              values to variables in the expression. If the expression returns
              <code>true</code>, the Tuple is removed from the stream.</para>

              <xi:include href="expression-filter.xml"/>

              <para>In this example, every line in the Apache log that does
              not have a status of "200" is filtered out. ExpressionFilter
              coerces the value into the specified type if necessary to make
              the comparison - in this case, coercing the status String into
              an <classname>int</classname>.</para>
            </listitem>
          </varlistentry>
        </variablelist></para>
    </section>

    <section>
      <title>XML Operations</title>

      <para>To use XML Operations in a Cascading application, include the
      <filename>cascading-xml-x.y.z.jar</filename> in the project. When using
      the <classname>TagSoupParser</classname> operation, this module requires
      the TagSoup library, which provides support for HTML and XML "tidying".
      More information is available at the TagSoup website, <link
      xlink:href="http://home.ccil.org/~cowan/XML/tagsoup/">http://home.ccil.org/~cowan/XML/tagsoup/</link>.</para>

      <para><variablelist>
          <varlistentry>
            <term> XPathParser</term>

            <listitem>
              <para>The
              <classname>cascading.operation.xml.XPathParser</classname>
              function uses one or more XPath expressions, passed into the
              constructor, to extract one or more node values from an XML
              document contained in the passed Tuple argument, and places the
              result(s) into one or more new fields in the current Tuple. In
              this way, it effectively parses an XML document into a table of
              fields, creating one Tuple field value for every given XPath
              expression. The <classname>Node</classname> is converted to a
              String type containing an XML document. If only the text values
              are required, search on the <code>text()</code> nodes, or
              consider using XPathGenerator to handle multiple
              <classname>NodeList</classname> values. If the returned result
              of an XPath expression is a <classname>NodeList</classname>,
              only the first <classname>Node</classname> is used for the field
              value and the rest are ignored.</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>XPathGenerator</term>

            <listitem>
              <para>Similar to XPathParser, the
              <classname>cascading.operation.xml.XPathGenerator</classname>
              function outputs a new <classname>Tuple</classname> for every
              <classname>Node</classname> returned by the given XPath
              expression from the XML in the current Tuple.</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>XPathFilter</term>

            <listitem>
              <para>The
              <classname>cascading.operation.xml.XPathFilter</classname>
              filter removes a Tuple if the specified XPath expression returns
              <code>false</code>. Set the removeMatch parameter to
              <code>true</code> if the filter should be reversed, i.e., to
              keep only those Tuples where the XPath expression returns
              <code>true</code>.</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>TagSoupParser</term>

            <listitem>
              <para>The
              <classname>cascading.operation.xml.TagSoupParser</classname>
              function uses the TagSoup library to convert incoming HTML to
              clean XHTML. Use the <code>setFeature( feature, value )</code>
              method to set TagSoup-specific features, which are documented on
              the TagSoup website.</para>
            </listitem>
          </varlistentry>
        </variablelist></para>
    </section>

    <section>
      <title>Assertions</title>

      <para>Cascading Stream Assertions are used to build robust reusable pipe
      assemblies. If desired, they can be planned out of a Flow instance at
      runtime. For more information, see the section on<xref
      linkend="stream-assertions"/>. Below we describe the Assertions
      available in the core library.</para>

      <para><variablelist>
          <varlistentry>
            <term>AssertEquals</term>

            <listitem>
              <para>The
              <classname>cascading.operation.assertion.AssertEquals</classname>
              Assertion asserts that the number of values given on the
              constructor is equal to the number of argument Tuple values, and
              that each constructor value <code>.equals()</code> its
              corresponding argument value.</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>AssertNotEquals</term>

            <listitem>
              <para>The
              <classname>cascading.operation.assertion.AssertNotEquals</classname>
              Assertion asserts that the number of values given on the
              constructor is equal to the number of argument Tuple values and
              that each constructor value is not <code>.equals()</code> to its
              corresponding argument value.</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>AssertEqualsAll</term>

            <listitem>
              <para>The
              <classname>cascading.operation.assertion.AssertEqualsAll</classname>
              Assertion asserts that every value in the argument Tuple
              <code>.equals()</code> the single value given on the
              constructor.</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>AssertExpression</term>

            <listitem>
              <para>The
              <classname>cascading.operation.assertion.AssertExpression</classname>
              Assertion dynamically resolves a given Java expression (see
              <xref linkend="operation-expression"/>) using argument Tuple
              values. Any Tuple that returns <code>true</code> for the given
              expression passes the assertion.</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>AssertMatches</term>

            <listitem>
              <para>The
              <classname>cascading.operation.assertion.AssertMatches</classname>
              Assertion matches the given regular expression pattern String
              against the entire argument Tuple. The comparison is made
              possible by concatenating all the fields of the Tuple, separated
              by the TAB character (\t). If a match is found, the Tuple passes
              the assertion.</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>AssertMatchesAll</term>

            <listitem>
              <para>The
              <classname>cascading.operation.assertion.AssertMatchesAll</classname>
              Assertion matches the given regular expression pattern String
              against each argument Tuple value individually.</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>AssertNotNull</term>

            <listitem>
              <para>The
              <classname>cascading.operation.assertion.AssertNotNull</classname>
              Assertion asserts that every position/field in the argument
              Tuple is not <code>null</code>.</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>AssertNull</term>

            <listitem>
              <para>The
              <classname>cascading.operation.assertion.AssertNull</classname>
              Assertion asserts that every position/field in the argument
              Tuple is <code>null</code>.</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>AssertSizeEquals</term>

            <listitem>
              <para>The
              <classname>cascading.operation.assertion.AssertSizeEquals</classname>
              Assertion asserts that the current Tuple in the tuple stream is
              exactly the given size. Size, here, is the number of fields in
              the Tuple, as returned by<code>Tuple.size()</code>. Note that
              some or all fields may be <code>null</code>.</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>AssertSizeLessThan</term>

            <listitem>
              <para>The
              <classname>cascading.operation.assertion.AssertSizeLessThan</classname>
              Assertion asserts that the current Tuple in the stream has a
              size less than (<code>&lt;</code>) the given size. Size, here,
              is the number of fields in the Tuple, as returned by
              <code>Tuple.size()</code>. Note that some or all fields may be
              <code>null</code>.</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>AssertSizeMoreThan</term>

            <listitem>
              <para>The
              <classname>cascading.operation.assertion.AssertSizeMoreThan</classname>
              Assertion asserts that the current Tuple in the stream has a
              size greater than (<code>&gt;</code>) the given size. Size,
              here, is the number of fields in the Tuple, as returned by
              <code>Tuple.size()</code>. Note that some or all fields may be
              <code>null</code>.</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>AssertGroupSizeEquals</term>

            <listitem>
              <para>The
              <classname>cascading.operation.assertion.AssertGroupSizeEquals</classname>
              Group Assertion asserts that the number of items in the current
              grouping is equal to (<code>==</code>) the given size. If a
              pattern String is given, only grouping keys that match the
              regular expression will have this assertion applied where
              multiple key values are delimited by a TAB character.</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>AssertGroupSizeLessThan</term>

            <listitem>
              <para>The
              <classname>cascading.operation.assertion.AssertGroupSizeEquals</classname>
              Group Assertion asserts that the number of items in the current
              grouping is less than (<code>&lt;</code>) the given size. If a
              pattern String is given, only grouping keys that match the
              regular expression will have this assertion applied where
              multiple key values are delimited by a TAB character.</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>AssertGroupSizeMoreThan</term>

            <listitem>
              <para>The
              <classname>cascading.operation.assertion.AssertGroupSizeEquals</classname>
              Group Assertion asserts that the number of items in the current
              grouping is greater than (<code>&gt;</code>) the given size. If
              a pattern String is given, only grouping keys that match the
              regular expression will have this assertion applied where
              multiple key values are delimited by a TAB character.</para>
            </listitem>
          </varlistentry>
        </variablelist></para>
    </section>

    <section>
      <title>Logical Filter Operators</title>

      <para>The logical <classname>Filter</classname> operators allow you to
      combine multiple filters to run in a single Pipe, instead of chaining
      multiple Pipes together to get the same logical result.</para>

      <para><variablelist>
          <varlistentry>
            <term>And</term>

            <listitem>
              <para>The <classname>cascading.operation.filter.And</classname>
              <classname>Filter</classname> performs a logical "and" on the
              results of the constructor-provided
              <classname>Filter</classname> instances. That is, if
              <methodname>Filter#isRemove()</methodname> returns
              <code>true</code> for all of the given instances, this filter
              returns<code>true</code>.</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>Or</term>

            <listitem>
              <para>The <classname>cascading.operation.filter.Or</classname>
              <classname>Filter</classname> performs a logical "or" on the
              results of the constructor-provided
              <classname>Filter</classname> instances. That is, if
              <methodname>Filter#isRemove()</methodname> returns
              <code>true</code> for any of the given instances, this filter
              returns<code>true</code>.</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>Not</term>

            <listitem>
              <para>The <classname>cascading.operation.filter.Not</classname>
              <classname>Filter</classname> performs a logical "not"
              (negation) on the results of the constructor-provided
              <classname>Filter</classname> instance. That is, if
              <methodname>Filter#isRemove()</methodname> returns
              <code>true</code> for the given instance, this filter returns
              <code>false</code>, and if
              <methodname>Filter#isRemove()</methodname> returns
              <code>false</code> for the given instance, this filter returns
              <code>true</code>.</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>Xor</term>

            <listitem>
              <para>The <classname>cascading.operation.filter.Xor</classname>
              <classname>Filter</classname> performs a logical "xor"
              (exclusive or) on the results of the constructor-provided
              <classname>Filter</classname> instances. Xor can only be applied
              to two instances at a time. It returns <code>true</code> if the
              two instances have different truth values, and
              <code>false</code> if they have the same truth value. That is,
              if <methodname>Filter.isRemove()</methodname> returns
              <code>true</code> for both, or returns <code>false</code> for
              both, this filter returns <code>false</code>; otherwise it
              returns <code>true</code>.</para>
            </listitem>
          </varlistentry>
        </variablelist></para>

      <example>
        <title>Combining Filters</title>

        <xi:include href="filter-and.xml"/>
      </example>

      <para>The example above performs a logical "and" on the two filters.
      Both must be satisfied for the data to pass through this one
      Pipe.</para>
    </section>
  </chapter>

  <chapter>
    <info>
      <title>Built-in Assemblies</title>
    </info>

    <para>Introductory text here, explaining about built-in assemblies.</para>

    <section>
      <title xreflabel="AggregateBy" xml:id="aggregate-by">AggregateBy</title>

      <para>text here</para>

      <section>
        <title xml:id="AverageBy">AverageBy</title>

        <para>text here</para>
      </section>

      <section>
        <title xml:id="CountBy">CountBy</title>

        <para>text here</para>
      </section>

      <section>
        <title xml:id="SumBy">SumBy</title>

        <para>text here</para>
      </section>
    </section>

    <section>
      <title>Coerce</title>

      <para>text here</para>
    </section>

    <section>
      <title>Discard</title>

      <para>text here</para>
    </section>

    <section>
      <title>Rename</title>

      <para>text here</para>
    </section>

    <section>
      <title>Retain</title>

      <para>text here</para>
    </section>

    <section>
      <title>Unique</title>

      <para>text here</para>
    </section>
  </chapter>

  <chapter>
    <info>
      <title> Best Practices</title>
    </info>

    <section>
      <title>Unit Testing</title>

      <para>Discrete testing of all Operations, pipe assemblies, and
      applications is a must. The
      <classname>cascading.CascadingTestCase</classname> provides a number of
      helper methods.</para>

      <para>When testing custom Operations, use the
      <methodname>invokeFunction()</methodname>,
      <methodname>invokeFilter()</methodname>,
      <methodname>invokeAggregator()</methodname>, and
      <methodname>invokeBuffer()</methodname> methods.</para>

      <para>When testing Flows, use the
      <methodname>validateLength()</methodname> methods. There are quite a few
      of them, and collectively they offer great flexibility. All of them read
      the sink tap, validate that it is the correct length and has the correct
      Tuple size, and check to see whether the values match a given regular
      expression pattern.</para>

      <para>New to Cascading 2, it is possible to write tests that are
      independent of the underlying platform or mode. Any unit-test should
      sub-class <classname>cascading.PlatformTestCase</classname> and apply
      the <classname>PlatformRunner.Platform</classname> annotion. </para>

      <para>For example, the following annotation,
      <code>@PlatformRunner.Platform({LocalPlatform.class,
      HadoopPlatform.class})</code>, will convince the PlatformTestCase to run
      all the unit tests defined on the sub-class with the
      <classname>LocalPlatform</classname> and
      <classname>HadoopPlatform</classname> platform instances. </para>

      <para>See the existing Cascading unit tests for examples.</para>

      <para>To use any of these helper classes, make sure that
      <filename>cascading-test-x.y.z.jar</filename> is in your testing class
      path.</para>
    </section>

    <section>
      <title>Local versus Distributed Mode</title>

      <para><?oxy_comment_start author="Jim" timestamp="20120326T232235-0700" comment="local v. distributed modes here?"?>
      Chris, we can probably cover the how-to of local v. distributed mode in
      Chapter 4, and put any tips, fine points, or best practices
      here.<?oxy_comment_end ?></para>
    </section>

    <section>
      <title>Flow Granularity</title>

      <para>Although using one large <classname>Flow</classname> may result in
      slightly more efficient performance, it's advisable to use a more
      modular and flexible approach, creating medium to small Flows with
      well-defined responsibilities, and passing all the resulting
      interdependent Flows to a <classname>Cascade</classname> to sequence and
      execute as a single unit. Similarly, using the
      <classname>TextDelimited</classname> <classname>Scheme</classname> (or
      any custom format for long term archival) between
      <classname>Flow</classname> instances allows you to hand off
      intermediate data to other systems for reporting or QA purposes,
      incurring a minimal performance penalty while remaining compatible with
      other tools.</para>
    </section>

    <section>
      <title>SubAssemblies, not Factories</title>

      <para>When developing your applications, use
      <classname>SubAssembly</classname> sub-classes, not "factory" methods.
      The resulting code is much easier to read and test.</para>

      <para>It's worth noting that the <classname>Object</classname>
      constructors are "factories", so there isn't much reason to build
      frameworks to duplicate what a constructor already does. Of course there
      are exceptions, but in practice they are rare when you have the option
      to use a <classname>SubAssembly</classname>.</para>
    </section>

    <section>
      <title>Logical Responsibilities for SubAssemblies</title>

      <para>SubAssembies provide a very convenient means to co-locate similar
      or related responsibilities into a single place. For example, it's
      simple to use a <classname>ParsingSubAssembly</classname> and a
      <classname>RulesSubAssembly</classname>, where the first is responsible
      solely for parsing incoming <classname>Tuple</classname> streams (log
      files for example), and the second applies rules to decide whether a
      given <classname>Tuple</classname> should be discarded or marked as
      bad.</para>

      <para>Additionally, in your unit tests you can create a
      <classname>TestAssertionsSubAssembly</classname> that simply inlines
      various <classname>ValueAssertions</classname> and
      <classname>GroupAssertions</classname>. The practice of inlining
      Assertions directly in your SubAssemblies is also important, but
      sometimes it makes sense to have more tests outside of the business
      logic.</para>
    </section>

    <section>
      <title>Java Operators in Field Names</title>

      <para>There are a number of Operations in Cascading (e.g.,
      <classname>ExpressionFunction</classname> and
      <classname>ExpressionFilter</classname>) that compile and apply Java
      expressions on the fly. In these expressions, Operation argument field
      names are used as variable names in the expression. For this reason,
      take care to create field names that don't contain characters which will
      cause compilation errors if they are used in an expression. For example,
      "first-name" is a valid field name for use with Cascading, but might
      result in the expression <code>first-name.trim()</code>, which will
      cause a compilation error.</para>
    </section>

    <section>
      <title>Debugging Planner Failures</title>

      <para>The <classname>FlowConnector</classname> will sometimes fail when
      attempting to plan a <classname>Flow</classname>. If the error message
      given by <classname>PlannerException</classname> is vague, use the
      method <code>PlannerException.writeDOT()</code> to export a
      representation of the internal pipe assembly. DOT files can be opened by
      GraphViz and OmniGraffle. These plans are only partial, but you will be
      able to see where the Cascading planner failed.</para>

      <para>Note that you can also create a DOT file from a
      <classname>Flow</classname>, by using
      <code>Flow.writeDOT()</code>.</para>
    </section>

    <section>
      <title>Optimizing Joins</title>

      <para>When joining two streams via a <classname>CoGroup</classname>
      <classname>Pipe</classname>, try to put the largest of the streams in
      the leftmost argument to the <classname>CoGroup</classname>. The reason
      for this is that joining multiple streams requires some accumulation of
      values before the join operator can begin, but the leftmost stream is
      not accumulated, so this technique should improve the performance of
      most joins.</para>
    </section>

    <section>
      <title>Debugging Streams</title>

      <para>When creating complex assemblies, it's safe to embed
      <classname>Debug</classname> operations (see<xref
      linkend="debug-function"/>) at appropriate debug levels as needed. Use
      the planner to remove them at runtime for production and staging runs,
      to avoid wasting resources.</para>
    </section>

    <section>
      <title xreflabel="Handling Good and Bad Data" xml:id="handling-bad-data"
      xml:lang="">Handling Good and Bad Data</title>

      <para>It's very common when processing raw data streams to encounter
      data that is corrupt or malformed in some way. For instance, bad content
      may be fetched from the web via a crawler upstream, or a bug may have
      leaked corrupt data into a browser widget somewhere that sends user
      behavior information back for analysis. Whatever the cause, it's a good
      practice to define a set of rules for identifying and discarding
      questionable records.</para>

      <para>It is tempting to simply throw an exception and have a Trap
      capture the offending<classname>Tuple</classname>, but Traps were not
      designed as a filtering mechanism, and consequently much valuable
      information would be lost.</para>

      <para>Instead of traps, use filters. Create a
      <classname>SubAssembly</classname> that applies rules to the stream by
      setting a binary field that marks the tuple as good or bad. After all
      the rules are applied, split the stream based on the value of the good
      or bad Boolean value. Consider setting a reason field that states why
      the Tuple was marked bad.</para>
    </section>

    <section>
      <title>Maintaining State in Operations</title>

      <para>When creating custom Operations
      (<classname>Function</classname>,<classname> Filter</classname>,
      <classname>Aggregator</classname>, or <classname>Buffer</classname>) do
      not store operation state in class fields. For example, if implementing
      a custom "counter" <classname>Aggregator</classname>, do not create a
      field named "count" and increment it on every
      <methodname>Aggregator.aggregate()</methodname> call. There is no
      guarantee your Operation will be called from a single thread in a JVM,
      future version of Hadoop or Cascading local mode could execute the same
      operation from multiple threads.</para>
    </section>

    <section>
      <title>Custom Types</title>

      <para>It is generally frowned upon to pass a custom class through a
      Tuple stream. One one hand this increases coupling of custom Operations
      to a particular type, and it removes opportunities for reducing the
      amount of data that passes over the network.</para>

      <para>To overcome the first objection, with every custom type with
      multiple instance fields, attempt to provide Functions that can promote
      a value from the custom object to a position in a Tuple or demote the
      Tuple value to a particular field back into the custom type. This allows
      existing operations (like <classname>ExpressionFunction</classname> or
      <classname>RegexFilter</classname>) to operate on values owned by a
      custom type. </para>

      <para>For example, if you have a <classname>Person</classname> object,
      have a Function named <classname>GetPersonAge</classname> that takes
      Person as an argument and only returns the age as the result. The next
      operation can then Filter all Persons based on their age. This may seem
      like more work and less effiicient, but it keeps your application
      flexible and reduces the amount of duplicate code (the only alternative
      here is to create a <classname>PersonAgeFilter</classname> which results
      in one more thing to test).</para>
    </section>

    <section>
      <title>Fields Constants</title>

      <para>Instead of having String field names strewn about, create an
      Interface that holds a constant value for each field name:</para>

      <para><code>public static Fields FIRST_NAME = new Fields( "firstname" );
      </code></para>

      <para>Using the Fields class, instead of String, allows for building
      more complex constants:</para>

      <para><code>public static Fields NAME = FIRST_NAME.append( LAST_NAME );
      </code></para>
    </section>

    <section>
      <title>Checking the Source Code</title>

      <para>When in doubt, look at the Cascading source code. If something is
      not documented in this User Guide or Javadoc, and it's a feature of
      Cascading, the source code will give you clear instructions on what to
      do or expect.</para>
    </section>
  </chapter>

  <chapter>
    <info>
      <title xml:id="extending-cascading">Extending Cascading</title>
    </info>

    <para>Possible introductory text here.</para>

    <section>
      <title>Scripting</title>

      <para>The Cascading API was designed with scripting in mind. Any
      Java-compatible scripting language can import and instantiate Cascading
      classes, create pipe assemblies and flows, and execute those flows. And
      if the scripting language in question supports Domain Specific Language
      (DSL) creation, users can create their own DSL's to handle common
      idioms.</para>

      <para>The Cascading website includes information on scripting language
      bindings that are publicly available.</para>
    </section>

    <section>
      <title xreflabel="Custom Types" xml:id="custom-types">Custom taps and
      schemes</title>

      <para>Cascading is designed to be easily configured and enhanced by
      developers. In addition to creating custom Operations, developers can
      create custom <classname>Tap</classname> and
      <classname>Scheme</classname> types that let applications connect to
      external systems.</para>

      <para>A Tap represents something physical, like a file or a database
      table. Consequently, Tap implementations are responsible for life-cycle
      issues around the resource they represent, such as tests for existence,
      or eventual deletion.</para>

      <para>A scheme represents a format or representation - such as a text
      format for a file, the columns in a table, etc. Schemes are used to
      convert between the source data's native format and a
      <classname>cascading.tuple.Tuple</classname> instance.</para>

      <para>Creating custom taps and schemes can be an involved process. If
      using the Cascading Hadoop mode, requires some knowledge of Hadoop and
      the Hadoop FileSystem API. If a flow needs to support a new file system,
      passing a fully-qualified URL to the <classname>Hfs</classname>
      constructor may be sufficient - the <classname>Hfs</classname> tap will
      look up a file system based on the URL scheme via the Hadoop FileSystem
      API. If not, a new system is commonly constructed by subclassing the
      <classname>cascading.tap.Hfs</classname> class.</para>

      <para>Delegating to the Hadoop FileSystem API is not a strict
      requirement. But if not using it, the developer must implement a Hadoop
      <classname>org.apache.hadoop.mapred.InputFormat</classname> and/or
      <classname>org.apache.hadoop.mapred.OutputFormat</classname> so that
      Hadoop knows how to split and handle the incoming/outgoing data. The
      custom <classname>Scheme</classname> is responsible for setting the
      <classname>InputFormat</classname> and
      <classname>OutputFormat</classname> on the
      <classname>JobConf</classname>, via the
      <methodname>sinkInit</methodname> and
      <methodname>sourceInit</methodname> methods.</para>

      <para>For examples of how to implement a custom tap and scheme, see the
      <link xlink:href="???">Cascading Modules</link> page.</para>
    </section>

    <section>
      <title>Custom Types and Serialization</title>

      <para>The <classname>Tuple</classname> class is a generic container for
      all <classname>java.lang.Object</classname> instances. Consequently, any
      primitive value or custom Class can be stored in a
      <classname>Tuple</classname> instance, that is, returned by a
      <classname>Function</classname>, <classname>Aggregator</classname>, or
      <classname>Buffer</classname> as a result value.</para>

      <para>But for this to work, when working with the Cascading Hadoop mode,
      any Class that isn't a primitive value or a Hadoop
      <classname>Writable</classname> type requires a corresponding Hadoop
      serialization class, registered in the Hadoop configuration files, for
      your cluster. Hadoop <classname>Writable</classname> types work because
      there is already a generic serialization implementation built into
      Hadoop. See the Hadoop documentation for information on registering a
      new serialization helper or creating <classname>Writable</classname>
      types. Cascading automatically inherits serialization implementations if
      they are registered.</para>

      <para>During serialization and deserialization of
      <classname>Tuple</classname> instances that contain custom types, the
      Cascading <classname>Tuple</classname> serialization framework must
      store the class name (as a <classname>String</classname>) before
      serializing the custom object. This can be very space inefficient. To
      overcome this, custom types can add the
      <classname>SerializationToken</classname> Java annotation to the custom
      type class. The <classname>SerializationToken</classname> annotation
      expects two arrays - one of integers that are used as tokens, and one of
      Class name strings. Both arrays must be the same size. The integer
      tokens must all have values of 128 or greater, since the first 128
      values are reserved for internal use.</para>

      <para>During serialization and deserialization, the token values are
      used instead of the <classname>String</classname> Class names, in order
      to reduce the amount of storage used.</para>

      <para>Serialization tokens may also be stored in the Hadoop config files
      or set as a property passed to the <classname>FlowConnector</classname>,
      with the property name <code>cascading.serialization.tokens</code>. The
      value of this property is a comma separated list of
      <code>token=classname</code> values.</para>

      <para>Note that Cascading natively serializes/deserializes all
      primitives and byte arrays (<code>byte[]</code>), if the developer
      registers the <classname>BytesSerialization</classname> class with
      <code>TupleSerialization.addSerialization(properties,
      BytesSerialization.class.getName()</code>. It uses the token 127 for the
      Hadoop <classname>BytesWritable</classname> class.</para>

      <para>By default, Cascading lazily deserializes each element in the
      Tuple when sorting for comparison when Hadoop sorts keys during the
      "shuffle" phase. </para>

      <para>Along with custom serialization, Cascading supports lazy
      deserialization during Tuple comparison for custom types. This is
      accomplished by implementing the <classname>StreamComparator</classname>
      interface. See the Javadoc for detailed instructions on implemention,
      and the unit tests for examples.</para>
    </section>

    <section>
      <title>Custom Comparators and Hashing</title>

      <para>Frequently objects in a <classname>Tuple</classname> will be
      compared to other instances in a second <classname>Tuple</classname>.
      This is especially true during the "sort" phase in Cascading Hadoop mode
      when a <classname>GroupBy</classname> or <classname>CoGroup</classname>
      pipe is being processed. By default Hadoop and Cascading will use the
      native <classname>Object</classname> methods
      <methodname>equals()</methodname> and
      <methodname>hashCode()</methodname> to compare two values and get a
      consistent hash code for a give value, respectively.</para>

      <para>In order to override this behavior, a custom
      <classname>java.util.Comparator</classname> class can be created and
      passed to cascading. This <classname>Comparator</classname> will become
      responsbile for comparing all the values in a given field in a Tuple.
      Thus if secondary sorting a collection of custom
      <classname>Person</classname> objects in a
      <classname>GroupBy</classname>, the <classname>Fields</classname>
      instance defining the fields to perform the secondary sort can be given
      a copy of the <classname>Comparator</classname> via the
      <code>Fields.setComparator()</code> method.</para>

      <para>Alternatively, a default <classname>Comparator</classname> may be
      set to be used by a <classname>Flow</classname>, or set on a given
      <classname>Pipe</classname> instance to be used locally. This can be
      done by calling <code>Tuples.setDefaultTupleElementComparator()</code>
      on a <classname>Properties</classname> instance or using the property
      key <code>cascading.tuple.element.comparator.default</code>. </para>

      <para>If how the hash code must also be customized, the custom
      Comparator may also imiplement the interface
      <classname>cascading.tuple.Hasher</classname>. See the
      <classname>Hasher</classname> Javadoc for more details.</para>
    </section>
  </chapter>

  <chapter>
    <info>
      <title>CookBook</title>
    </info>

    <para>This chapter discusses some common idioms used in Cascading
    applications.</para>

    <section>
      <title>Tuples and Fields</title>

      <para><variablelist>
          <varlistentry>
            <term>Copy a Tuple instance</term>

            <listitem>
              <xi:include href="cookbook-copy.xml"/>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>Nest a Tuple instance within a Tuple</term>

            <listitem>
              <xi:include href="cookbook-nest.xml"/>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>Build a longer Fields instance</term>

            <listitem>
              <xi:include href="cookbook-fieldsappend.xml"/>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>Remove a field from a longer Fields instance</term>

            <listitem>
              <xi:include href="cookbook-fieldssubtract.xml"/>
            </listitem>
          </varlistentry>
        </variablelist></para>
    </section>

    <section>
      <title>Stream Shaping</title>

      <para><variablelist>
          <varlistentry>
            <term>Split (branch) a Tuple Stream</term>

            <listitem>
              <xi:include href="cookbook-split.xml"/>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>Copy a field value</term>

            <listitem>
              <xi:include href="cookbook-copyfield.xml"/>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>Discard (drop) a field</term>

            <listitem>
              <xi:include href="cookbook-discardfield.xml"/>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>Retain (keep) a field</term>

            <listitem>
              <xi:include href="cookbook-retainfield.xml"/>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>Rename a field</term>

            <listitem>
              <xi:include href="cookbook-renamefield.xml"/>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>Coerce field values from Strings to primitives</term>

            <listitem>
              <xi:include href="cookbook-coercefields.xml"/>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>Insert constant values into a stream</term>

            <listitem>
              <xi:include href="cookbook-insertvalue.xml"/>
            </listitem>
          </varlistentry>
        </variablelist></para>
    </section>

    <section>
      <title>Common Operations</title>

      <para><variablelist>
          <varlistentry>
            <term>Parse a String date/time value</term>

            <listitem>
              <xi:include href="cookbook-parsedate.xml"/>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>Format a time-stamp to a date/time value</term>

            <listitem>
              <xi:include href="cookbook-formatdate.xml"/>
            </listitem>
          </varlistentry>
        </variablelist>Stream Ordering</para>
    </section>

    <section>
      <para>
        <variablelist>
          <varlistentry>
            <term>Remove duplicate tuples in a stream</term>

            <listitem>
              <xi:include href="cookbook-distinctgroup.xml"/>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>Create a list of unique values</term>

            <listitem>
              <xi:include href="cookbook-distinctvalue.xml"/>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>Find first occurrence in time of a unique value</term>

            <listitem>
              <xi:include href="cookbook-distinctorder.xml"/>
            </listitem>
          </varlistentry>
        </variablelist>
      </para>
    </section>

    <section>
      <title>API Usage</title>

      <para><variablelist>
          <varlistentry>
            <term>Pass properties to a custom Operation</term>

            <listitem>
              <xi:include href="cookbook-passproperties.xml"/>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>Bind multiple sources and sinks to a Flow</term>

            <listitem>
              <xi:include href="cookbook-sourcessinks.xml"/>
            </listitem>
          </varlistentry>
        </variablelist></para>
    </section>
  </chapter>

  <chapter>
    <info>
      <title>How It Works</title>
    </info>

    <section>
      <title xreflabel="MapReduce Job Planner" xml:id="job-planner">MapReduce
      Job Planner</title>

      <para>The MapReduce Job Planner is an internal feature of
      Cascading.</para>

      <para>When a collection of functions, splits, and joins are all tied up
      together into a "pipe assembly", the FlowConnector object is used to
      create a new Flow instance against input and output data paths. This
      Flow is a single Cascading job.</para>

      <para>Internally, the FlowConnector employs an intelligent planner to
      convert the pipe assembly to a graph of dependent MapReduce jobs that
      can be executed on a Hadoop cluster.</para>

      <para>All this happens behind the scenes - as does the scheduling of the
      individual MapReduce jobs, as well as the cleanup of intermediate data
      sets that bind the jobs together.</para>

      <para><inlinemediaobject>
          <imageobject role="fo">
            <imagedata align="center" contentwidth="5.5in"
                       fileref="images/planned-flow.svg"/>
          </imageobject>

          <imageobject role="html">
            <imagedata align="center" contentwidth="5.5in"
                       fileref="images/planned-flow.png"/>
          </imageobject>
        </inlinemediaobject></para>

      <para>The diagram above shows how a typical Flow is partitioned into
      MapReduce jobs. Every job is delimited by a temporary file that serves
      as the sink from the first job and the source for the next.</para>

      <para>To create a visualization of how your Flows are partitioned, call
      the <classname>Flow#writeDOT()</classname> method. This writes a <link
      xlink:href="http://en.wikipedia.org/wiki/DOT_language">DOT </link> file
      out to the path specified, which can be viewed in a graphics package
      like OmniGraffle or Graphviz.</para>
    </section>

    <section>
      <title xreflabel="Topological Scheduling" xml:id="cascade-scheduler"
      xml:lang="">The Cascade Topological Scheduler</title>

      <para>Cascading has a simple class, <classname>Cascade</classname> ,
      that executes a collection of Cascading Flows on a target cluster in
      dependency order.</para>

      <para>Consider the following example.</para>

      <itemizedlist>
        <listitem>
          <para>Flow 1 reads input file A and outputs B.</para>
        </listitem>

        <listitem>
          <para>Flow 2 expects input B and outputs C and D.</para>
        </listitem>

        <listitem>
          <para>Flow 3 expects input C and outputs E.</para>
        </listitem>
      </itemizedlist>

      <para>A <classname>Cascade</classname> is constructed through the
      <classname>CascadeConnector</classname> class, by building an internal
      graph that makes each Flow a "vertex", and each file an "edge". A
      topological walk on this graph will touch each vertex in order of its
      dependencies. When a vertex has all its incoming edges (i.e., files)
      available, it is scheduled on the cluster.</para>

      <para>In the example above, Flow 1 goes first, Flow 2 goes second, and
      Flow 3 is last.</para>

      <para>If two or more Flows are independent of one another, they are
      scheduled concurrently.</para>

      <para>And by default, if any outputs from a Flow are newer than the
      inputs, the Flow is skipped. The assumption is that the Flow was
      executed recently, since the output isn't stale. So there is no reason
      to re-execute it and use up resources or add time to the job. This is
      similar behavior a compiler would exhibit if a source file wasn't
      updated before a recompile.</para>

      <para>This is very handy if you have a large set of jobs, with varying
      interdependencies between them, that needs to be executed as a logical
      unit. Just pass them to the CascadeConnector and let it sort them all
      out.</para>
    </section>

    <section>
      <title><?oxy_comment_start author="Jim" timestamp="20120403T002605-0700" comment="Chris - another place you could talk about Operating Mode if you want to.  Otherwise let&apos;s delete this section."?>Operating
      Mode<?oxy_comment_end ?></title>

      <para>This section is probably unnecessary but I'm throwing it in just
      in case you want to talk about how this works. Local v. distributed mode
      are or can be mentioned in the Hadoop section of the introduction,
      described in some detail in the Execution section, and discussed pro/con
      in the Best Practices section, just after Testing.</para>
    </section>
  </chapter>
</book>
