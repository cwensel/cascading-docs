<?xml version="1.0" encoding="UTF-8"?>
<!--
  ~ Copyright (c) 2007-2012 Concurrent, Inc. All Rights Reserved.
  ~
  ~ Project and contact information: http://www.concurrentinc.com/
  -->
<book version="5.0" xml:lang="en" xmlns="http://docbook.org/ns/docbook"
      xmlns:xlink="http://www.w3.org/1999/xlink"
      xmlns:xi="http://www.w3.org/2001/XInclude"
  >
  <info>
    <title>Cascading 2 - User Guide</title>

    <pubdate>March, 2012</pubdate>

    <copyright>
      <year>2007-2012</year>

      <holder>Concurrent, Inc.</holder>
    </copyright>

    <releaseinfo>V 2.01</releaseinfo>

    <productname>Cascading</productname>

    <authorgroup>
      <author>
        <orgname>Concurrent, Inc.</orgname>
      </author>
    </authorgroup>

    <mediaobject>
      <imageobject role="fo">
        <imagedata contentwidth="1in" fileref="images/cascading-logo.svg"/>
      </imageobject>

      <imageobject role="html">
        <imagedata contentwidth="1in" fileref="images/cascading-logo.png"/>
      </imageobject>
    </mediaobject>
  </info>

  <toc/>

  <chapter>
    <info>
      <title>About Cascading</title>
    </info>

    <section>
      <title>What is Cascading?</title>

      <para>Cascading is a query API and query planner used for defining,
        sharing, and executing data-processing workflows on a distributed data
        grid or cluster. Cascading adds an abstraction layer to the Apache
        Hadoop platform, greatly simplifying Hadoop application development, job
        creation, and job scheduling. Cascading can be used locally for
        development and testing, or on a cluster for production purposes, but
        Hadoop must be installed in either case.
      </para>
    </section>

    <section>
      <title>Usage Scenarios</title>

      <section>
        <title>Why use Cascading?</title>

        <para>Cascading was developed to allow organizations to rapidly
          develop complex data processing applications with Hadoop. The need for
          Cascading is typically driven by one of two cases:
        </para>

        <para>
          <emphasis role="bold">Increasing data size</emphasis>
          exceeds
          the processing capacity of a single computing system. In response,
          developers may adopt Apache Hadoop as the base computing
          infrastructure, but discover that developing useful applications on
          Hadoop is not trivial. Cascading eases the burden on these developers
          and allows them to rapidly create, refactor, test, and execute complex
          applications that scale linearly across a cluster of computers.
        </para>

        <para>
          <emphasis role="bold">Increasing process complexity in data
            centers
          </emphasis>
          results in one-off data-processing applications
          sprawling haphazardly onto any available disk space or CPU. Apache
          Hadoop solves the problem with its Global Namespace file system, which
          provides a single reliable storage framework. In this scenario,
          Cascading eases the learning curve for developers as they convert
          their existing applications for execution on a Hadoop cluster. In
          addition, it lets developers create reusable libraries and
          applications for use by analysts, who use them to extract data from
          the Hadoop file system.
        </para>
      </section>

      <section>
        <title>Who are the users?</title>

        <para>Cascading users typically fall into three roles:</para>

        <para>
          <emphasis role="bold">The application Executor</emphasis>
          is a
          person (e.g., a developer or analyst) or process (e.g., a cron job)
          that runs a data processing application on a given cluster. This is
          typically done via the command line, using a pre-packaged Java Jar
          file compiled against the Apache Hadoop and Cascading libraries. The
          application may accept command line parameters to customize it for a
          given execution, and generally results in a data-set to be exported
          from the Hadoop file system for some specific purpose.
        </para>

        <para>
          <emphasis role="bold">The process Assembler</emphasis>
          is a
          person who assembles data processing workflows into unique
          applications. This work is generally a development task that involves
          chaining together operations to act on one or more input data sets,
          producing one or more output data sets. This can be done with the raw
          Java Cascading API, or with a scripting language such as
          Cascalog/Clojure, Groovy, JRuby, or Jython.
        </para>

        <para>
          <emphasis role="bold">The operation Developer</emphasis>
          is a
          person who writes individual functions or operations (typically in
          Java) or reusable sub-assemblies that act on the data that passes
          through the data processing workflow. A simple example would be a
          parser that takes a string and converts it to an Integer. Operations
          are equivalent to Java functions in the sense that they take input
          arguments and return data. And they can execute at any granularity,
          from simply parsing a string to performing complex procedures on the
          argument data using third-party libraries.
        </para>

        <para>All three roles can be filled by a developer, but because
          Cascading supports a clean separation of these responsibilities, some
          organizations may choose to use non-developers to run ad-hoc
          applications or build production processes on a Hadoop cluster.
        </para>
      </section>
    </section>

    <section>
      <title>What is Apache Hadoop?</title>

      <para>From the Hadoop website, it<quote>is a software platform that
        lets one easily write and run applications that process vast amounts of
        data</quote>. Hadoop does this by providing a storage layer that holds
        vast amounts of data, and an execution layer that runs an application in
        parallel across the cluster, using coordinated subsets of the stored
        data.
      </para>

      <para>The storage layer, called the Hadoop File System (HDFS), looks
        like a single storage volume that has been optimized for many concurrent
        serialized reads of large data files - where "large" might be measured
        in gigabytes or petabytes. However, it does have limitations. For
        example, random access to the data is not really possible in an
        efficient manner. And Hadoop only supports a single writer for output.
        But this limit helps make Hadoop very performant and reliable, in part
        because it allows for the data to be replicated across the cluster,
        reducing the chance of data loss.
      </para>

      <para>The execution layer, called MapReduce, relies on a
        divide-and-conquer strategy to manage massive data sets and computing
        processes. Explaining MapReduce is beyond the scope of this document,
        but its complexity, and the difficulty of creating real-world
        applications against it, are the chief driving force behind the creation
        of Cascading.
      </para>

      <para><?oxy_comment_start author="Jim" timestamp="20120326T223416-0700" comment="Chris - this paragraph is new, intended to just graze the subject of local/distributed mode.  You can edit or remove it as you like."?>
        Hadoop<?oxy_comment_end ?>
        can be configured to run in three modes: standalone mode (i.e., on the
        local computer, useful for coding), pseudo-distributed mode (i.e., on an
        emulated "cluster" of one computer, useful for testing), and
        fully-distributed mode (on a full cluster, for production purposes). The
        execution mode affects both processing and data access. Cascading
        supports the Hadoop execution mode with three modes of its own, which
        are discussed elsewhere in this document: Cascading local mode, Hadoop
        current mode, or Hadoop distributed mode.
      </para>

      <para>Apache Hadoop is an Open Source Apache project and is freely
        available. It can be downloaded from the Hadoop website:<link
          xlink:href="http://hadoop.apache.org/core/">http://hadoop.apache.org/core/</link>.
      </para>
    </section>
  </chapter>

  <chapter>
    <info>
      <title>Diving In</title>
    </info>

    <para>The most common example presented to new Hadoop (and MapReduce)
      developers is an application that counts words. It is the Hadoop
      equivalent to a "Hello World" application.
    </para>

    <para>In the word-counting application, a document is parsed into
      individual words and the frequency of each word is counted. In the last
      paragraph, for example, "is" appears twice and "equivalent" appears
      once.
    </para>

    <para>The following code example uses Cascading to read each line of text
      from our document file, parse it into words, then count the number of
      times each word appears.
    </para>

    <example>
      <title>Word Counting</title>

      <xi:include href="basic-word-count.xml"/>
    </example>

    <para>Several features of this example are worth highlighting.</para>

    <para>First, notice that the pipe assembly is not coupled to the data
      (i.e., the tap instances) until the last moment before execution. File
      paths or references are not embedded in the pipe assembly; instead, the
      pipe assembly is specified independent of data inputs and outputs. The
      only dependency is the data scheme, i.e., the field names. In Cascading,
      every input or output file has field names associated with it, and every
      processing element of the pipe assembly either expects the specified
      fields or creates them. This allows developers to easily self-document
      their code, and allows the Cascading planner to "fail fast" if an expected
      dependency between elements isn't satisfied - for instance, if a needed
      field name is missing or incorrect. (If more information is desired on the
      planner, see
      <xref linkend="job-planner"/>
      .)
    </para>

    <para>Second, note that pipe assemblies are assembled through constructor
      chaining. This may seem odd, but it is done for two reasons. First, it
      keeps the code more concise. Second, it prevents developers from creating
      "cycles" (i.e., recursive filter loops) in the resulting pipe assembly.
      Pipe assemblies are intended to be Directed Acyclic Graphs (DAG's), and in
      keeping with this, the Cascading planner is not designed to handle
      processes that feed themselves. (If desired, there are safer approaches to
      achieving this result.)
    </para>

    <para><?oxy_comment_start author="Jim" timestamp="20120326T141330-0700" comment="This is a possible place to introduce the idea of groups, to establish what they are for the rest of the document. I
      &apos;ve marked one or two other possible places later."?>Third,
      <?oxy_comment_end ?></para>

    <para>Finally, notice that the very first
      <code>Pipe</code>
      instance has a
      name. That instance is the
      <emphasis role="italic">head</emphasis>
      of this
      particular pipe assembly. Pipe assemblies can have any number of heads,
      and any number of<emphasis role="italic">tails</emphasis>. Although the
      tail in this example does not have a name, in a more complex assembly it
      would. In general, heads and tails of pipe
      assemblies <?oxy_comment_start author="Jim" timestamp="20120325T172702-0700" comment="Chris - I don&apos;t know if
      &quot;should&quot; is strong enough. You may want to change this to &quot;require&quot; if that&apos;s more
      accurate."?>should
      be assigned<?oxy_comment_end ?> names. One reason is that names are used
      to bind sources and sinks to pipes during planning. (The example above is
      an exception, because there is only one head and one tail - and
      consequently only one source and one sink - so the binding is
      unmistakable.) Another reason is that the naming of pipes contributes to
      self-documention of pipe assemblies, especially where there are splits,
      joins, and merges in the assembly.
    </para>

    <para>To sum up, the example word-counting application will:</para>

    <itemizedlist>
      <listitem>
        <para>Read each line of text from a file and give it the field name
          "line"
        </para>
      </listitem>

      <listitem>
        <para>parse each "line" into words with the
          <code>RegexGenerator</code>
          object, which returns each word in the
          field named "word"
        </para>
      </listitem>

      <listitem>
        <para>sort and group all the tuples on the "word" field, using the
          <code>GroupBy</code>
          object
        </para>
      </listitem>

      <listitem>
        <para>count the number of elements in each group, using the
          <code>Count()</code>
          object, and store this value in the "count"
          field
        </para>
      </listitem>

      <listitem>
        <para>and write out the "word" and "count" fields.</para>
      </listitem>
    </itemizedlist>
  </chapter>

  <chapter>
    <info>
      <title>Data Processing</title>
    </info>

    <section>
      <title>Terminology</title>

      <para>The Cascading processing model is based on a metaphor of pipes
        (data streams) and filters (data operations). Thus the Cascading API
        allows the developer to assemble pipe assemblies that split, merge,
        group, or join streams of data while applying operations to each data
        record or groups of records.
      </para>

      <para><?oxy_comment_start author="Jim" timestamp="20120326T141625-0700" comment="Another possible place to explain what a Group is - could just put it in a sentence after this one, such as
        &quot;In some cases, it&apos;s expedient to organize tuples into tuple groups, based on a common value such as
        &apos;zipcode&apos;, &apos;event&apos;, or &apos;customer_type&apos;.&quot; I&apos;m not sure this is the best
        way to go, just suggesting a possibility."?>In
        Cascading, we call a data record a<emphasis
          role="italic">tuple</emphasis>, a pipeline a<emphasis
          role="italic">pipe assembly</emphasis>, and a series of tuples passing
        through a pipe assembly a<emphasis role="italic">tuple
          stream</emphasis>.<?oxy_comment_end ?>
      </para>

      <para>Pipe assemblies are specified independently of the data source
        they are to process. So before a pipe assembly can be executed, it must
        be bound to<emphasis role="italic">taps</emphasis>, i.e., data sources
        and sinks. The result of binding one or more pipe assemblies to taps is
        a<emphasis role="italic">flow</emphasis>, which is executed on a
        computer or cluster using the Hadoop framework.
      </para>

      <para>Multiple flows can be grouped together and executed as a single
        process. In this context, if one flow depends on the output of another,
        it is not executed until all of its data dependencies are satisfied.
        Such a collection of flows is called a<emphasis
          role="italic">cascade</emphasis>.
      </para>
    </section>

    <section>
      <title>Pipe Assemblies</title>

      <para>Pipe assemblies define what work should be done against tuple
        streams, which are read from tap
        <emphasis
          role="italic">sources
        </emphasis>
        and written to tap<emphasis
          role="italic">sinks</emphasis>. The work performed on the data stream
        may include actions such as filtering, transforming, organizing, and
        calculating. Pipe assemblies may use multiple sources and multiple
        sinks, and may define splits, merges, and joins to manipulate the tuple
        streams.
      </para>

      <section>
        <title>Pipe Assembly Workflow</title>

        <para>Pipe assemblies are created by chaining
          <classname>cascading.pipe.Pipe</classname>
          classes and subclasses
          together. Chaining is accomplished by passing the names of previous
          <classname>Pipe</classname>
          instances to the constructor of the next
          <classname>Pipe</classname>
          instance.
        </para>

        <para>The following example demonstrates this type of chaining. It
          creates two pipes - a "left-hand side" (lhs) and a "right-hand side"
          (rhs) - and performs some processing on them both, using the Each
          pipe. Then it joins the two pipes into one, using the CoGroup pipe,
          and performs several operations on the joined pipe using Every and
          GroupBy. The specific operations performed are not important in the
          example; the point is to show the general flow of the data streams.
          The diagram after the example gives a visual representation of the
          workflow.
        </para>

        <example>
          <title xreflabel="Chaining Pipes" xml:id="chaining-pipes">Chaining
            Pipes
          </title>

          <xi:include href="simple-pipe-assembly.xml"/>
        </example>

        <para>The following diagram is a visual representation of the example
          above.
        </para>

        <mediaobject>
          <imageobject role="fo">
            <imagedata align="center" contentwidth="5.4in"
                       fileref="images/simple-pipe-assembly.svg"/>
          </imageobject>

          <imageobject role="html">
            <imagedata align="center" contentwidth="5.4in"
                       fileref="images/simple-pipe-assembly.png"/>
          </imageobject>
        </mediaobject>
      </section>

      <section>
        <title>Common Stream Patterns</title>

        <para>As data moves through the pipe, streams may be separated or
          combined for various purposes. Here are the three basic
          patterns:
        </para>

        <variablelist>
          <varlistentry>
            <term>Split</term>

            <listitem>
              <para>A split takes a single stream and sends it down multiple
                paths - that is, it feeds a single
                <classname>Pipe</classname>
                instance into two or more subsequent separate
                <classname>Pipe</classname>
                instances. This is done by using the
                <classname>Each</classname>
                class, or by using named instances
                of the
                <classname>Pipe</classname>
                class. (The branch names are
                useful for binding<xref linkend="failure-traps"/>.)
              </para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>Merge</term>

            <listitem>
              <para>A merge combines two or more streams that have identical
                fields into a single stream. This is done by passing two or more
                <classname>Pipe</classname>
                instances to a
                <classname>Merge</classname>
                or
                <classname>GroupBy</classname>
                pipe.
              </para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>Join</term>

            <listitem>
              <para>A join combines data from two or more streams that have
                different fields, based on common field values (analogous to a
                SQL join.) This is done by passing two or more
                <classname>Pipe</classname>
                instances to a
                <classname>Join</classname>
                or
                <classname>CoGroup</classname>
                pipe. The code sequence and diagram above give an
                example.
              </para>
            </listitem>
          </varlistentry>
        </variablelist>
      </section>

      <section>
        <title>Data Processing</title>

        <para>In addition to directing the tuple streams - using splits,
          merges, and joins - pipe assemblies can examine, filter, tabulate,
          organize, and transform the tuple data as the streams move through the
          pipe assemblies. To facilitate this, the values in the tuple are
          typically given field names, just as database columns are given names,
          so that they may be referenced or selected. The following terminology
          is used:
        </para>

        <variablelist>
          <varlistentry>
            <term>Operation</term>

            <listitem>
              <para>Operations
                (<classname>cascading.operation.Operation</classname>) accept an
                input argument Tuple, and output zero or more result tuples.
                There are a few sub-types of operations defined below. Cascading
                has a number of generic Operations that can be used, or
                developers can create their own custom Operations.
              </para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>Tuple</term>

            <listitem>
              <para>In Cascading, data is processed as a stream of Tuples
                (<classname>cascading.tuple.Tuple</classname>), which are
                composed of fields, much like a database record or row. A Tuple
                is effectively an array of (field) values, where each value can
                be any
                <classname>java.lang.Object</classname>
                Java type (or
                <code>byte[]</code>
                array). For information on supporting
                non-primitive types, see<xref linkend="custom-types"/>.
              </para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>Fields</term>

            <listitem>
              <para>Fields (<classname>cascading.tuple.Fields</classname>) are
                used either to declare the field names for fields in a Tuple, or
                reference field values in a Tuple. They can either be strings
                (such as "firstname" or "birthdate"), integers (for the field
                position, starting at<code>0</code>, and using
                <code>-1</code>
                for the last position), or one of eight predefined Fields "sets"
                (such as<code>Fields.ALL</code>, which selects all values in
                the Tuple, like an asterisk in SQL). For more on Fields sets,
                see<xref linkend="field-algebra"/>).
              </para>
            </listitem>
          </varlistentry>
        </variablelist>
      </section>
    </section>

    <section>
      <title>Pipes</title>

      <para>The code for the sample pipe assembly above,<xref
        linkend="chaining-pipes"/>, consists almost entirely of a series of
        <classname>Pipe</classname>
        constructors, which are the subject of this
        section. The base class
        <classname>cascading.pipe.Pipe</classname>
        and
        its subclasses, the subject of this section, are shown in the diagram
        below.
      </para>

      <mediaobject>
        <imageobject role="fo">
          <?oxy_comment_start author="Jim" timestamp="20120331T130533-0700" comment="Need to add Merge &amp; Join to
          this diagram"?>

          <imagedata align="center" contentwidth="5in"
                     fileref="images/pipes.svg"/>

          <?oxy_comment_end ?>
        </imageobject>

        <imageobject role="html">
          <imagedata align="center" contentwidth="5in"
                     fileref="images/pipes.png"/>
        </imageobject>
      </mediaobject>

      <section>
        <title>Types of Pipes</title>

        <variablelist>
          <para>
            <emphasis role="bold">The
              <classname>Pipe</classname>
              class
            </emphasis>
            is used to instantiate and name a pipe. Pipe names
            are used by the planner to bind taps to the pipe as sources or
            sinks. (A third option is to bind a tap to the pipe as a<emphasis
            role="italic">trap</emphasis>, discussed elsewhere as an advanced
            topic.)
          </para>

          <para>
            <emphasis role="bold">The
              <classname>SubAssembly</classname>
              subclass
            </emphasis>
            is a special type of pipe. It is used to nest
            reusable pipe assemblies within a
            <classname>Pipe</classname>
            class
            for inclusion in a larger pipe assembly. For more information on
            this, see the section on<xref linkend="subassemblies"/>.
          </para>

          <para>
            <emphasis role="bold">The other six types of pipes</emphasis>
            are used to perform operations on the tuple streams as they pass
            through the pipe assemblies. This may involve operating on the
            individual tuples, on groups of related tuples, or on entire
            streams. These six pipe types are introduced here, then explored in
            detail further below.
          </para>

          <varlistentry>
            <term>
              <classname>Each</classname>
              and
              <classname>Every</classname>
            </term>

            <listitem>
              <para>These pipes perform operations based on the data contents
                of tuples - analyze, transform, tabulate, or filter. The
                <classname>Each</classname>
                pipe operates on individual tuples
                in the stream, applying functions or filters such as search and
                replace, removing tuples that have values outside a target
                range, etc. The
                <classname>Every</classname>
                pipe operates on
                groups of tuples that share a common value (such as all the
                tuples for a particular date, or zipcode), applying aggregator
                or buffer operations such as counting, totaling, or averaging
                field values within each group.
              </para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>
              <classname>Merge</classname>
              and
              <classname>Join</classname>
            </term>

            <listitem>
              <para>These pipes combine two or more tuple streams into
                one.
              </para>

              <para>A
                <emphasis role="italic">Merge</emphasis>
                accepts two or
                more streams that have identical fields, and outputs a single
                stream of tuples (in arbitrary order) that contains all the
                tuples from all the specified input streams. Thus a Merge is
                just a mingling of all the tuples from the input streams, as if
                shuffling multiple card decks into one. A sample use case might
                be to merge the Apache access logs from an entire server farm
                onto a single file in the Hadoop file system.
              </para>

              <para>A
                <emphasis role="italic">Join</emphasis>
                is a logical
                joining of tuples from two or more streams into a stream of
                hybrid tuples based on matching field values. It is analogous to
                a SQL join. With a join, the tuples in the different source
                streams do typically not contain the same field lists, but must
                have at least one field in common (for the purpose of matching).
                One sample use case would be to combine the values from a
                customer file, a sales order file, and a shipped orders file
                into a single tuple stream for analysis.
              </para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>
              <classname>GroupBy</classname>
              and
              <classname>CoGroup</classname>
            </term>

            <listitem>
              <para>These pipes extend the functionality of Merge and Join,
                adding the ability to logically sort the tuples on field values
                and express them via the Group interface (described in the
                Javadoc). This makes the output suitable for the Every pipe,
                which performs aggregator and buffer operations, such as
                averaging values in a group of tuples that have the same field
                value.
              </para>

              <para>
                <emphasis role="italic">GroupBy</emphasis>
                sorts a stream
                and expresses the results via the Group interface. If passed
                multiple streams as inputs, it performs a Merge before the sort.
                (Recall that Merge requires that input streams share the same
                field structure.)
              </para>

              <para>
                <emphasis role="italic">CoGroup</emphasis>
                performs a Join
                and sorts the results, expressing the resulting stream via the
                Group interface. As with a Join, the resulting output tuples
                will contain fields from all the input streams.
              </para>
            </listitem>
          </varlistentry>
        </variablelist>

        <para>The following table summarizes the different types of
          pipes.
        </para>

        <table colsep="1" rowsep="1">
          <title>Comparison of pipe types</title>

          <tgroup cols="4">
            <colspec colname="c1" colnum="1" colwidth="90px"/>

            <colspec colname="c2" colnum="2" colwidth="1.08*"/>

            <colspec colname="c3" colnum="3" colwidth="1*"/>

            <colspec colname="c4" colnum="4" colwidth="2.36*"/>

            <tbody>
              <row>
                <entry>
                  <emphasis role="bold">
                    <emphasis role="underline">Pipe
                      type
                    </emphasis>
                  </emphasis>
                </entry>

                <entry>
                  <emphasis role="underline">
                    <emphasis
                      role="bold">Purpose
                    </emphasis>
                  </emphasis>
                </entry>

                <entry>
                  <emphasis role="underline">
                    <emphasis
                      role="bold">Input
                    </emphasis>
                  </emphasis>
                </entry>

                <entry>
                  <emphasis role="underline">
                    <emphasis
                      role="bold">Output
                    </emphasis>
                  </emphasis>
                </entry>
              </row>

              <row>
                <entry>
                  <classname>Pipe</classname>
                </entry>

                <entry>instantiate a pipe; name a pipe</entry>

                <entry>name</entry>

                <entry>a (named) pipe</entry>
              </row>

              <row>
                <entry>
                  <classname>SubAssembly</classname>
                </entry>

                <entry>create nested subassemblies</entry>

                <entry/>

                <entry/>
              </row>

              <row>
                <entry>
                  <classname>Each</classname>
                </entry>

                <entry>apply a filter or function; create a split</entry>

                <entry>tuple stream (grouped or not)</entry>

                <entry>the tuple stream, filtered or transformed; field list
                  may be altered
                </entry>
              </row>

              <row>
                <entry>
                  <classname>Every</classname>
                </entry>

                <entry>apply aggregator or buffer</entry>

                <entry>grouped stream</entry>

                <entry>the tuple stream; field list may be altered;
                  <?oxy_comment_start author="Jim" timestamp="20120331T200827-0700" comment="Hopefully this is correct"?>
                  plus
                  fields with operation results<?oxy_comment_end ?></entry>
              </row>

              <row>
                <entry>
                  <classname>Merge</classname>
                </entry>

                <entry>merge 2 or more streams with identical fields</entry>

                <entry>2 or more tuple streams, any order</entry>

                <entry>tuple stream, unsorted; field list may be
                  altered
                </entry>
              </row>

              <row>
                <entry>
                  <classname>Join</classname>
                </entry>

                <entry>join 2 streams on a matching field value</entry>

                <entry>2 or more tuple streams (ordered?)</entry>

                <entry>tuple stream, unsorted; field list may be
                  altered
                </entry>
              </row>

              <row>
                <entry>
                  <classname>GroupBy</classname>
                </entry>

                <entry>sort/group on field values; optionally merge 2 or more
                  streams with identical fields
                </entry>

                <entry>1 or more tuple streams</entry>

                <entry>sorted/grouped tuples; optional secondary sort</entry>
              </row>

              <row>
                <entry>
                  <classname>CoGroup</classname>
                </entry>

                <entry>join 2 streams on a matching field value</entry>

                <entry>2 or more tuple streams</entry>

                <entry>sorted/grouped tuples</entry>
              </row>
            </tbody>
          </tgroup>
        </table>
      </section>

      <section>
        <title xreflabel="Each and Every Pipes" xml:id="each-every">Each and
          Every Pipes
        </title>

        <para>The
          <classname>Each</classname>
          and
          <classname>Every</classname>
          pipes perform operations on tuple data - for instance, perform a
          search-and-replace on tuple contents, filter out some of the tuples
          based on their contents, or count the number of tuples in a stream
          that share a common field value.
        </para>

        <para>Here is the syntax for these pipes:</para>

        <para>
          <programlisting>new Each( previousPipe, argumentSelector, operation, outputSelector )</programlisting>
        </para>

        <para>
          <programlisting>new Every( previousPipe, argumentSelector, operation, outputSelector )</programlisting>
        </para>

        <para>Both types take four arguments:</para>

        <para>
          <itemizedlist>
            <listitem>
              <para>a Pipe instance</para>
            </listitem>

            <listitem>
              <para>an argument selector</para>
            </listitem>

            <listitem>
              <para>an Operation instance</para>
            </listitem>

            <listitem>
              <para>an output selector on the constructor (selectors here are
                Fields instances)
              </para>
            </listitem>
          </itemizedlist>
        </para>

        <para>The key difference between Each and Every is that one operates
          on individual tuples, and the other operates on sorted groups of
          tuples expressed through the Group interface (described in the
          Javadoc). This affects the kind of operations that these two pipes can
          perform, and the kind of output they produce as a result.
        </para>

        <para>The
          <emphasis role="italic">Each</emphasis>
          pipe acts on one
          tuple at a time, applying operations that are subclasses of
          <classname>Functions</classname>
          and
          <classname>Filters</classname>
          (described in the Javadoc). For example, using Each pipes you can
          split a line from a logfile into multiple fields, filter out
          everything but the GET requests, convert timestring fields into date
          and time fields, and keep only the dates. The Each pipe outputs the
          stream of resulting tuples after the filter or function has been
          applied.
        </para>

        <para>Similarly, since the
          <emphasis role="italic">Every</emphasis>
          pipe works on groups of tuples, it applies operations that are
          subclasses of
          <classname>Aggregators</classname>
          and
          <classname>Buffers</classname>. These operate on groups of tuples -
          i.e., the output of a
          <classname>GroupBy</classname>
          or
          <classname>CoGroup</classname>
          pipe, expressed via the Group interface
          - one group at a time. For example, an Every pipe could accept the
          output of the above Each pipes and count the page requests by date.
          <?oxy_comment_start author="Jim" timestamp="20120331T200848-0700" comment="is this correct?"?>The
          Every pipe outputs the results of the function, e.g., one tuple per
          group, containing the date and the count value.<?oxy_comment_end ?>
        </para>

        <mediaobject>
          <imageobject role="fo">
            <imagedata align="center" contentwidth="6in"
                       fileref="images/pipe-operation-relationship.svg"/>
          </imageobject>

          <imageobject role="html">
            <imagedata align="center" contentwidth="6in"
                       fileref="images/pipe-operation-relationship.png"/>
          </imageobject>
        </mediaobject>

        <para>In the syntax shown at the start of this section, the
          <emphasis
            role="italic">argument selector
          </emphasis>
          specifies fields from the
          input Tuple to use as input values. If the argument selector is not
          specified, the whole input Tuple (<code>Fields.ALL</code>) is passed
          to the Operation as a set of argument values.
        </para>

        <para>Most Operations declare result fields (shown as "declared
          fields" in the diagram). The
          <emphasis role="italic">output
            selector
          </emphasis>
          specifies the fields of the output Tuple
          <?oxy_comment_start author="Jim" timestamp="20120331T192641-0700" comment="Not sure what this means - are we getting just one tuple per grouping, containing the operation results? Or are we getting all the original tuples with a few extra fields that contain operation results?"?>
          from
          an "appended" version of the input Tuple and the Operation result
          Tuple<?oxy_comment_end ?>. This new output Tuple becomes the input
          Tuple to the next pipe in the pipe assembly.
        </para>

        <para>Note that if a
          <classname>Function</classname>
          or
          <classname>Aggregator</classname>
          outputs more than one Tuple,
          <?oxy_comment_start author="Jim" timestamp="20120331T193611-0700" comment="not sure what this means."?>this
          process is repeated for each result Tuple against the original input
          Tuple<?oxy_comment_end ?>. Depending on the output selector, input
          Tuple values could be duplicated across each output Tuple.
        </para>

        <mediaobject>
          <imageobject role="fo">
            <imagedata align="center" contentwidth="6in"
                       fileref="images/each-operation-relationship.svg"/>
          </imageobject>

          <imageobject role="html">
            <imagedata align="center" contentwidth="6in"
                       fileref="images/each-operation-relationship.png"/>
          </imageobject>
        </mediaobject>

        <para>If the result selector is not specified for an
          <classname>Each</classname>
          pipe performing a
          <classname>Functions</classname>
          operation, the Operation results are
          returned by default (<code>Fields.RESULTS</code>), replacing the input
          Tuple values in the tuple stream. (This is not true of
          <classname>Filters</classname>
          , which either discard the input Tuple
          or return it intact, and thus do not use an output selector.)
        </para>

        <mediaobject>
          <imageobject role="fo">
            <imagedata align="center" contentwidth="6in"
                       fileref="images/every-operation-relationship.svg"/>
          </imageobject>

          <imageobject role="html">
            <imagedata align="center" contentwidth="6in"
                       fileref="images/every-operation-relationship.png"/>
          </imageobject>
        </mediaobject>

        <para>For the
          <classname>Every</classname>
          pipe, the Aggregator
          results are appended to the input Tuple (<code>Fields.ALL</code>) by
          default.
        </para>

        <para>Note that the
          <classname>Every</classname>
          pipe associates
          Aggregator results with the current group Tuple. For example, if you
          are grouping on the field "department" and counting the number of
          "names" grouped by that department, the resulting output Fields will
          be ["department","num_employees"]. This is true for both
          <classname>Aggregator</classname>, seen above, and
          <classname>Buffer</classname>.
        </para>

        <para>If you are also adding up the salaries associated with each
          "name" in each "department", the output Fields will be
          ["department","num_employees","total_salaries"]. This is only true for
          chains of
          <classname>Aggregator</classname>
          Operations - you are not
          allowed to chain
          <classname>Buffer</classname>
          Operations.
        </para>

        <mediaobject>
          <imageobject role="fo">
            <imagedata align="center" contentwidth="6in"
                       fileref="images/buffer-operation-relationship.svg"/>
          </imageobject>

          <imageobject role="html">
            <imagedata align="center" contentwidth="6in"
                       fileref="images/buffer-operation-relationship.png"/>
          </imageobject>
        </mediaobject>

        <para>When the
          <classname>Every</classname>
          pipe is used with a
          <classname>Buffer</classname>, the behavior is slightly different.
          Instead of being associated with the current grouping Tuple, the
          Buffer results are associated with the current values Tuple. This is
          analogous to how an
          <classname>Each</classname>
          pipe works with a
          <classname>Function</classname>. It may seem slightly more confusing,
          but provides much more flexibility.
        </para>
      </section>

      <section>
        <title>Merge and Join Pipes</title>

        <para>The
          <classname>Merge</classname>
          and
          <classname>Join</classname>
          pipes combine two or more tuple streams into one.
        </para>

        <section>
          <title>The Merge Pipe</title>

          <para>A
            <emphasis role="italic">Merge</emphasis>
            accepts two or more
            streams that have
            identical <?oxy_comment_start author="Jim" timestamp="20120327T013033-0700" comment="Would it be better to say
            &quot;schemes&quot; instead of &quot;fields&quot;, or can streams that have different schemes but the same
            fields still be merged?"?>fields<?oxy_comment_end ?>,
            and outputs a single stream of tuples (in arbitrary order) that
            contains all the tuples from all the specified input streams. Thus a
            Merge is just a mingling of all the tuples from the input streams,
            as if shuffling multiple card decks into one. A sample use case
            might be to merge the Apache access logs from an entire server farm
            onto a single file in the Hadoop file system.
          </para>

          <example>
            <title>Merging Two Tuple Streams</title>

            <xi:include href="simple-merge.xml"/>
          </example>

          <para>The example above simply combines all the tuples from two
            existing streams into a new tuple stream. The resulting stream is in
            arbitrary sequence.
          </para>
        </section>

        <section>
          <title>The Join Pipe</title>

          <para>A
            <emphasis role="italic">Join</emphasis>
            is a logical joining
            of tuples from two or more streams into a stream of hybrid tuples
            based on matching field values. It is analogous to a SQL join, and
            can be an inner, outer, left, or right join. With a join, the tuples
            in the different source streams do typically not contain the same
            field lists, but must have at least one field in common (for the
            purpose of matching). One sample use case would be to combine the
            values from a customer file, a sales order file, and a shipped
            orders file into a single tuple stream for analysis; this would be
            done by joining the customer and sales streams on the unique
            customer ID, and joining the result with the shipped orders on the
            order number.
          </para>

          <example>
            <title>Joining Two Tuple Streams</title>

            <xi:include href="simple-join.xml"/>
          </example>

          <para>The example above performs an inner join on two streams ("lhs"
            and "rhs"), based on common values in two fields. The fieldnames
            that are specified in
            <classname>lhsFields</classname>
            and
            <classname>rhsFields</classname>
            are among the field names
            previously declared for the two input streams.
          </para>

          <section>
            <title>Field Names</title>

            <para>Note that all the field names used in any tuple must be
              unique; duplicates are not allowed. This includes tuples created
              by the Join operation, which leads to a possible problem, shown in
              the following diagram.
            </para>

            <mediaobject>
              <imageobject role="fo">
                <imagedata align="center" contentwidth="5in"
                           fileref="images/cogrouping-fields-fail.svg"/>
              </imageobject>

              <imageobject role="html">
                <imagedata align="center" contentwidth="5in"
                           fileref="images/cogrouping-fields-fail.png"/>
              </imageobject>
            </mediaobject>

            <para>In this figure, two streams are to be joined on the "url"
              field, resulting in a new Tuple that contains fields from the two
              input tuples. However, the resulting tuple would include two
              fields with the same name ("url"), which is unworkable. To handle
              the conflict, developers can use the
              <classname>Join</classname>
              pipe's
              <parameter>declaredFields</parameter>
              argument (described
              in the Javadoc) to declare unique field names for the output
              tuple.
            </para>

            <example>
              <title>Joining Two Tuple Streams with Duplicate Field
                Names
              </title>

              <xi:include href="duplicate-join.xml"/>
            </example>

            <mediaobject>
              <imageobject role="fo">
                <imagedata align="center" contentwidth="5in"
                           fileref="images/cogrouping-fields-pass.svg"/>
              </imageobject>

              <imageobject role="html">
                <imagedata align="center" contentwidth="5in"
                           fileref="images/cogrouping-fields-pass.png"/>
              </imageobject>
            </mediaobject>

            <para>This revised figure demonstrates the use of declared field
              names to prevent a planning failure.
            </para>

            <para>It might seem preferable for Cascading to automatically
              recognize the duplication and simply merge the identically-named
              fields, saving effort for the developer. However, the internal
              implementation relies on field position, not field names, when
              reading tuples; the field names are a device for the developer.
              This approach is beneficial in terms of execution speed,
              readability, and abstraction of subassemblies from specific field
              names.
            </para>
          </section>

          <section>
            <title>The Joiner Class</title>

            <para>In the example above, we explicitly specified a Joiner class
              to perform a join on our data. The reason the
              <classname>CoGroup</classname>
              pipe is named "CoGroup", and not
              "Join", is because the joining of data is done after all the
              parallel streams are co-grouped by their common keys. A detailed
              explanation is beyond the scope of this document. But in brief,
              before a join can be performed, Cascading must create a "bag" of
              data (borrowing from Pig terminology) for every input tuple
              stream, consisting of all the Tuple instances associated with a
              given grouping.
            </para>

            <mediaobject>
              <imageobject role="fo">
                <imagedata align="center" contentwidth="5in"
                           fileref="images/cogrouped-values.svg"/>
              </imageobject>

              <imageobject role="html">
                <imagedata align="center" contentwidth="5in"
                           fileref="images/cogrouped-values.png"/>
              </imageobject>
            </mediaobject>

            <para>The most commonly-used type of join is the inner join, which
              tries to match
              <emphasis role="italic">each</emphasis>
              Tuple on
              the "lhs" with
              <emphasis role="italic">every</emphasis>
              Tuple on
              the "rhs", based on field values. This is the default behavior for
              a join in SQL. With an inner join, if one of the bags is empty, no
              tuples are joined. An outer join, conversely, allows for either
              bag to be empty and simply substitutes a Tuple containing
              <code>null</code>
              values for the non-existent tuple.
            </para>

            <mediaobject>
              <imageobject role="fo">
                <imagedata align="center" contentwidth="3.5in"
                           fileref="images/joins.svg"/>
              </imageobject>

              <imageobject role="html">
                <imagedata align="center" contentwidth="3.5in"
                           fileref="images/joins.png"/>
              </imageobject>
            </mediaobject>

            <para>The diagram above shows all the supported Joiner
              types.
            </para>

            <para><programlisting>LHS = [0,a] [1,b] [2,c]
              RHS = [0,A] [2,C] [3,D]</programlisting>Using the above simple data sets, we
              define each join type below, where the values are joined on the
              first position, a numeric value. Note that, when Cascading joins
              tuples, the resulting Tuple contains all the incoming values. The
              duplicate common key(s) is not discarded if given. And on outer
              joins, where there is no equivalent key in the alternate stream,
              <code>null</code>
              values are used as placeholders.
              <variablelist>
                <varlistentry>
                  <term>InnerJoin</term>

                  <listitem>
                    <para>An inner join only returns a joined Tuple if neither
                      bag is empty.
                      <programlisting>[0,a,0,A] [2,c,2,C]</programlisting>
                    </para>
                  </listitem>
                </varlistentry>

                <varlistentry>
                  <term>OuterJoin</term>

                  <listitem>
                    <para>An outer join performs a join if one bag (left or
                      right) is empty, or if neither bag is
                      empty.
                      <programlisting>[0,a,0,A] [1,b,null,null] [2,c,2,C] [null,null,3,D]</programlisting>
                    </para>
                  </listitem>
                </varlistentry>

                <varlistentry>
                  <term>LeftJoin</term>

                  <listitem>
                    <para>A left join can also be stated as a left inner and
                      right outer join, where it is acceptable for the right bag
                      to be empty (but not the left).
                    </para>

                    <programlisting>[0,a,0,A] [1,b,null,null] [2,c,2,C]</programlisting>
                  </listitem>
                </varlistentry>

                <varlistentry>
                  <term>RightJoin</term>

                  <listitem>
                    <para>A right join can also be stated as a left outer and
                      right inner join, where it is acceptable for the left bag
                      to be empty (but not the right).
                      <programlisting>[0,a,0,A] [2,c,2,C] [null,null,3,D]</programlisting>
                    </para>
                  </listitem>
                </varlistentry>

                <varlistentry>
                  <term>MixedJoin</term>

                  <listitem>
                    <para>A mixed join is where 3 or more tuple streams are
                      joined, using a small Boolean array to specify each of the
                      join types to use. For more information, see the
                      <classname>cascading.pipe.cogroup.MixedJoin</classname>
                      class in the Javadoc.
                    </para>
                  </listitem>
                </varlistentry>

                <varlistentry>
                  <term>
                    <emphasis>Custom</emphasis>
                  </term>

                  <listitem>
                    <para>Developers can subclass the
                      <classname>cascading.pipe.cogroup.Joiner</classname>
                      class
                      to create custom join operations.
                    </para>
                  </listitem>
                </varlistentry>
              </variablelist>
            </para>
          </section>

          <section>
            <title><?oxy_comment_start author="Jim" timestamp="20120402T014844-0700" comment="Chris - not sure I&apos;ve
              got this quite right..."?>Memory
              considerations<?oxy_comment_end ?></title>

            <para>During a join of two streams, the current tuple group of the
              right-hand stream is stored in memory for rapid comparison. If the
              group is very large, it may exceed a configurable spill threshold
              and be spilled to disk, reducing performance. For this reason,
              alway use the shorter stream or the stream with shorter groups on
              the right-hand side. If necessary, adjust the spill threshold as
              described in the Javadoc.
            </para>
          </section>
        </section>
      </section>

      <section>
        <title>GroupBy and CoGroup Pipes</title>

        <section>
          <title>Basic Operation</title>

          <para>The
            <classname>GroupBy</classname>
            and
            <classname>CoGroup</classname>
            pipes extend the functionality of
            <classname>Merge</classname>
            and<classname>Join</classname>, adding
            the ability to logically group the tuples on field values and
            express them via the Group interface (described in the Javadoc).
            This makes the output suitable for the
            <classname>Every</classname>
            pipe, which performs
            <classname>Aggregator</classname>
            and
            <classname>Buffer</classname>
            operations, such as averaging values
            in a group of tuples that have the same field value.
          </para>

          <para>
            <emphasis role="italic">GroupBy</emphasis>
            sorts an input
            stream and expresses the results via the Group interface. If passed
            multiple input streams, it performs a Merge before the sort. (Recall
            that Merge requires that input streams share the same field
            structure.)
          </para>

          <para>
            <emphasis role="italic">CoGroup</emphasis>
            performs a Join and
            sorts the results, expressing the resulting stream via the Group
            interface. As with a Join, the resulting output tuples will contain
            fields from all the input streams
          </para>

          <para>At the end of a
            <classname>GroupBy</classname>
            or
            <classname>CoGroup</classname>
            operation, the resulting groups of
            tuples are sorted by the designated grouping values, and then are
            passed to an
            <classname>Aggregator</classname>
            or
            <classname>Buffer</classname>
            operation in that sequence. However,
            by default there is no secondary sort, so within each group the
            tuples are in arbitrary order. For instance, when grouping on
            "lastname", the tuples
            <code>[doe, john]</code>
            and
            <code>[doe,
              jane]
            </code>
            end up in the same group, but in arbitrary sequence
            (e.g., not necessarily sorted by the "firstname" values).
          </para>
        </section>

        <section>
          <title>Secondary Sorting</title>

          <para>If multi-level sorting is desired, the names of the sort
            fields on must be specified to the GroupBy instance, as seen below.
            In this example,
            <code>value1</code>
            and
            <code>value2</code>
            will
            arrive in their natural sort order (assuming they are
            <classname>java.lang.Comparable</classname>).
          </para>

          <example>
            <title>Secondary Sorting</title>

            <xi:include href="simple-groupby-secondary.xml"/>
          </example>

          <para>If we don't care about the order of<code>value2</code>, we
            can leave it out of the
            <code>sortFields</code>
            <classname>Fields</classname>
            constructor.
          </para>

          <para>In the next example, we reverse the order of
            <code>value1</code>
            while keeping the natural order of
            <code>value2</code>.
          </para>

          <example>
            <title>Reversing Secondary Sort Order</title>

            <xi:include href="simple-groupby-secondary-comparator.xml"/>
          </example>

          <para>Whenever there is an implied sort during grouping or secondary
            sorting, a custom
            <classname>java.util.Comparator</classname>
            can
            optionally be supplied to the grouping
            <classname>Fields</classname>
            or secondary sort<classname>Fields</classname>, in order to
            influence the sorting through the
            <code>Fields.setComparator()</code>
            call.
          </para>

          <para>To sort or group on non-
            <classname>Comparable</classname>
            classes, consider creating a custom
            <classname>Comparator</classname>.
          </para>

          <para>Below is a more practical example, where we group by the "day
            of the year", but want to reverse the order of the tuples within
            that grouping by "time of day".
          </para>

          <example>
            <title>Reverse Order by Time</title>

            <xi:include href="simple-groupby-secondary-time.xml"/>
          </example>
        </section>
      </section>

      <section>
        <title><?oxy_comment_start author="Jim" timestamp="20120327T003740-0700" comment="Chris, this new section is for discussing advanced local override properties for a pipe. I threw in some bad boilerplate to get you started..."?>
          Setting
          Custom Pipe Properties<?oxy_comment_end ?></title>

        <para>The Cascading planner assigns the same properties to all pipes
          in a flow, based on the defaults for that flow. However, it's possible
          to override the default properties by using
          <classname>xxx</classname>.
        </para>

        <para>The following code sample demonstrates the basic form.</para>

        <para/>

        <para>Below are some examples of properties that you might wish to
          override. For complete lists of flow and pipe properties, consult the
          Javadoc.
        </para>

        <para>
          <variablelist>
            <varlistentry>
              <term/>

              <listitem>
                <para/>
              </listitem>
            </varlistentry>
          </variablelist>
          <variablelist>
            <varlistentry>
              <term/>

              <listitem>
                <para/>
              </listitem>
            </varlistentry>
          </variablelist>
          <variablelist>
            <varlistentry>
              <term/>

              <listitem>
                <para/>
              </listitem>
            </varlistentry>
          </variablelist>
        </para>

        <para>A final cautionary or encouraging note about using this
          feature... For more information, see...
        </para>
      </section>
    </section>

    <section>
      <title>Source and Sink taps</title>

      <para>All input data comes in from, and all output data goes out to,
        some instance of<classname>cascading.tap.tap</classname>. A tap
        represents a data resource - such as a file on the local file system, on
        a Hadoop distributed file system, or on Amazon S3. A tap can be read
        from, which makes it a<emphasis role="italic">source</emphasis>, or
        written to, which makes it a<emphasis role="italic">sink</emphasis>.
        Or, more commonly, taps act as both sinks and sources when shared
        between Flows.
      </para>

      <para>Depending on your operating mode (Cascading local mode, Hadoop
        current mode, or Hadoop distributed mode), the specific classes you use
        may vary. Details are provided in the sections below.
      </para>

      <section>
        <title>Schemes</title>

        <para>If the Tap is about where the data is and how to access it, the
          Scheme is about what the data is and how to read it. Every Tap must
          have a Scheme that describes the data. Cascading supports four Scheme
          classes:
        </para>

        <variablelist>
          <varlistentry>
            <term>TextLine</term>

            <listitem>
              <para>TextLine reads and writes raw text files and returns
                tuples with two field names by default, "num" (i.e., line
                number) and "line". These values are inherited from Hadoop. When
                written to, all Tuple values are converted to Strings delimited
                with the TAB character (\t).
              </para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>TextDelimited</term>

            <listitem>
              <para>TextDelimited reads and writes character-delimited files
                in standard formats such as CSV (comma-separated variables), TSV
                (tab-separated variables), and so on. When written to, all Tuple
                values are converted to Strings and joined with the specified
                character delimiter. This Scheme can optionally handle quoted
                values with custom quote characters. Further, TextDelimited can
                coerce each value to a primitive type.
              </para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>SequenceFile</term>

            <listitem>
              <para>SequenceFile is based on the Hadoop Sequence file, which
                is a binary format. When written to or read from, all Tuple
                values are saved in their native binary form. This is the most
                efficient file format - but be aware that the resulting files
                are binary and can only be read by Hadoop applications.
              </para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>WritableSequenceFile</term>

            <listitem>
              <para>WritableSequenceFile is based on the Hadoop Sequence file,
                like the SequenceFile Scheme, but was designed to read and write
                key and/or value Hadoop
                <classname>Writable</classname>
                objects
                directly. This is very useful if you have sequence files created
                by other applications. During writing (sinking), specified key
                and/or value fields are serialized directly into the sequence
                file. During reading (sourcing), the key and/or value objects
                are deserialized and wrapped in a Cascading Tuple object and
                passed to the downstream pipe assembly.
              </para>
            </listitem>
          </varlistentry>
        </variablelist>

        <para>There's a key difference between the
          <classname>TextLine</classname>
          and
          <classname>SequenceFile</classname>
          schemes. With the
          <classname>SequenceFile</classname>
          option, data is stored as tuples,
          which can be read without having to be parsed. But with the
          <classname>TextLine</classname>
          option, Cascading must parse each line
          into a
          <classname>Tuple</classname>
          before processing it, causing a
          performance hit. The resulting tuples are saved in the
          <classname>SequenceFile</classname>
          scheme, so future applications can
          just read the file directly into
          <classname>Tuple</classname>
          instances without parsing.
        </para>

        <section>
          <title><?oxy_comment_start author="Jim" timestamp="20120328T005308-0700" comment="Chris - this is an experiment to see if you can fit the necessary details into a table. If not, we can use a variable list."?>
            Mode-specific
            implementation details<?oxy_comment_end ?></title>

          <para>Depending on your operating mode (Cascading local, Hadoop
            local, or Distributed), the classes you use to specify schemes will
            vary. Mode-specifric details for each standard scheme are shown
            below.
          </para>

          <table frame="all">
            <title>Mode-specific details for setting scheme</title>

            <tgroup cols="4">
              <colspec colname="c1" colnum="1" colwidth="1.0*"/>

              <colspec colname="c2" colnum="2" colwidth="1.0*"/>

              <colspec colname="c3" colnum="3" colwidth="1.0*"/>

              <colspec colname="c4" colnum="4" colwidth="1.0*"/>

              <tbody>
                <row>
                  <entry/>

                  <entry>
                    <emphasis role="bold">Cascading local
                      mode
                    </emphasis>
                  </entry>

                  <entry>
                    <emphasis role="bold">Hadoop current
                      mode
                    </emphasis>
                  </entry>

                  <entry>
                    <emphasis role="bold">Distributed
                      mode
                    </emphasis>
                  </entry>
                </row>

                <row>
                  <entry>
                    <emphasis role="bold">TextLine</emphasis>
                  </entry>

                  <entry/>

                  <entry/>

                  <entry/>
                </row>

                <row>
                  <entry>
                    <emphasis
                      role="bold">TextDelimited
                    </emphasis>
                  </entry>

                  <entry/>

                  <entry/>

                  <entry/>
                </row>

                <row>
                  <entry>
                    <emphasis role="bold">SequenceFile</emphasis>
                  </entry>

                  <entry/>

                  <entry/>

                  <entry/>
                </row>

                <row>
                  <entry>
                    <emphasis
                      role="bold">WritableSequenceFile
                    </emphasis>
                  </entry>

                  <entry/>

                  <entry/>

                  <entry/>
                </row>
              </tbody>
            </tgroup>
          </table>
        </section>

        <section>
          <title>Sequence File Compression</title>

          <para>For best performance when operating in Hadoop local or
            Distributed mode, enable Sequence File Compression in the Hadoop
            property settings - either block or record-based compression. Refer
            to the Hadoop documentation for the available properties and
            compression types.
          </para>
        </section>
      </section>

      <section>
        <title>Taps</title>

        <para>This sample code creates a new Hadoop FileSystem Tap that can
          read and write raw text files. Since only one field name is provided,
          the "offset" field is discarded, resulting in an input tuple stream
          with only "line" values.
        </para>

        <example>
          <title>Creating a new tap</title>

          <xi:include href="simple-tap.xml"/>
        </example>

        <para>Here are the three most commonly-used tap types:</para>

        <variablelist>
          <varlistentry>
            <term>Lfs</term>

            <listitem>
              <para>The
                <classname>cascading.tap.Lfs</classname>
                tap is used
                to reference local files, i.e., files located on the machine on
                which your Cascading application is started. Note that even when
                Hadoop is configured to use a remote cluster, if an Lfs tap is
                used as either a source or sink in a Flow, Cascading is
                automatically forced to run in local mode and not on the
                cluster. <?oxy_comment_start author="Jim" timestamp="20120402T024640-0700" comment="It&apos;d be nice if
                we could be more clear about this. If we are forced to run in local mode, how are we able to write
                anything to the Hadoop distributed file system? This is one of the use cases described in the Overview
                section, so it&apos;s probably worth explaining."?>This
                is useful when creating applications to read local files and
                import them into the Hadoop distributed file
                system.<?oxy_comment_end ?>
              </para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>Dfs</term>

            <listitem>
              <para>The
                <classname>cascading.tap.Dfs</classname>
                tap is used
                to reference files on the Hadoop distributed file system.
              </para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>Hfs</term>

            <listitem>
              <para>The
                <classname>cascading.tap.Hfs</classname>
                tap uses the
                current Hadoop default file system. If Hadoop is configured for
                "local mode" its default file system is the local file system.
                If configured for distributed mode, the default file system is
                typically the Hadoop distributed file system. The Hfs is
                convenient when writing Cascading applications that may or may
                not be run on a cluster. Lhs and Dfs subclass the Hfs
                tap.
              </para>
            </listitem>
          </varlistentry>
        </variablelist>

        <para>Also provided are four utility taps:</para>

        <variablelist>
          <varlistentry>
            <term>MultiSourcetap</term>

            <listitem>
              <para>The
                <classname>cascading.tap.MultiSourcetap</classname>
                is
                used to tie multiple tap instances into a single tap for use as
                an input source. The only restriction is that all the tap
                instances passed to a new MultiSourcetap share the same Scheme
                classes (not necessarily the same Scheme instance).
              </para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>MultiSinktap</term>

            <listitem>
              <para>The
                <classname>cascading.tap.MultiSinktap</classname>
                is
                used to tie multiple tap instances into a single tap for use as
                output sinks. At runtime, for every Tuple output by the pipe
                assembly, each child tap to the MultiSinktap will sink the
                Tuple.
              </para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>Templatetap</term>

            <listitem>
              <para>The
                <classname>cascading.tap.Templatetap</classname>
                is
                used to sink tuples into directory paths based on the values in
                the Tuple. More can be read below in<xref
                  linkend="template-tap"/>.
              </para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>GlobHfs</term>

            <listitem>
              <para>The
                <classname>cascading.tap.GlobHfs</classname>
                tap
                accepts Hadoop style "file globbing" expression patterns. This
                allows for multiple paths to be used as a single source, where
                all paths match the given pattern.
              </para>
            </listitem>
          </varlistentry>
        </variablelist>

        <section>
          <title><?oxy_comment_start author="Jim" timestamp="20120328T005308-0700" comment="Chris - this is an experiment to see if you can fit the necessary details into a table."?>
            Mode-specific
            implementation details<?oxy_comment_end ?></title>

          <para>Depending on your operating mode (Cascading local, Hadoop
            mode, or Distributed), the classes you use to specify file systems
            will vary. Mode-specifric details for each tap type are shown
            below.
          </para>

          <table frame="all">
            <title>Mode-specific details for setting file system</title>

            <tgroup cols="2">
              <colspec colname="c1" colnum="1" colwidth="1*"/>

              <colspec colname="c2" colnum="2" colwidth="4.41*"/>

              <tbody>
                <row>
                  <entry>
                    <emphasis role="bold">File system
                      mode
                    </emphasis>
                  </entry>

                  <entry>
                    <emphasis role="bold">Class
                      Details
                    </emphasis>
                  </entry>
                </row>

                <row>
                  <entry>Cascading local file system</entry>

                  <entry/>
                </row>

                <row>
                  <entry>Hadoop distributed</entry>

                  <entry/>
                </row>

                <row>
                  <entry>Hadoop current</entry>

                  <entry/>
                </row>

                <row>
                  <entry>Multiple Source</entry>

                  <entry/>
                </row>

                <row>
                  <entry>Multiple Sink</entry>

                  <entry/>
                </row>

                <row>
                  <entry>Tuple-directed file path</entry>

                  <entry/>
                </row>

                <row>
                  <entry>Hadoop file globbing</entry>

                  <entry/>
                </row>
              </tbody>
            </tgroup>
          </table>
        </section>
      </section>
    </section>

    <section>
      <title>Sink modes</title>

      <para>
        <example>
          <title>Overwriting An Existing Resource</title>

          <xi:include href="simple-replace-tap.xml"/>
        </example>
      </para>

      <para>It's important to understand how Hadoop deals with directories.
        Hadoop cannot source data from directories with nested sub-directories,
        and it cannot write to directories that already exist. However, the good
        news is that you can simply point the
        <classname>Hfs</classname>
        <classname>tap</classname>
        to a directory of data files, and they are
        all used as input - there's no need to enumerate each individual file
        into a<classname>MultiSourcetap</classname>.
      </para>

      <para>To deal with the possibility of existing directories, the
        Hadoop-related taps allow for a
        <classname>SinkMode</classname>
        value to
        be set when constructed. Here are the modes supported:
      </para>

      <variablelist>
        <varlistentry>
          <term>
            <classname>SinkMode.KEEP</classname>
          </term>

          <listitem>
            <para>This is the default behavior. If the resource exists,
              attempting to write to it will fail.
            </para>
          </listitem>
        </varlistentry>

        <varlistentry>
          <term>
            <classname>SinkMode.REPLACE</classname>
          </term>

          <listitem>
            <para>This allows Cascading to delete the file immediately after
              the Flow is started.
            </para>
          </listitem>
        </varlistentry>

        <varlistentry>
          <term>
            <classname>SinkMode.UPDATE</classname>
          </term>

          <listitem>
            <para>Allows for new tap types that can update or append - for
              example, to update or add records in a database. Each tap may
              implement this functionality in its own way. Cascading recognizes
              this update mode, and if a resource exists, will not fail or
              attempt to delete it.
            </para>
          </listitem>
        </varlistentry>
      </variablelist>
    </section>

    <section>
      <title xreflabel="Field Algebra" xml:id="field-algebra">Fields
        Sets
      </title>

      <para>Cascading applications can perform complex manipulation or "field
        algebra" on the fields stored in tuples, using<emphasis
          role="italic">Fields sets</emphasis>, a feature of the
        <classname>Fields</classname>
        class that provides a sort of wildcard
        tool for referencing sets of field values.
      </para>

      <para>The eight predefined Fields sets are constant values on the
        <classname>Fields</classname>
        class. They can be used in many places
        where the
        <classname>Fields</classname>
        class is expected. They are:
        <variablelist>
          <varlistentry>
            <term>Fields.ALL</term>

            <listitem>
              <para>The
                <classname>cascading.tuple.Fields.ALL</classname>
                constant is a wildcard that represents all the current available
                fields.
              </para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>Fields.RESULTS</term>

            <listitem>
              <para>The
                <classname>cascading.tuple.Fields.RESULTS</classname>
                constant is used to represent the field names of the current
                operations return values. This Fields set may only be used as an
                output selector on a pipe, causing the pipe to output a tuple
                containing the operation results.
              </para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>Fields.REPLACE</term>

            <listitem>
              <para>The
                <classname>cascading.tuple.Fields.REPLACE</classname>
                constant is used as an output selector to inline-replace values
                in the incoming tuple with the results of an operation. This
                convenient Fields set allows operations to overwrite the value
                stored in the specified field. The current operation must either
                specify the identical field names used by the pipe, or use the
                <classname>ARGS</classname>
                Fields set.
              </para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>Fields.SWAP</term>

            <listitem>
              <para>The
                <classname>cascading.tuple.Fields.SWAP</classname>
                constant is used as an output selector to swap the operation
                arguments with its results. Neither the argument and result
                field names, nor the size, need to be the same. This is useful
                for when the operation arguments are no longer necessary and the
                result Fields and values should be appended to the remainder of
                the input field names and Tuple.
              </para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>Fields.ARGS</term>

            <listitem>
              <para>The
                <classname>cascading.tuple.Fields.ARGS</classname>
                constant is used to let a given operation inherit the field
                names of its argument Tuple. This Fields set is a convenience
                and is typically used when the Pipe output selector is
                <classname>RESULTS</classname>
                or
                <classname>REPLACE</classname>. It is specifically used by the
                Identity Function when coercing values from Strings to primitive
                types.
              </para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>Fields.GROUP</term>

            <listitem>
              <para>The
                <classname>cascading.tuple.Fields.GROUP</classname>
                constant represents all the fields used as grouping key in the
                most recent grouping. If no previous grouping exists in the pipe
                assembly,
                <classname>GROUP</classname>
                represents all the
                current field names.
              </para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>Fields.VALUES</term>

            <listitem>
              <para><?oxy_comment_start author="Jim" timestamp="20120402T030534-0700" comment="This doesn&apos;t make
                sense to me or seem to match the Javadoc, which says &quot;all fields used as values for the last
                grouping&quot;. Not actually sure what that means, either, but it doesn&apos;t seem like the same
                thing."?>The
                <classname>cascading.tuple.Fields.VALUES</classname>
                constant
                represent all the fields not used as grouping fields in a
                previous Group.<?oxy_comment_end ?>
              </para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>Fields.UNKNOWN</term>

            <listitem>
              <para>The
                <classname>cascading.tuple.Fields.UNKNOWN</classname>
                constant is used when Fields must be declared, but it's not
                known how many fields or what their names are. This allows for
                processing tuples of arbitrary length from an input source or
                some operation. Use this Fields set with caution.
              </para>
            </listitem>
          </varlistentry>
        </variablelist>
      </para>

      <para>Below is a reference chart showing common ways to merge input and
        result fields for the desired output fields. See the section on
        <xref
          linkend="each-every"/>
        for details on the different columns and their
        relationships to the
        <classname>Each</classname>
        and
        <classname>Every</classname>
        Pipes and Functions, Aggregators, and
        Buffers.
      </para>

      <mediaobject>
        <imageobject role="fo">
          <imagedata align="center" contentwidth="7in"
                     fileref="images/field-algebra.svg"/>
        </imageobject>

        <imageobject role="html">
          <imagedata align="center" contentwidth="7in"
                     fileref="images/field-algebra.png"/>
        </imageobject>
      </mediaobject>
    </section>

    <section>
      <title>Flows</title>

      <para>When pipe assemblies are bound to source and sink taps, a
        <classname>Flow</classname>
        is created. Flows are executable in the
        sense that, once they are created, they can be started and will execute
        on a configured Hadoop cluster.
      </para>

      <para>A Flow is essentially a data processing workflow that reads data
        from sources, processes the data as defined by the pipe assembly, and
        writes data to the sinks. Input source data does not need to exist at
        the time the Flow is created, but it must exist by the time the Flow is
        executed (unless it is executed as part of a Cascade - see<xref
          linkend="cascades"/>).
      </para>

      <para>The most common pattern is to create a Flow from an existing pipe
        assembly. But there are cases where a MapReduce job has already been
        created and it makes sense to encapsulate it in a Flow class, so that it
        may participate in a Cascade and be scheduled with other Flow instances.
        Alternatively, via the
        <link
          xlink:href="http://github.com/cwensel/riffle">Riffle
        </link>
        annotations,
        third-party applications can participate in a Cascade, and complex
        algorithms that result in iterative Flow executions can be encapsulated
        as a single Flow. All patterns are covered here.
      </para>

      <section>
        <title>Creating Flows from Pipe Assemblies</title>

        <example>
          <title>Creating a new Flow</title>

          <xi:include href="simple-flow.xml"/>
        </example>

        <para>To create a Flow, it must be planned though the FlowConnector
          object. The
          <code>connect()</code>
          method is used to create new Flow
          instances based on a set of sink taps, source taps, and a pipe
          assembly. The example above is quite trivial.
        </para>

        <example>
          <title>Binding taps in a Flow</title>

          <xi:include href="complex-flow.xml"/>
        </example>

        <para>The example above expands on our previous pipe assembly example
          by creating source and sink taps and planning a Flow. Note there are
          two branches in the pipe assembly - one named "lhs" and the other
          named "rhs". Cascading uses those names to bind the source taps to the
          pipe assembly. A HashMap of names and taps must be passed to
          FlowConnector in order to bind taps to branches.
        </para>

        <para>Since there is only one tail (the "join" pipe), we don't need to
          bind the sink to a branch name. Nor do we need to pass the heads of
          the assembly to the FlowConnector, since it can determine the heads of
          the pipe assembly on the fly. However, when creating more complex
          Flows with multiple heads and tails, all taps must be explicitly named
          and the proper
          <code>connect()</code>
          method must be called.
        </para>
      </section>

      <section>
        <title xreflabel="Configuring Flows"
               xml:id="configuring-flows">Configuring Flows
        </title>

        <para>The FlowConnector constructor accepts the
          <classname>java.util.Property</classname>
          object so that default
          Cascading and Hadoop properties can be passed down through the planner
          to the Hadoop runtime. Subsequently any relevant Hadoop
          <code>hadoop-default.xml</code>
          properties may be added. For instance,
          it's very common to add
          <code>mapred.map.tasks.speculative.execution</code>,
          <code>mapred.reduce.tasks.speculative.execution</code>, or
          <code>mapred.child.java.opts</code>.
        </para>

        <para>One property that must be set for production applications is the
          application Jar class or Jar path.
        </para>

        <example>
          <title>Configuring the Application Jar</title>

          <xi:include href="flow-properties.xml"/>
        </example>

        <para>More information on packaging production applications can be
          found in<xref linkend="executing-processes"/>.
        </para>

        <para>Note the pattern of using a static property-setter method
          (<classname>cascading.flow.FlowConnector.setApplicationJarPath</classname>).
          Other classes that can be used to set properties are
          <classname>cascading.flow.MultiMapReducePlanner</classname>
          and
          <classname>cascading.flow.Flow</classname>.
        </para>

        <para>Since the
          <classname>FlowConnector</classname>
          can be reused,
          any properties passed on the constructor will be handed to all the
          Flows it is used to create. If Flows need to be created with different
          default properties, a new FlowConnector will need to be instantiated
          with those properties.
        </para>
      </section>

      <section>
        <title xreflabel="Skipping Flows" xml:id="skipping-flows">Skipping
          Flows
        </title>

        <para>When a Flow participates in a Cascade, the
          <classname>Flow#isSkip()</classname>
          method is consulted before
          calling
          <classname>Flow#start()</classname>
          on the flow. The result is
          based on the Flow's "skip strategy". By default,
          <methodname>isSkip()</methodname>
          returns true if any of the sinks are
          stale - i.e., the sinks don't exist or the resources are older than
          the sources. However, the strategy can be changed via the
          <classname>Cascade#setFlowSkipStrategy()</classname>
          method, which can
          be called before or after a particular
          <classname>Flow</classname>
          instance has been created.
        </para>

        <para>Cascading provides a choice of two standard skip
          strategies:
        </para>

        <variablelist>
          <varlistentry>
            <term>FlowSkipIfSinkStale</term>

            <listitem>
              <para>This strategy -
                <classname>cascading.flow.FlowSkipIfSinkStale</classname>
                - is
                the default. Sinks are treated as stale if they don't exist or
                the resources are older than the sources. If the SinkMode for
                the sink tap is REPLACE, then the tap is treated as
                stale.
              </para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>FlowSkipIfSinkExists</term>

            <listitem>
              <para>The
                <classname>cascading.flow.FlowSkipIfSinkExists</classname>
                strategy skips the Flow if the sink tap exists, regardless of
                age. If the
                <classname>SinkMode</classname>
                for the sink tap is
                <code>REPLACE</code>, then the tap is treated as stale.
              </para>
            </listitem>
          </varlistentry>
        </variablelist>

        <para>Additionally, you can implement custom skip strategies by using
          the interface
          <classname>cascading.flow.FlowSkipStrategy</classname>.
        </para>

        <para>Note that
          <classname>Flow#start()</classname>
          does not consult
          the
          <methodname>isSkip()</methodname>
          method, and consequently always
          tries to start the Flow if called. It is up to the user code to call
          <classname>isSkip()</classname>
          to determine whether the current
          strategy indicates that the Flow should be skipped.
        </para>
      </section>

      <section>
        <title>Creating Flows from a JobConf</title>

        <para>If a MapReduce job already exists and needs to be managed by a
          Cascade, then the
          <classname>cascading.flow.MapReduceFlow</classname>
          class should be used. To do this, after creating a Hadoop
          <classname>JobConf</classname>
          instance simply pass it into the
          <classname>MapReduceFlow</classname>
          constructor. The resulting
          <classname>Flow</classname>
          instance can be used like any other
          Flow.
        </para>
      </section>

      <section>
        <title>Creating Custom Flows</title>

        <para>Any custom Class can be treated as a Flow if given the correct
          <link xlink:href="http://github.com/cwensel/riffle">Riffle</link>
          annotations. Riffle is an Apache-licensed set of Java Annotations that
          identify specific methods on a Class as providing specific life-cycle
          and dependency functionality. For more information, see the Riffle
          documentation and examples. To use with Cascading, a Riffle-annotated
          instance must be passed to the
          <classname>cascading.flow.ProcessFlow</classname>
          constructor method.
          The resulting
          <classname>ProcessFlow</classname>
          instance can be used
          like any other Flow instance.
        </para>

        <para>Since many algorithms need to perform multiple passes over a
          given data set, a Riffle-annotated Class can be written that
          internally creates Cascading Flows and executes them until no more
          passes are needed. This is like nesting Flows or Cascades in a parent
          Flow, which in turn can participate in a Cascade.
        </para>
      </section>
    </section>

    <section>
      <title xreflabel="Cascades" xml:id="cascades">Cascades</title>

      <mediaobject>
        <imageobject role="fo">
          <imagedata contentwidth="2in" fileref="images/cascade.svg"/>
        </imageobject>

        <imageobject role="html">
          <imagedata contentwidth="2in" fileref="images/cascade.png"/>
        </imageobject>
      </mediaobject>

      <para>A Cascade allows multiple Flow instances to be executed as a
        single logical unit. If there are dependencies between the Flows, they
        are executed in the correct order. Further, Cascades act like Ant builds
        or Unix make files.
      </para>

      <para>By default, a Cascade only executes Flows that have stale sinks -
        i.e., output data that is older than the input data. For more on this,
        see<xref linkend="skipping-flows"/>.
      </para>

      <example>
        <title>Creating a new Cascade</title>

        <xi:include href="simple-cascade.xml"/>
      </example>

      <para>When passing Flows to the CascadeConnector, order is not
        important. The CascadeConnector automatically identifies the
        dependencies between the given Flows and creates a scheduler that starts
        each Flow as its data sources become available. If two or more Flow
        instances have no dependencies, they are submitted together so that they
        can execute in parallel.
      </para>

      <para>For more information, see the section on<xref
        linkend="cascade-scheduler"/>.
      </para>

      <para>If an instance of
        <classname>cascading.flow.FlowSkipStrategy</classname>
        is given to a
        <classname>Cascade</classname>
        instance (via the
        <classname>Cascade#setFlowSkipStrategy()</classname>
        method), it is
        consulted for every Flow instance managed by that Cascade, and all skip
        strategies on those Flow instances are ignored. For more information on
        skip strategies, see<xref linkend="skipping-flows"/>.
      </para>
    </section>
  </chapter>

  <chapter>
    <info>
      <title xreflabel="Executing Processes"
             xml:id="executing-processes">Executing Processes
      </title>
    </info>

    <section>
      <title>Introduction</title>

      <para>Cascading requires Apache Hadoop to be installed and correctly
        configured. Hadoop is an Open Source Apache project, freely available
        for download from the Hadoop website,<link
          xlink:href="http://hadoop.apache.org/core/">http://hadoop.apache.org/core/</link>.
      </para>
    </section>

    <section>
      <title xreflabel="Building Cascading Applications"
             xml:id="building-processes">Building
      </title>

      <para>Cascading ships with several jars, including the following:</para>

      <variablelist>
        <varlistentry>
          <term>cascading-1.2.x.jar</term>

          <listitem>
            <para>This jar contains the relevant Cascading class files and
              libraries, with a Hadoop-friendly
              <filename>lib</filename>
              folder
              that contains all third-party dependencies.
            </para>
          </listitem>
        </varlistentry>

        <varlistentry>
          <term>cascading-core-1.2.x.jar</term>

          <listitem>
            <para>This jar contains the Cascading Core class files. It should
              be packaged with<filename>lib/*.jar</filename>.
            </para>
          </listitem>
        </varlistentry>

        <varlistentry>
          <term>cascading-xml-1.2.x.jar</term>

          <listitem>
            <para>This jar contains Cascading XML module class files. It
              should be packaged with<filename>lib/xml/*.jar</filename>.
            </para>
          </listitem>
        </varlistentry>

        <varlistentry>
          <term>cascading-test-1.2.x.jar</term>

          <listitem>
            <para>This jar contains Cascading unit tests. If writing custom
              modules for cascading, it may be helpful to sub-class
              <classname>cascading.CascadingTestCase</classname>.
            </para>
          </listitem>
        </varlistentry>
      </variablelist>

      <para><?oxy_comment_start author="Jim" timestamp="20120402T042508-0700" comment="Jim - this plus the text following the code samples is a little repetitive/messy."?>
        Cascading<?oxy_comment_end ?>
        works with either of the two Hadoop processing modes - the default local
        stand-alone mode, and the pseudo- or fully-distributed cluster mode. As
        specified in the Hadoop documentation, running in cluster mode requires
        the creation of a Hadoop job jar that includes the Cascading jars, and
        any dependent third-party jars, in its
        <filename>lib</filename>
        directory.
      </para>

      <example>
        <title>Sample Ant Build - Properties</title>

        <xi:include href="sample-build-properties.xml"/>
      </example>

      <example>
        <title>Sample Ant Build - Target</title>

        <xi:include href="sample-build-target.xml"/>
      </example>

      <para>The above Ant snippets can be used in your project to create a
        Hadoop jar for submission on your cluster. As noted earlier, all Hadoop
        applications that are to be run in a cluster must be packaged with all
        third-party libraries in a directory named
        <filename>lib</filename>
        in
        the final application Jar file. This is true regardless of whether they
        are Cascading applications or raw Hadoop MapReduce applications. For
        more information, see<xref linkend="operating-modes"/>.
      </para>

      <para>Note that the snippets above are only intended to show how to
        include Cascading libraries. You are still required to compile your
        project into the
        <property>build.classes</property>
        path.
      </para>
    </section>

    <section>
      <title>Configuring</title>

      <para>During runtime, Hadoop must be told which application jar file
        should be pushed to the cluster. Typically, this is done via the Hadoop
        API
        <classname>JobConf</classname>
        object.
      </para>

      <para>Cascading offers a shorthand for configuring this
        parameter.
      </para>

      <xi:include href="flow-properties.xml"/>

      <para>Above we see two ways to set the same property - via the
        <methodname>setApplicationJarClass()</methodname>
        method, and via the
        <methodname>setApplicationJarPath()</methodname>
        method. The first
        method takes a Class object that owns the "main" function for this
        application. The assumption here is that
        <code>Main.class</code>
        is not
        located in a Java Jar that is stored in the
        <filename>lib</filename>
        folder of the application Jar. If it is, that Jar is pushed to the
        cluster, not the parent application jar.
      </para>

      <para>In your application, only one of these methods needs to be called,
        but one of them must be called to properly configure Hadoop.
      </para>
    </section>

    <section>
      <title
        xml:id="operating-modes"><?oxy_comment_start author="Jim" timestamp="20120402T042700-0700" comment="Chris, it
        &apos;s probably best if you write this section. I just threw in this dummy text to get you started."?>Operating
        Modes<?oxy_comment_end ?></title>

      <para>Cascading and Hadoop both support multiple operating modes that
        let you control whether data storage and processing are handled on a
        single computer or distributed throughout a cluster. (For development
        purposes, obviously, it's more practical to work on a single computer in
        local mode.)
      </para>

      <para>In Hadoop, the three modes are: standalone mode (run on the local
        computer, useful for coding), pseudo-distributed mode (run on an
        emulated "cluster" of one computer, useful for testing), and
        fully-distributed mode (run on a cluster, usually reserved for
        production purposes).
      </para>

      <para>In Cascading, the three modes are Cascading local mode (forced
        local processing), Hadoop current mode (local or distributed, as
        currently set in Hadoop), or Hadoop distributed mode (forced distributed
        processing).
      </para>

      <para>The three modes are described below.</para>

      <variablelist>
        <varlistentry>
          <term>Local mode (Cascading)</term>

          <listitem>
            <para>description</para>
          </listitem>
        </varlistentry>

        <varlistentry>
          <term>Local mode (Hadoop)</term>

          <listitem>
            <para>description</para>
          </listitem>
        </varlistentry>

        <varlistentry>
          <term>Pseudo-distributed mode</term>

          <listitem>
            <para>description</para>
          </listitem>
        </varlistentry>

        <varlistentry>
          <term>Distributed mode</term>

          <listitem>
            <para>description</para>
          </listitem>
        </varlistentry>
      </variablelist>
    </section>

    <section>
      <title>Executing</title>

      <para>Running a Cascading application is the same as running any Hadoop
        application. After packaging your application into a single jar (see
        <xref linkend="building-processes"/>), you must use
        <filename>bin/hadoop</filename>
        to submit the application to the
        cluster.
      </para>

      <para>For example, to execute an application stuffed into
        <filename>your-application.jar</filename>, call the Hadoop shell
        script:
      </para>

      <example>
        <title>Running a Cascading Application</title>

        <para>
          <programlisting>$HADOOP_HOME/bin/hadoop jar your-application.jar [some params]</programlisting>
        </para>
      </example>

      <para>If the configuration scripts in
        <envar>$HADOOP_CONF_DIR</envar>
        are configured to use a cluster, the Jar is pushed into the cluster for
        execution.
      </para>

      <para>Cascading does not rely on any environment variables like
        <envar>$HADOOP_HOME</envar>
        or<envar>$HADOOP_CONF_DIR</envar>, only
        <filename>bin/hadoop</filename>
        does.
      </para>

      <para>It should be noted that even though
        <filename>your-application.jar</filename>
        is passed on the command line
        to<filename>bin/hadoop</filename>, this in no way configures Hadoop to
        push this jar into the cluster. You must still call one of the property
        setters mentioned above to set the proper path to the application jar.
        If misconfigured, it's likely that one of the internal libraries (found
        in the lib folder) will be pushed to the cluster instead, and
        <classname>ClassNotFoundException</classname>'s will be thrown.
      </para>
    </section>
  </chapter>

  <chapter>
    <info>
      <title>Using and Developing Operations</title>
    </info>

    <section>
      <title>Introduction</title>

      <para>So far we've talked about setting up sources and sinks, shaping
        the data streams, referencing the data fields, and so on. Within this
        Pipe framework, Operations are used to act upon the data - alter it,
        filter it, tabulate it, analyze it, and so on. You can use the standard
        Operations in the Cascading library to create powerful and robust
        applications, and you can combine them in chains (much like Unix
        operations such as<command>sed</command>,<command>grep</command>,
        <command>sort</command>,<command>uniq</command>, and
        <command>awk</command>). And if you want to go further, it's also very
        simple to develop custom Operations in Cascading.
      </para>

      <para>There are four kinds of Operations:
        <classname>Function</classname>,<classname>Filter</classname>,
        <classname>Aggregator</classname>, and
        <classname>Buffer</classname>.
      </para>

      <mediaobject>
        <imageobject role="fo">
          <imagedata align="center" contentwidth="3in"
                     fileref="images/operations.svg"/>
        </imageobject>

        <imageobject role="html">
          <imagedata align="center" contentwidth="3in"
                     fileref="images/operations.png"/>
        </imageobject>
      </mediaobject>

      <para>All Operations require an input argument Tuple. All Operations
        return zero or more Tuple object results - except
        <classname>Filter</classname>, which simply returns a Boolean indicating
        whether to discard the current Tuple. A<classname>Function</classname>,
        for instance, can parse a string and return a new Tuple for every value
        parsed (i.e., one Tuple for each "word"), or it may create a single
        Tuple with every parsed value included as an element in one Tuple object
        (e.g., one Tuple with "first-name" and "last-name" fields).
      </para>

      <para>In theory, a
        <classname>Function</classname>
        can be used as a
        <classname>Filter</classname>. However, the
        <classname>Filter</classname>
        type is optimized for filtering, and can
        be combined with logical Operations such as<classname>Not</classname>,
        <classname>And</classname>,<classname>Or</classname>, etc.
      </para>

      <para>During runtime, Operations actually receive arguments as one or
        more instances of the TupleEntry object. The TupleEntry object holds
        both an instance of
        <classname>Fields</classname>
        and the current
        <classname>Tuple</classname>
        for which the
        <classname>Fields</classname>
        object defines fields.
      </para>

      <para>Except for<classname>Filter</classname>, all Operations must
        declare result Fields, and if the actual output does not match the
        declaration, the process will fail. For example, consider a
        <classname>Function</classname>
        written to parse words out of a String
        and return a new Tuple for each word. If it declares that its intended
        output is a Tuple with a single field named "word", and then returns
        more values in the Tuple beyond that single "word", processing will
        halt. However, Operations designed to return arbitrary numbers of values
        in a result Tuple may declare<code>Fields.UNKNOWN</code>.
      </para>

      <para>The Cascading planner always attempts to "fail fast" where
        possible by checking the field name dependencies between Pipes and
        Operations, but there may be some cases the planner can't account
        for.
      </para>

      <para>All Operations must be wrapped by either an
        <classname>Each</classname>
        or an
        <classname>Every</classname>
        pipe
        instance. The pipe is responsible for passing in an argument Tuple and
        accepting the resulting output Tuple.
      </para>

      <para>Operations, by
        default, <?oxy_comment_start author="Jim" timestamp="20120312T181425-0700" comment="Does this mean that Cascading guarantees they are safe, or that devs should make them safe?"?>
        are<?oxy_comment_end ?>
        "safe". A safe Operation is idempotent; it can safely execute multiple
        times on the same Tuple; it has no side-effects. If an Operation is not
        idempotent, the method
        <code>isSafe()</code>
        must return
        <code>false</code>. This value influences how the Cascading planner
        renders the Flow under certain circumstances.
      </para>
    </section>

    <section>
      <title>Functions</title>

      <para>A
        <classname>Function</classname>
        expects a stream of individual
        argument Tuples, and returns zero or more result Tuples for each of
        them. Like a<classname>Filter</classname>, a
        <classname>Function</classname>
        is used with an
        <classname>Each</classname>
        pipe, which may follow any pipe type.
      </para>

      <para>To create a custom<classname>Function</classname>, subclass the
        class
        <code>cascading.operation.BaseOperation</code>
        and implement the
        interface<code>cascading.operation.Function</code>. Since the
        <code>BaseOperation</code>
        has been subclassed, the
        <code>operate</code>
        method, as defined on the
        <code>Function</code>
        interface, is the only
        method that must be implemented.
      </para>

      <example>
        <title>Custom Function</title>

        <xi:include href="custom-function.xml"/>
      </example>

      <para>Whenever possible, functions should declare both the number of
        argument values they expect and the field names of the Tuple they
        return. However, these declarations are optional, as explained
        below.
      </para>

      <para>For input, functions must accept one or more values in a Tuple as
        arguments. If not specified, the default is to accept any number of
        values (<code>Operation.ANY</code>). Cascading verifies during planning
        that the number of arguments selected matches the number of arguments
        expected.
      </para>

      <para>For output, it's a good practice to declare the field names that a
        function returns. If not specified, the default is
        <code>Fields.UNKNOWN</code>, meaning that an unknown number of fields
        are returned in each Tuple.
      </para>

      <para>Both declarations - input arguments and output fields - must be
        done on the constructor, either by passing default values to the
        <code>super</code>
        constructor, or by accepting the values from the user
        via a constructor implementation.
      </para>

      <example>
        <title>Add Values Function</title>

        <xi:include href="sum-function.xml"/>
      </example>

      <para>The example above implements a
        <classname>Function</classname>
        that accepts two values in the argument Tuple, adds them together, and
        returns the result in a new Tuple.
      </para>

      <para>The first constructor above assumes a default field name for the
        field that this
        <classname>Function</classname>
        returns. In practice,
        it's good to give the user the option of overriding the declared field
        names, allowing them to prevent possible field name collisions that
        might cause the planner to fail.
      </para>
    </section>

    <section>
      <title>Filter</title>

      <para>A
        <classname>Filter</classname>
        expects a stream of individual
        argument Tuples and returns a Boolean value for each, stating whether it
        should be discarded. Like a<classname>Function</classname>, a
        <classname>Filter</classname>
        is used with an
        <classname>Each</classname>
        pipe, which may follow any pipe type.
      </para>

      <para>To create a custom<classname>Filter</classname>, subclass the
        class
        <code>cascading.operation.BaseOperation</code>
        and implement the
        interface<code>cascading.operation.Filter</code>. Because
        <code>BaseOperation</code>
        has been subclassed, the
        <code>isRemove</code>
        method, as defined on the
        <code>Filter</code>
        interface, is the only method that must be implemented.
      </para>

      <example>
        <title>Custom Filter</title>

        <xi:include href="custom-filter.xml"/>
      </example>

      <para>Filters must accept one or more values in a Tuple as arguments,
        and should declare the number of argument values they expect. If not
        specified, the default is to accept any number of values
        (<code>Operation.ANY</code>). Cascading verifies during planning that
        the number of arguments selected matches the number of arguments
        expected.
      </para>

      <para>The number of arguments declaration must be done on the
        constructor, either by passing a default value to the
        <code>super</code>
        constructor, or by accepting the value from the user via a constructor
        implementation.
      </para>

      <example>
        <title>String Length Filter</title>

        <xi:include href="stringlength-filter.xml"/>
      </example>

      <para>The example above implements a
        <classname>Filter</classname>
        that
        accepts two arguments and filters out the current Tuple if the first
        argument, String length, is greater than the integer value of the second
        argument.
      </para>
    </section>

    <section>
      <title>Aggregator</title>

      <para>An
        <classname>Aggregator</classname>
        expects a stream of tuple
        groups (the output of a GroupBy or CoGroup pipe), and returns zero or
        more result tuples for every group. An
        <classname>Aggregator</classname>
        may only be used with an
        <classname>Every</classname>
        pipe, which may
        follow a<classname>GroupBy</classname>, a
        <classname>CoGroup</classname>, or another
        <classname>Every</classname>
        pipe, but not an<classname>Each</classname>.
      </para>

      <para>To create a custom<classname>Aggregator</classname>, subclass the
        class
        <code>cascading.operation.BaseOperation</code>
        and implement the
        interface<code>cascading.operation.Aggregator</code>. Because
        <code>BaseOperation</code>
        has been subclassed, the<code>start</code>,
        <code>aggregate</code>, and
        <code>complete</code>
        methods, as defined on
        the
        <code>Aggregator</code>
        interface, are the only methods that must be
        implemented.
      </para>

      <example>
        <title>Custom Aggregator</title>

        <xi:include href="custom-aggregator.xml"/>
      </example>

      <para>Whenever possible, Aggregators should declare both the number of
        argument values they expect and the field names of the Tuple they
        return. However, these declarations are optional, as explained
        below.
      </para>

      <para>For input, Aggregators must accept one or more values in a Tuple
        as arguments. If not specified, the default is to accept any number of
        values (<code>Operation.ANY</code>). Cascading verifies during planning
        that the number of arguments selected is the same as the number of
        arguments expected.
      </para>

      <para>For output, it's good practice for Aggregators to declare the
        field names they return. If not specified, the default is
        <code>Fields.UNKNOWN</code>, meaning that an unknown number of fields
        are returned in each Tuple.
      </para>

      <para>Both declarations - input arguments and output fields - must be
        done on the constructor, either by passing default values to the
        <code>super</code>
        constructor, or by accepting the values from the user
        via a constructor implementation.
      </para>

      <example>
        <title>Add Tuples Aggregator</title>

        <xi:include href="sum-aggregator.xml"/>
      </example>

      <para>The example above implements an
        <classname>Aggregator</classname>
        that accepts a value in the argument Tuple, adds all the argument tuples
        in the current grouping, and returns the result as a new Tuple.
      </para>

      <para>The first constructor above assumes a default field name that this
        <classname>Aggregator</classname>
        returns. In practice, it's good to
        give the user the option of overriding the declared field names,
        allowing them to prevent possible field name collisions that might cause
        the planner to fail.
      </para>

      <para>There are several constraints on the use of Aggregators that may
        not be self-evident. These are detailed in the Javadoc
      </para>
    </section>

    <section>
      <title>Buffer</title>

      <para>A
        <classname>Buffer</classname>
        expects set of argument tuples in
        the same grouping, and may return zero or more result tuples.
      </para>

      <para>A
        <classname>Buffer</classname>
        is very similar to an
        <classname>Aggregator</classname>, except that it receives the current
        Grouping Tuple, and an iterator of all the arguments it expects, for
        every value Tuple in the current grouping - all on the same method call.
        This is very similar to the typical Reducer interface, and is best used
        for operations that need high visibility to the previous and next
        elements in the stream - such as smoothing a series of time-stamps where
        there are missing values.
      </para>

      <para>A
        <classname>Buffer</classname>
        may only be used with an
        <classname>Every</classname>
        pipe, and it may only follow a
        <classname>GroupBy</classname>
        or
        <classname>CoGroup</classname>
        pipe
        type.
      </para>

      <para>To create a custom<classname>Buffer</classname>, subclass the
        class
        <code>cascading.operation.BaseOperation</code>
        and implement the
        interface<code>cascading.operation.Buffer</code>. Because
        <code>BaseOperation</code>
        has been subclassed, the
        <code>operate</code>
        method, as defined on the
        <code>Buffer</code>
        interface, is the only
        method that must be implemented.
      </para>

      <example>
        <title>Custom Buffer</title>

        <xi:include href="custom-buffer.xml"/>
      </example>

      <para>Buffers should declare both the number of argument values they
        expect and the field names of the Tuple they return.
      </para>

      <para>For input, Buffers must accept one or more values in a Tuple as
        arguments. If not specified, the default is to accept any number of
        values (Operation.ANY). During the planning phase, Cascading verifies
        that the number of arguments selected is the same as the number of
        arguments expected.
      </para>

      <para>For output, it's good practice for Aggregators to declare the
        field names they return. If not specified, the default is
        <code>Fields.UNKNOWN</code>, meaning that an unknown number of fields
        are returned in each Tuple.
      </para>

      <para>Both declarations - input arguments and output fields - must be
        done on the constructor, either by passing default values to the
        <code>super</code>
        constructor, or by accepting the values from the user
        via a constructor implementation.
      </para>

      <example>
        <title>Average Buffer</title>

        <xi:include href="average-buffer.xml"/>
      </example>

      <para>The example above implements a buffer that accepts a value in the
        argument Tuple, adds all these argument tuples in the current grouping,
        and returns the result divided by the number of argument tuples counted
        in a new Tuple.
      </para>

      <para>The first constructor above assumes a default field name for the
        field that this
        <classname>Buffer</classname>
        returns. In practice, it's
        good to give the user the option of overriding the declared field names,
        allowing them to prevent possible field name collisions that might cause
        the planner to fail
      </para>

      <para>Note that this example is somewhat artificial. In actual practice,
        an
        <classname>Aggregator</classname>
        would be a better way to compute
        averages for an entire dataset. A
        <classname>Buffer</classname>
        is
        better suited for calculating running averages across very large spans,
        for example.
      </para>

      <para>There are several constraints on the use of Buffers that may not
        be self-evident. These are detailed in the Javadoc.
      </para>
    </section>

    <section>
      <title>Operation and BaseOperation</title>

      <para>In all of the above sections, the
        <classname>cascading.operation.BaseOperation</classname>
        class was
        subclassed. This class is an implementation of the
        <classname>cascading.operation.Operation</classname>
        interface, and
        provides a few default method implementations. It is not strictly
        required to extend
        <classname>BaseOperation</classname>
        when
        implementing this interface, but it is very convenient to do so.
      </para>

      <para>When developing custom operations, the developer may need to
        initialize and destroy a resource. For example, when doing pattern
        matching, you might need to initialize a
        <classname>java.util.regex.Matcher</classname>
        and use it in a
        thread-safe way. Or you might need to open, and eventually close, a
        remote connection. But for performance reasons, the operation should not
        create or destroy the connection for each Tuple or every Tuple group
        that passes through.
      </para>

      <para>For this reason, the interface
        <interfacename>Operation</interfacename>
        declares two methods:
        <methodname>prepare()</methodname>
        and
        <methodname>cleanup()</methodname>. In the case of Hadoop and MapReduce,
        the
        <methodname>prepare()</methodname>
        and
        <methodname>cleanup()</methodname>
        methods are called once per Map or
        Reduce task. The
        <methodname>prepare()</methodname>
        method is called
        before any argument Tuple is passed in, and the
        <methodname>cleanup()</methodname>
        method is called after all Tuple
        arguments have been operated on. Within each of these methods, the
        developer can initialize a "context" object that can hold an open socket
        connection or
        <classname>Matcher</classname>
        instance. This context is
        user defined, and is the same mechanism used by the
        <classname>Aggregator</classname>
        operation - except that the
        <classname>Aggregator</classname>
        is also given the opportunity to
        initialize and destroy its context, via the
        <classname>start()</classname>
        and
        <classname>complete()</classname>
        methods.
      </para>

      <para>Note that if a "context" object is used, its type should be
        declared in the sub-class class declaration using the Java Generics
        notation.
      </para>
    </section>
  </chapter>

  <chapter>
    <info>
      <title>Advanced Processing</title>
    </info>

    <section>
      <title xreflabel="SubAssemblies"
             xml:id="subassemblies">SubAssemblies
      </title>

      <para>In Cascading, SubAssemblies are reusable pipe assemblies that are
        linked into larger pipe assemblies. They function much like subroutines
        in a larger program. Subassemblies are a good way to organize complex
        pipe assemblies, and they allow for commonly-used pipe assemblies to be
        packaged into libraries for inclusion in other projects by other
        users.
      </para>

      <para>To create a SubAssembly, subclass the
        <classname>cascading.pipe.SubAssembly</classname>
        class.
      </para>

      <example>
        <title>Creating a SubAssembly</title>

        <xi:include href="custom-subassembly.xml"/>
      </example>

      <para>In the example above, we pass in (as parameters via the
        constructor) the pipes that we wish to continue assembling against, and
        in the last line we register the "join" pipe as a tail. This allows
        SubAssemblies to be nested within larger pipe assemblies or other
        SubAssemblies.
      </para>

      <example>
        <title>Using a SubAssembly</title>

        <xi:include href="simple-subassembly.xml"/>
      </example>

      <para>The example above demonstrates how to include a SubAssembly into a
        new pipe assembly.
      </para>

      <para>Note that in a SubAssembly that represents a split - that is, a
        SubAssembly with two or more tails - you can use the
        <methodname>getTails()</methodname>
        method to access the array of tails
        set internally by the
        <methodname>setTails()</methodname>
        method.
      </para>

      <example>
        <title>Creating a Split SubAssembly</title>

        <xi:include href="split-subassembly.xml"/>
      </example>

      <example>
        <title>Using a Split SubAssembly</title>

        <xi:include href="simple-split-subassembly.xml"/>
      </example>

      <para>To rephrase, if a
        <classname>SubAssembly</classname>
        does not
        split the incoming Tuple stream, the SubAssembly instance can be passed
        directly to the next Pipe instance. But, if the
        <classname>SubAssembly</classname>
        splits the stream into multiple
        branches, handles will be needed to access them. The solution is to pass
        each branch tail to the
        <methodname>setTails()</methodname>
        method, and
        call the
        <methodname>getTails()</methodname>
        method to get handles for
        the desired branches, which can be passed to subsequent instances of
        <classname>Pipe</classname>.
      </para>
    </section>

    <section>
      <title xreflabel="Stream Assertions"
             xml:id="stream-assertions"><?oxy_comment_start author="Jim" timestamp="20120323T171050-0700" comment="Jim - this section could be tightened up a little, esp at the end."?>
        Stream
        Assertions<?oxy_comment_end ?></title>

      <mediaobject>
        <imageobject role="fo">
          <imagedata align="center" contentwidth="6in"
                     fileref="images/stream-assertions.svg"/>
        </imageobject>

        <imageobject role="html">
          <imagedata align="center" contentwidth="6in"
                     fileref="images/stream-assertions.png"/>
        </imageobject>
      </mediaobject>

      <para>Stream assertions are simply a mechanism for asserting that one or
        more values in a tuple stream meet certain criteria. This is similar to
        the Java language "assert" keyword, or a unit test. An example would be
        "assert not null" or "assert matches".
      </para>

      <para>Assertions are treated like any other function or aggregator in
        Cascading. They are embedded directly into the pipe assembly by the
        developer. By default, if an assertion fails, the processing stops. As
        an alternative, an assertion failure can trigger a Failure Trap.
      </para>

      <para>Assertions may be more, or less, desirable in different contexts.
        For this reason, stream assertions can be treated as either "strict" or
        "validating".
        <emphasis role="italic">Strict</emphasis>
        assertions make
        sense when running tests against regression data - which should be
        small, and should represent many of the edge cases that the processing
        assembly must robustly support.
        <emphasis
          role="italic">Validating
        </emphasis>
        assertions, on the other hand, make
        more sense when running tests in staging, or when using data that may
        vary in quality due to an unmanaged source.
      </para>

      <para>And of course there are cases where assertions are unnecessary and
        only impede processing, and it would be best to just bypass them
        altogether.
      </para>

      <para>To handle all three of these situations, Cascading can be
        instructed to
        <emphasis role="italic">plan out</emphasis>
        (i.e., omit)
        strict assertions, validation assertions, or both when building the
        final MapReduce jobs. To create optimal performance, Cascading
        implements this by actually leaving the undesired assertions out of the
        final job (not merely switching them off).
      </para>

      <para>In this way, assertions support robust code while saving labor,
        demonstrating some of the advantages of building MapReduce jobs via a
        planner instead of manually hardcoding them.
      </para>

      <example>
        <title>Adding Assertions</title>

        <xi:include href="simple-assertion.xml"/>
      </example>

      <para>Again, assertions are added to a pipe assembly like any other
        operation, except that the
        <classname>AssertionLevel</classname>
        must be
        set to tell the planner how to treat the assertion during
        planning.
      </para>

      <example>
        <title>Planning Out Assertions</title>

        <xi:include href="simple-assertion-planner.xml"/>
      </example>

      <para>To configure the planner to remove some or all assertions, a
        property must be set via the
        <classname>FlowConnector#setAssertionLevel()</classname>
        method. Setting
        <classname>AssertionLevel.NONE</classname>
        removes all assertions.
        <classname>AssertionLevel.VALID</classname>
        keeps
        <code>VALID</code>
        assertions but removes
        <code>STRICT</code>
        ones. And
        <classname>AssertionLevel.STRICT</classname>
        keeps all assertions - the
        planner default value.
      </para>
    </section>

    <section>
      <title xreflabel="Failure Traps" xml:id="failure-traps">Failure
        Traps
      </title>

      <para>The following diagram shows the use of
        <emphasis
          role="italic">Failure Traps
        </emphasis>
        in a pipe assembly.
      </para>

      <para>
        <inlinemediaobject>
          <imageobject role="fo">
            <imagedata align="center" contentwidth="5in"
                       fileref="images/failure-traps.svg"/>
          </imageobject>

          <imageobject role="html">
            <imagedata align="center" contentwidth="5in"
                       fileref="images/failure-traps.png"/>
          </imageobject>
        </inlinemediaobject>
      </para>

      <para>Failure Traps are very similar to tap sinks (as opposed to tap
        sources), except
        that, <?oxy_comment_start author="Jim" timestamp="20120402T043525-0700" comment="This does not seem clear to me."?>
        since
        they are bound to a particular tail element of the pipe assembly, traps
        can be bound to intermediate pipe assembly segments - just like to a
        Stream Assertion.<?oxy_comment_end ?></para>

      <para>Whenever an operation fails and throws an exception, if there is
        an associated trap, the offending Tuple is saved to the resource
        specified by the trap tap. This allows the job to continue processing
        without any data loss.
      </para>

      <para>By design, clusters are hardware fault-tolerant - lose a node, and
        the cluster continues working. But fault tolerance for software is a
        little different. Failure Traps provide a means for the processing to
        continue without losing track of the data that caused the fault. For
        high fidelity applications, this may not be very important, but for low
        fidelity applications (like webpage indexing) this can dramatically
        improve processing reliability.
      </para>

      <example>
        <title>Setting Traps</title>

        <xi:include href="simple-traps.xml"/>
      </example>

      <para>The example above binds a trap tap to the pipe assembly segment
        named "assertions". Note how we can name branches and segments by using
        a single
        <classname>Pipe</classname>
        instance, and that the naming
        applies to all subsequent
        <classname>Pipe</classname>
        instances.
      </para>

      <para>Traps are for exceptional cases, in the same way that Java
        Exception handling is. Traps are not intended for application flow
        control, and not a means to filter some data into other locations.
        Applications that need to filter out bad data should do so explicitly,
        using filters. For more on this, see<xref
          linkend="handling-bad-data"/>.
      </para>
    </section>

    <section>
      <title>Event Handling</title>

      <para>Each Flow has the ability to execute callbacks via an event
        listener. This ability is useful when an external application needs to
        be notified that a Flow has started, halted, completed, or thrown an
        exception.
      </para>

      <para>For instance, at the completion of a flow that runs on an Amazon
        EC2 Hadoop cluster, an SQS message can be sent to notify another
        application to fetch the job results from S3. At the same time,
        <?oxy_comment_start author="Jim" timestamp="20120326T002427-0700" comment="Chris - this originally said &quot;it
        can start&quot;, but I wasn&apos;t clear what &quot;it&quot; is, so I tried this wording to see if it would fly.
        Change it if it&apos;s not quite right."?>the
        callback can start<?oxy_comment_end ?> the process of shutting down the
        cluster if no more work is queued up.
      </para>

      <para>Flows support event listeners through the
        <classname>cascading.flow.FlowListener</classname>
        interface, which
        supports four events:
      </para>

      <variablelist>
        <varlistentry>
          <term>onStarting</term>

          <listitem>
            <para>The onStarting event is fired when a Flow instance receives
              the
              <code>start()</code>
              message.
            </para>
          </listitem>
        </varlistentry>

        <varlistentry>
          <term>onStopping</term>

          <listitem>
            <para>The onStopping event is fired when a Flow instance receives
              the
              <code>stop()</code>
              message.
            </para>
          </listitem>
        </varlistentry>

        <varlistentry>
          <term>onCompleted</term>

          <listitem>
            <para>The onCompleted event is fired when a Flow instance has
              completed all work, regardless of success or failure. If an
              exception was thrown, onThrowable will be fired before this
              event.
            </para>
          </listitem>
        </varlistentry>

        <varlistentry>
          <term>onThrowable</term>

          <listitem>
            <para>The onThrowable event is fired if any internal job client
              throws a Throwable type. This throwable is passed as an argument
              to the event. onThrowable should return true if the given
              throwable was handled, and should not be rethrown from the
              <code>Flow.complete()</code>
              method.
            </para>
          </listitem>
        </varlistentry>
      </variablelist>
    </section>

    <section>
      <title xreflabel="Template Taps" xml:id="template-tap">Template
        taps
      </title>

      <para>The
        <classname>Templatetap</classname>
        <classname>tap</classname>
        class provides a simple means to break large datasets into smaller sets
        based on data item values. This is commonly called "binning" the data,
        where each "bin" of data is named after the data value(s) shared by the
        members of that bin. For example, organizing log files by month and
        year.
      </para>

      <xi:include href="template-tap.xml"/>

      <para>In the example above, we construct a parent
        <classname>Hfs</classname>
        <classname>tap</classname>
        and pass it to the
        constructor of a
        <classname>Templatetap</classname>
        instance, along with
        a String format "template". This format template is populated in the
        order in which values are declared via the
        <classname>Scheme</classname>
        class. If more complex path formatting is necessary, you may subclass
        the<classname>Templatetap</classname>.
      </para>

      <para>Note that you can only create sub-directories to bin data into.
        Hadoop must still write "part" files into each bin directory.
      </para>

      <para>One last thing to keep in mind is whether "binning" happens during
        the Map phase or the Reduce phase. By doing a
        <classname>GroupBy</classname>
        on the values used to populate the
        template, binning will happen during the Reduce phase, and will likely
        scale much better in cases where there are a very large number of unique
        grouping keys.
      </para>
    </section>

    <section>
      <title>Partial Aggregation instead of Combiners</title>

      <para>Cascading does not support MapReduce "Combiners".
        <?oxy_comment_start author="Jim" timestamp="20120325T191036-0700" comment="At some future point it might be good to add a one-sentence definition of Combiner to serve a larger, less Hadoop-savvy audience. This hints at it but doesn
        &apos;t quite state what it is."?>Combiners<?oxy_comment_end ?>
        are very powerful in that they reduce the I/O between the Mappers and
        Reducers - why send all of your Mapper data to Reducers when you can
        compute some values on the Map side and combine them in the Reducer? But
        Combiners are limited to Associative and Commutative functions only,
        such as "sum" and "max". And the process requires that the values output
        by the Map task must be serialized, sorted (which involves
        deserialization and comparison), deserialized again, and operated on -
        after which the results are again serialized and sorted. Combiners trade
        CPU for gains in I/O.
      </para>

      <para>Cascading takes a different approach. It provides a mechanism to
        perform partial aggregations on the Map side and combine the results on
        the Reduce side, but trades memory, instead of CPU, for I/O gains by
        caching values (up to a threshold limit). This bypasses the redundant
        serialization, deserialization, and sorting. Also, Cascading allows
        aggregate function to be implemented - not just Associative and
        Commutative functions.
      </para>

      <para>Cascading supports a few built-in partial aggregate operations,
        including AverageBy, CountBy, and SumBy. These are actually
        SubAssemblies, not operations, and are implementations of the
        AggregateBy SubAssembly. For more on this, see the section on<xref
          linkend="aggregate-by"/>.
      </para>

      <para>Using partial aggregate operations is quite easy. They are
        actually less verbose than a standard Aggregate operation.
      </para>

      <example>
        <title>Using a SumBy</title>

        <xi:include href="partials-sumby.xml"/>
      </example>

      <para>For composing multiple partial aggregate operations, things are
        done a little differently.
      </para>

      <example>
        <title>Composing partials with AggregateBy</title>

        <xi:include href="partials-compose.xml"/>
      </example>

      <para>It's important to note that a
        <classname>GroupBy</classname>
        Pipe
        is embedded in the resulting assemblies above. But only one GroupBy is
        performed in the case of the AggregateBy, and all of the partial
        aggregations will be performed simultaneously. It is also important to
        note that, depending on the final pipe assembly, the Map side partial
        aggregate functions may be planned into the previous Reduce operation in
        Hadoop, further improving the performance of the application.
      </para>
    </section>
  </chapter>

  <chapter>
    <info>
      <title>Built-In Operations</title>
    </info>

    <section>
      <title>Identity Function</title>

      <para>The
        <classname>cascading.operation.Identify</classname>
        function
        is used to "shape" a tuple stream. Here are some common patterns.
      </para>

      <para>
        <variablelist>
          <varlistentry>
            <term>Discard unused fields</term>

            <listitem>
              <para>Here Identity passes its arguments out as results, thanks
                to the
                <code>Fields.ARGS</code>
                field declaration.
              </para>

              <xi:include href="identity-discard-fields-long.xml"/>

              <para>In practice the field declaration can be left out, as
                <code>Field.ARGS</code>
                is the default declaration for the
                Identity function. And
                <code>Fields.RESULTs</code>
                can be left
                off, as it is the default for the
                <classname>Every</classname>
                pipe. Thus, simpler code yields the same result:
              </para>

              <xi:include href="identity-discard-fields.xml"/>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>Rename all fields</term>

            <listitem>
              <para>Here Identity renames the incoming arguments. Since
                Fields.RESULTS is implied, the incoming Tuple is replaced by the
                selected arguments and given new field names as declared on
                Identity.
              </para>

              <xi:include href="identity-rename-fields-explicit.xml"/>

              <para>In the example above, if there were more fields than "ip"
                and "method", it would work fine - all the extra fields would be
                discarded. But if the same were true for the next example, the
                planner would fail.
              </para>

              <xi:include href="identity-rename-fields-long.xml"/>

              <para>Since
                <code>Fields.ALL</code>
                is the default argument
                selector for the
                <classname>Each</classname>
                pipe, it can be
                left out as shown below. Again, the above and below examples
                will fail unless there are exactly two fields in the tuples of
                the incoming stream.
              </para>

              <xi:include href="identity-rename-fields.xml"/>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>Rename a single field</term>

            <listitem>
              <para>Here we rename a single field and return it, along with an
                input Tuple field, as the result. All other fields are
                dropped.
              </para>

              <xi:include href="identity-rename-some.xml"/>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>Coerce values to specific primitive types</term>

            <listitem>
              <para>Here we replace the Tuple String values "status" and
                "size" with
                <classname>int</classname>
                and
                <classname>long</classname>
                values, respectively. All other
                fields are dropped.
              </para>

              <xi:include href="identity-coerce.xml"/>

              <para>Or we can replace just the Tuple String value "status"
                with an<classname>int</classname>, while keeping all the other
                values in the output Tuple.
              </para>

              <xi:include href="identity-coerce-single.xml"/>
            </listitem>
          </varlistentry>
        </variablelist>
      </para>
    </section>

    <section>
      <title xreflabel="Debug Function" xml:id="debug-function">Debug
        Function
      </title>

      <para>The
        <classname>cascading.operation.Debug</classname>
        function is a
        utility function (actually, it's a<classname>Filter</classname>) that
        prints the current argument Tuple to either
        <code>stdout</code>
        or
        <code>stderr</code>. Used with one of the
        <classname>DebugLevel</classname>
        enum values
        (<classname>NONE</classname>,<classname>DEFAULT</classname>, or
        <classname>VERBOSE</classname>), different debug levels can be embedded
        in a pipe assembly.
      </para>

      <para>The example below inserts a
        <classname>Debug</classname>
        operation
        at the
        <classname>VERBOSE</classname>
        level, but configures the planner
        to remove all
        <classname>Debug</classname>
        operations from the resulting
        <classname>Flow</classname>.
      </para>

      <xi:include href="flow-debug.xml"/>
    </section>

    <section>
      <title>Sample and Limit Functions</title>

      <para>The Sample and Limit functions are used to limit the number of
        tuples that pass through a pipe assembly.
      </para>

      <para>
        <variablelist>
          <varlistentry>
            <term>Sample</term>

            <listitem>
              <para>The
                <classname>cascading.operation.filter.Sample</classname>
                filter
                allows a percentage of tuples to pass.
              </para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>Limit</term>

            <listitem>
              <para>The
                <classname>cascading.operation.filter.Limit</classname>
                filter
                allows a set number of tuples to pass.
              </para>
            </listitem>
          </varlistentry>
        </variablelist>
      </para>
    </section>

    <section>
      <title>Insert Function</title>

      <para>The
        <classname>cascading.operation.Insert</classname>
        function
        allows for insertion of constant literal values into the tuple
        stream.
      </para>

      <para>This is most useful when a splitting a tuple stream and one of the
        branches needs some identifying value, or when some missing parameter or
        value, like a date String for the current date, needs to be
        inserted.
      </para>
    </section>

    <section>
      <title>Text Functions</title>

      <para>Cascading includes a number of text functions in the
        <classname>cascading.operation.text</classname>
        package.
      </para>

      <para>
        <variablelist>
          <varlistentry>
            <term><?oxy_comment_start author="Jim" timestamp="20120321T173204-0700" comment="I resequenced these four items to match the sequence of the cascading.operation.text package documentation in the Javadoc. It
              &apos;s a trivial change but it seemed as good a sequence as any."?>
              DateFormatter<?oxy_comment_end ?></term>

            <listitem>
              <para>The
                <classname>cascading.operation.text.DateFormatter</classname>
                function is used to convert a date timestamp to a formatted
                String. This function expects a
                <classname>long</classname>
                value representing the number of milliseconds since January 1,
                1970, 00:00:00 GMT, and formats the output using
                <classname>java.text.SimpleDateFormat</classname>
                syntax.
              </para>

              <xi:include href="text-format-date.xml"/>

              <para>The example above converts a
                <classname>long</classname>
                timestamp ("ts") to a date String.
              </para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>DateParser</term>

            <listitem>
              <para>The
                <classname>cascading.operation.text.DateParser</classname>
                function is used to convert a text date String to a timestamp,
                using the
                <classname>java.text.SimpleDateFormat</classname>
                syntax. The timestamp is a
                <classname>long</classname>
                value
                representing the number of milliseconds since January 1, 1970,
                00:00:00 GMT. By default, the output is a field with the name
                "ts" (for timestamp), but this can be overridden by passing a
                declared Fields value.
              </para>

              <xi:include href="text-create-timestamp.xml"/>

              <para>In the example above, an Apache log-style date-time field
                is converted into a
                <classname>long</classname>
                timestamp.
              </para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>FieldJoiner</term>

            <listitem>
              <para>The
                <classname>cascading.operation.text.FieldJoiner</classname>
                function joins all the values in a Tuple with a specified
                delimiter and places the result into a new field. (For the
                opposite effect, see the RegexSplitter function.)
              </para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>FieldFormatter</term>

            <listitem>
              <para>The
                <classname>cascading.operation.text.FieldFormatter</classname>
                function formats Tuple values with a given String format and
                stuffs the result into a new field. The
                <classname>java.util.Formatter</classname>
                class is used to
                create a new formatted String.
              </para>
            </listitem>
          </varlistentry>
        </variablelist>
      </para>
    </section>

    <section>
      <title>Regular Expression Operations</title>

      <para>
        <variablelist>
          <varlistentry>
            <term>RegexSplitter</term>

            <listitem>
              <para>The
                <classname>cascading.operation.regex.RegexSplitter</classname>
                function splits an argument value based on a regex pattern
                String. (For the opposite effect, see the FieldJoiner function.)
                Internally, this function uses
                <classname>java.util.regex.Pattern#split()</classname>, and it
                behaves accordingly. By default, it splits on the TAB character
                ("\t"). If it is known that a determinate number of values will
                emerge from this function, it can declare field names. In this
                case, if the splitter encounters more split values than field
                names, the remaining values are discarded. For more information,
                see<classname>java.util.regex.Pattern#split( input, limit
                  )</classname>.
              </para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>RegexParser</term>

            <listitem>
              <para>The
                <classname>cascading.operation.regex.RegexParser</classname>
                function is used to extract a regex-matched value from an
                incoming argument value. If the regular expression is
                sufficiently complex, an
                <classname>int</classname>
                array may be
                provided to specify which regex groups should be returned in
                which field names.
              </para>

              <xi:include href="regex-parser.xml"/>

              <para>In the example above, a line from an Apache access log is
                parsed into its component parts. Note that the
                <classname>int[]</classname>
                groups array starts at 1, not 0.
                Group 0 is the whole group, so if the first field is included,
                it is a copy of "line" and not "ip".
              </para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>RegexReplace</term>

            <listitem>
              <para>The
                <classname>cascading.operation.regex.RegexReplace</classname>
                function is used to replace a regex-matched value with a
                specified replacement value. It can operate in a "replace all"
                or "replace first" mode. For more information, see the methods
                <classname>java.util.regex.Matcher#replaceAll()</classname>
                and
                <classname>java.util.regex.Matcher#replaceFirst()</classname>.
              </para>

              <xi:include href="regex-replace.xml"/>

              <para>In the example above, all adjoined white space characters
                are replaced with a single space character.
              </para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>RegexFilter</term>

            <listitem>
              <para>The
                <classname>cascading.operation.regex.RegexFilter</classname>
                function filters a Tuple stream based on a specified regex
                value. By default, tuples that match the given pattern are kept,
                and tuples that do not match are filtered out. This can be
                reversed by setting "removeMatch" to<code>true</code>. Also, by
                default, the whole Tuple is matched against the given regex
                String (in tab-delimited sections). If "matchEachElement" is set
                to<code>true</code>, the pattern is applied to each Tuple value
                individually. For more information, see the
                <classname>java.util.regex.Matcher#find()</classname>
                method.
              </para>

              <xi:include href="regex-filter.xml"/>

              <para>The above keeps all lines in which "68." appears at the
                start of the IP address.
              </para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>RegexGenerator</term>

            <listitem>
              <para>The
                <classname>cascading.operation.regex.RegexGenerator</classname>
                function emits a new tuple for every string (found in an input
                tuple) that matches a specified regex pattern.
              </para>

              <xi:include href="regex-generator.xml"/>

              <para>Above each "line" in a document is parsed into unique
                words and stored in the "word" field of each result
                Tuple.
              </para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>RegexSplitGenerator</term>

            <listitem>
              <para>The
                <classname>cascading.operation.regex.RegexSplitGenerator</classname>
                function emits a new Tuple for every split on the incoming
                argument value delimited by the given pattern String.
                <?oxy_comment_start author="Jim" timestamp="20120321T191258-0700" comment="Chris - hopefully this statement is accurate."?>
                The
                behavior is similar to the RegexSplitter function, except that
                (assuming multiple matches) RegexSplitter outputs a single tuple
                that may contain multiple values, and RegexSplitGenerator
                outputs multiple tuples that each contain only one
                value<?oxy_comment_end ?>.
              </para>
            </listitem>
          </varlistentry>
        </variablelist>
      </para>
    </section>

    <section>
      <title xreflabel="Expression Operations"
             xml:id="operation-expression">Java Expression Operations
      </title>

      <para>Cascading provides some support for dynamically-compiled Java
        expressions to be used in either
        <classname>Functions</classname>
        or
        <classname>Filters</classname>. This capability is provided by the
        Janino embedded Java compiler, which compiles the expressions into byte
        code for optimal processing speed. Janino is documented in detail on its
        website,<link
          xlink:href="http://www.janino.net/">http://www.janino.net/</link>.
      </para>

      <para>This capability allows an Operation to evaluate a suitable
        one-line Java expression, such as
        <code>a + 3 * 2</code>
        or<code>a &lt;
          7</code>, where the variable values (
        <code>a</code>
        and<code>b</code>)
        are passed in as Tuple fields. The result of the Operation thus depends
        on the evaluated result of the expression - in the first example, some
        number, and in the second, a Boolean value.
      </para>

      <para>
        <variablelist>
          <varlistentry>
            <term>ExpressionFunction</term>

            <listitem>
              <para><?oxy_comment_start author="Jim" timestamp="20120321T203349-0700" comment="Originally said: &quot;
                function dynamically resolves a given expression using argument Tuple values as inputs to the fields
                specified in the expression.&quot;"?>The<?oxy_comment_end ?>
                <classname>cascading.operation.expression.ExpressionFunction</classname>
                function dynamically composes a string expression, using
                incoming Tuple values to replace variables in the
                expression.
              </para>

              <xi:include href="expression-function.xml"/>

              <para>Above, we create a new String value that contains an
                expression containing values from the current Tuple. Note that
                you must declare the type for every input Tuple field so that
                the expression compiler knows how to treat the variables in the
                expression.
              </para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>ExpressionFilter</term>

            <listitem>
              <para>The
                <classname>cascading.operation.expression.ExpressionFilter</classname>
                filter evaluates a Boolean expression, using argument Tuple
                values to replace variables in the expression. If the expression
                returns<code>true</code>, the Tuple is removed from the
                stream.
              </para>

              <xi:include href="expression-filter.xml"/>

              <para>In this example, every line in the Apache log that does
                not have a status of "200" is filtered out. ExpressionFilter
                coerces the value into the specified type if necessary to make
                the comparison - in this case, coercing the status String into
                an<classname>int</classname>.
              </para>
            </listitem>
          </varlistentry>
        </variablelist>
      </para>
    </section>

    <section>
      <title>XML Operations</title>

      <para>To use XML Operations in a Cascading application, include the
        <filename>cascading-xml-x.y.z.jar</filename>
        in the project.
        <?oxy_comment_start author="Jim" timestamp="20120323T163548-0700" comment="Chris - I changed this to say that the TagSoup library only has to be included if using the tagSoupParser operation. Hopefully that is correct."?>
        When
        using the tagSoupParser operation, <?oxy_comment_end ?>this module
        requires the TagSoup library, which provides support for HTML and XML
        "tidying". More information is available at the TagSoup website,<link
          xlink:href="http://home.ccil.org/~cowan/XML/tagsoup/">http://home.ccil.org/~cowan/XML/tagsoup/</link>.
      </para>

      <para>
        <variablelist>
          <varlistentry>
            <term><?oxy_comment_start author="Jim" timestamp="20120323T164425-0700" comment="Chris - please check this to see if it is correct."?>
              XPathParser<?oxy_comment_end ?></term>

            <listitem>
              <para>The
                <classname>cascading.operation.xml.XPathParser</classname>
                function uses one or more XPath expressions, passed into the
                constructor, to extract one or more node values from an XML
                document contained in the passed Tuple argument, and places the
                result(s) into one or more new fields in the current Tuple. In
                this way, it effectively converts an XML document (in a tuple)
                into a table of fields (added to the same tuple), creating one
                Tuple field value for every given XPath expression. The
                <classname>Node</classname>
                is converted to a String type
                containing an XML document. If only the text values are
                required, search on the
                <code>text()</code>
                nodes, or consider
                using XPathGenerator to handle multiple
                <classname>NodeList</classname>
                values. If the returned result
                of an XPath expression is a<classname>NodeList</classname>,
                only the first
                <classname>Node</classname>
                is used for the field
                value and the rest are ignored.
              </para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>XPathGenerator</term>

            <listitem>
              <para>Similar to XPathParser, the
                <classname>cascading.operation.xml.XPathGenerator</classname>
                function outputs a
                new <?oxy_comment_start author="Jim" timestamp="20120323T164856-0700" comment="Please check for accuracy."?>
                Tuple<?oxy_comment_end ?>
                for every
                <classname>Node</classname>
                returned by the given
                XPath expression from the XML in the current Tuple.
              </para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>XPathFilter</term>

            <listitem>
              <para>The
                <classname>cascading.operation.xml.XPathFilter</classname>
                filter removes a Tuple if the specified XPath expression returns
                <code><?oxy_comment_start author="Jim" timestamp="20120325T210244-0700" comment="Does this mean that no node was found at the specified location, or that the value of the node was somehow
                  &quot;false&quot;? If the latter, should we explain how it is false - e.g., the node contains a
                  Boolean &quot;false&quot;, a specific integer value... what if it is a string, a long, etc."?>
                  false<?oxy_comment_end ?></code>.
                Set the removeMatch parameter to
                <code>true</code>
                if the filter
                should be reversed, i.e., to keep only those Tuples where the
                XPath expression returns<code>true</code>.
              </para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>tagSoupParser</term>

            <listitem>
              <para>The
                <classname>cascading.operation.xml.TagSoupParser</classname>
                function uses the TagSoup library to convert incoming HTML to
                clean XHTML. Use the
                <code>setFeature( feature, value )</code>
                method to set TagSoup-specific features, which are documented on
                the TagSoup website.
              </para>
            </listitem>
          </varlistentry>
        </variablelist>
      </para>
    </section>

    <section>
      <title>Assertions</title>

      <para>Cascading Stream Assertions are used to build robust reusable pipe
        assemblies. If desired, they can be planned out of a Flow instance at
        runtime. For more information, see the section on<xref
          linkend="stream-assertions"/>. Below we describe the Assertions
        available in the core library.
      </para>

      <para>
        <variablelist>
          <varlistentry>
            <term>AssertEquals</term>

            <listitem>
              <para>The
                <classname>cascading.operation.assertion.AssertEquals</classname>
                Assertion asserts that the number of values given on the
                constructor is equal to the number of argument Tuple values, and
                that each constructor value
                <code>.equals()</code>
                its
                corresponding argument value.
              </para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>AssertNotEquals</term>

            <listitem>
              <para>The
                <classname>cascading.operation.assertion.AssertNotEquals</classname>
                Assertion asserts that the number of values given on the
                constructor is equal to the number of argument Tuple values and
                that each constructor value is not
                <code>.equals()</code>
                to its
                corresponding argument value.
              </para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>AssertEqualsAll</term>

            <listitem>
              <para>The
                <classname>cascading.operation.assertion.AssertEqualsAll</classname>
                Assertion asserts that every value in the argument Tuple
                <code>.equals()</code>
                the single value given on the
                constructor.
              </para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>AssertExpression</term>

            <listitem>
              <para>The
                <classname>cascading.operation.assertion.AssertExpression</classname>
                Assertion dynamically resolves a given Java expression (see
                <xref linkend="operation-expression"/>) using argument Tuple
                values. Any Tuple that returns
                <code>true</code>
                for the given
                expression passes the assertion.
              </para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>AssertMatches</term>

            <listitem>
              <para>The
                <classname>cascading.operation.assertion.AssertMatches</classname>
                Assertion matches the given regular expression pattern String
                against the entire argument Tuple. The comparison is made
                possible by concatenating all the fields of the Tuple, separated
                by the TAB character (\t). If a match is found, the Tuple passes
                the assertion.
              </para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>AssertMatchesAll</term>

            <listitem>
              <para>The
                <classname>cascading.operation.assertion.AssertMatchesAll</classname>
                Assertion matches the given regular expression pattern String
                against each argument Tuple value individually.
              </para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>AssertNotNull</term>

            <listitem>
              <para>The
                <classname>cascading.operation.assertion.AssertNotNull</classname>
                Assertion asserts that every value in the argument Tuple is not
                a
                <code><?oxy_comment_start author="Jim" timestamp="20120323T185743-0700" comment="Should we say how this is checked and/or whether Cascading treats null, empty, and white space as the same thing? This may be obvious to the target readership in this context, I don
                  &apos;t know."?>null<?oxy_comment_end ?></code>
                value.
              </para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>AssertNull</term>

            <listitem>
              <para>The
                <classname>cascading.operation.assertion.AssertNull</classname>
                Assertion asserts that every value in the argument Tuple is a
                <code><?oxy_comment_start author="Jim" timestamp="20120323T185841-0700" comment="Same note as in AssertNull"?>
                  null<?oxy_comment_end ?></code>
                value.
              </para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>AssertSizeEquals</term>

            <listitem>
              <para>The
                <classname>cascading.operation.assertion.AssertSizeEquals</classname>
                Assertion asserts that the current Tuple in the tuple stream is
                exactly the given size. <?oxy_comment_start author="Jim" timestamp="20120326T202613-0700" comment="I
                &apos;ve added an explanation (to these three assertions) that size() is the number of fields, not a
                byte length."?>Size,
                here, is the number of fields in the Tuple, as returned by
                <code>Tuple#size()</code>. Note that some or all fields may
                contain
                <code>null</code>
                values.<?oxy_comment_end ?>
              </para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>AssertSizeLessThan</term>

            <listitem>
              <para>The
                <classname>cascading.operation.assertion.AssertSizeLessThan</classname>
                Assertion asserts that the current Tuple in the stream has a
                size less than (<code>&lt;</code>) the given size. Size, here,
                is the number of fields in the Tuple, as returned by
                <code>Tuple#size()</code>. Note that some or all fields may
                contain
                <code>null</code>
                values.
              </para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>AssertSizeMoreThan</term>

            <listitem>
              <para>The
                <classname>cascading.operation.assertion.AssertSizeMoreThan</classname>
                Assertion asserts that the current Tuple in the stream has a
                size greater than (<code>&gt;</code>) the given size. Size,
                here, is the number of fields in the Tuple, as returned by
                <code>Tuple#size()</code>. Note that some or all fields may
                contain
                <code>null</code>
                values.
              </para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>AssertGroupSizeEquals</term>

            <listitem>
              <para>The
                <classname>cascading.operation.assertion.AssertGroupSizeEquals</classname>
                Group Assertion asserts that the number of items in the current
                grouping is equal to (<code>==</code>) the given size. If a
                pattern String is given, only grouping keys that match the
                regular expression will have this assertion applied where
                multiple key values are delimited by a TAB character.
              </para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>AssertGroupSizeLessThan</term>

            <listitem>
              <para>The
                <classname>cascading.operation.assertion.AssertGroupSizeEquals</classname>
                Group Assertion asserts that the number of items in the current
                grouping is less than (<code>&lt;</code>) the given size. If a
                pattern String is given, only grouping keys that match the
                regular expression will have this assertion applied where
                multiple key values are delimited by a TAB character.
              </para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>AssertGroupSizeMoreThan</term>

            <listitem>
              <para>The
                <classname>cascading.operation.assertion.AssertGroupSizeEquals</classname>
                Group Assertion asserts that the number of items in the current
                grouping is greater than (<code>&gt;</code>) the given size. If
                a pattern String is given, only grouping keys that match the
                regular expression will have this assertion applied where
                multiple key values are delimited by a TAB character.
              </para>
            </listitem>
          </varlistentry>
        </variablelist>
      </para>
    </section>

    <section>
      <title>Logical Filter Operators</title>

      <para>The logical
        <classname>Filter</classname>
        operators allow you to
        combine multiple filters to run in a single Pipe, instead of chaining
        multiple Pipes together to get the same logical result.
      </para>

      <para>
        <variablelist>
          <varlistentry>
            <term>And</term>

            <listitem>
              <para>The
                <classname>cascading.operation.filter.And</classname>
                <classname>Filter</classname>
                performs a logical "and" on the
                results of the constructor-provided
                <classname>Filter</classname>
                instances. That is, if
                <methodname>Filter#isRemove()</methodname>
                returns
                <code>true</code>
                for all of the given instances, this filter
                returns<code>true</code>.
              </para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>Or</term>

            <listitem>
              <para>The
                <classname>cascading.operation.filter.Or</classname>
                <classname>Filter</classname>
                performs a logical "or" on the
                results of the constructor-provided
                <classname>Filter</classname>
                instances. That is, if
                <methodname>Filter#isRemove()</methodname>
                returns
                <code>true</code>
                for any of the given instances, this filter
                returns<code>true</code>.
              </para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>Not</term>

            <listitem>
              <para>The
                <classname>cascading.operation.filter.Not</classname>
                <classname>Filter</classname>
                performs a logical "not"
                (negation) on the results of the constructor-provided
                <classname>Filter</classname>
                instance. That is, if
                <methodname>Filter#isRemove()</methodname>
                returns
                <code>true</code>
                for the given instance, this filter returns
                <code>false</code>, and if
                <methodname>Filter#isRemove()</methodname>
                returns
                <code>false</code>
                for the given instance, this filter returns
                <code>true</code>.
              </para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>Xor</term>

            <listitem>
              <para>The
                <classname>cascading.operation.filter.Xor</classname>
                <classname>Filter</classname>
                performs a logical "xor"
                (exclusive or) on the results of the constructor-provided
                <classname>Filter</classname>
                instances. Xor can only be applied
                to two instances at a time. It returns
                <code><?oxy_comment_start author="Jim" timestamp="20120324T161338-0700" comment="Chris - The 1.2 document says the opposite of what I
                  &apos;m saying here, and so does the Javadoc - i.e., they say that this will return True if both truth
                  values match. To me that seems the opposite of XOR, so I changed it. Thought I&apos;d better warn you
                  that I changed it."?>true<?oxy_comment_end ?></code>
                if the two instances have different truth values, and
                <code>false</code>
                if they have the same truth value. That is,
                if
                <methodname>Filter#isRemove()</methodname>
                returns
                <code>true</code>
                for both, or returns
                <code>false</code>
                for
                both, this filter returns<code>false</code>; otherwise it
                returns<code>true</code>.
              </para>
            </listitem>
          </varlistentry>
        </variablelist>
      </para>

      <example>
        <title>Combining Filters</title>

        <xi:include href="filter-and.xml"/>
      </example>

      <para>The example above performs a logical "and" on the two filters.
        Both must be satisfied for the data to pass through this one
        Pipe.
      </para>
    </section>
  </chapter>

  <chapter>
    <info>
      <title>Built-in Assemblies</title>
    </info>

    <para>Introductory text here, explaining about built-in assemblies.</para>

    <section>
      <title xreflabel="AggregateBy" xml:id="aggregate-by">AggregateBy</title>

      <para>text here</para>

      <section>
        <title xml:id="AverageBy">AverageBy</title>

        <para>text here</para>
      </section>

      <section>
        <title xml:id="CountBy">CountBy</title>

        <para>text here</para>
      </section>

      <section>
        <title xml:id="SumBy">SumBy</title>

        <para>text here</para>
      </section>
    </section>

    <section>
      <title>Coerce</title>

      <para>text here</para>
    </section>

    <section>
      <title>Discard</title>

      <para>text here</para>
    </section>

    <section>
      <title>Rename</title>

      <para>text here</para>
    </section>

    <section>
      <title>Retain</title>

      <para>text here</para>
    </section>

    <section>
      <title>Unique</title>

      <para>text here</para>
    </section>
  </chapter>

  <chapter>
    <info>
      <title><?oxy_comment_start author="Jim" timestamp="20120402T045152-0700" comment="There are a lot of these in the list, so it would be nice if we can logically group them, which would add a little more value for the reader. And the groupings themselves may suggest some further ideas for best practices. If you want to suggest some categories to me (add a list here, for instance), I can create subsections and move things around. Otherwise I can try to come up with my own categories for these."?>
        Best
        Practices<?oxy_comment_end ?></title>
    </info>

    <section>
      <title>Unit Testing</title>

      <para>Discrete testing of all Operations, pipe assemblies, and
        applications is a must. The
        <classname>cascading.CascadingTestCase</classname>
        provides a number of
        helper methods.
      </para>

      <para>When testing custom Operations, use the
        <methodname>invokeFunction()</methodname>,
        <methodname>invokeFilter()</methodname>,
        <methodname>invokeAggregator()</methodname>, and
        <methodname>invokeBuffer()</methodname>
        methods.
      </para>

      <para>When testing Flows, use the
        <methodname>validateLength()</methodname>
        methods. There are quite a few
        of them, and collectively they offer great flexibility. All of them read
        the sink tap, validate that it is the correct length and has the correct
        Tuple size, and check to see whether the values match a given regular
        expression pattern.
      </para>

      <para>The
        <classname>cascading.ClusterTestCase</classname>
        can be used
        if you want to launch an embedded Hadoop cluster inside your
        TestCase.
      </para>

      <para>To use any of these helper classes, make sure that
        <filename>cascading-test-x.y.z.jar</filename>
        is in your testing class
        path.
      </para>
    </section>

    <section>
      <title>Local versus Distributed Mode</title>

      <para><?oxy_comment_start author="Jim" timestamp="20120326T232235-0700" comment="local v. distributed modes here?"?>
        Chris,
        if we don't explain this in Chapter 4, then maybe this would be a good
        place to talk about local v. distributed modes.<?oxy_comment_end ?></para>
    </section>

    <section>
      <title>Flow Granularity</title>

      <para>Although using one large
        <classname>Flow</classname>
        may result in
        slightly more efficient performance, it's advisable to use a more
        modular and flexible approach, creating smaller Flows with well-defined
        responsibilities, and passing all the resulting interdependent Flows to
        a
        <classname>Cascade</classname>
        to sequence and execute as a single
        unit. Similarly, using the
        <classname>TextDelimited</classname>
        <classname>Scheme</classname>
        between
        <classname>Flow</classname>
        instances allows you to hand off intermediate data to other systems for
        reporting or QA purposes, incurring a minimal performance penalty while
        remaining compatible with other tools.
      </para>
    </section>

    <section>
      <title>SubAssemblies, not Factories</title>

      <para>When developing your applications, use
        <classname>SubAssembly</classname>
        sub-classes, not "factory" methods.
        The resulting code is much easier to read and test.
      </para>

      <para>It's worth noting that the
        <classname>Object</classname>
        constructors are "factories", so there isn't much reason to build
        frameworks to duplicate what a constructor already does. Of course there
        are exceptions, but in practice they are rare when you can use a
        <classname>SubAssembly</classname>.
      </para>
    </section>

    <section>
      <title>Logical Responsibilities for SubAssemblies</title>

      <para>SubAssembies provide a very convenient means to co-locate similar
        or related responsibilities into a single place. For example, it's
        simple to use a
        <classname>ParsingSubAssembly</classname>
        and a
        <classname>RulesSubAssembly</classname>, where the first is responsible
        solely for parsing incoming
        <classname>Tuple</classname>
        streams (log
        files for example), and the second applies rules to decide whether a
        given
        <classname>Tuple</classname>
        should be discarded or marked as
        bad.
      </para>

      <para>Additionally, in your unit tests you can create a
        <classname>TestAssertionsSubAssembly</classname>
        that simply inlines
        various
        <classname>ValueAssertions</classname>
        and
        <classname>GroupAssertions</classname>. The practice of inlining
        Assertions directly in your SubAssemblies is also important, but
        sometimes it makes sense to have more tests outside of the business
        logic.
      </para>
    </section>

    <section>
      <title>Java Operators in Field Names</title>

      <para>There are a number of Operations in Cascading (e.g.,
        <classname>ExpressionFunction</classname>
        and
        <classname>ExpressionFilter</classname>) that compile and apply Java
        expressions on the fly. In these expressions, Operation argument field
        names are used as variable names in the expression. For this reason,
        take care to create field names that don't contain characters which will
        cause compilation errors if they are used in an expression. For example,
        "first-name" is a valid field name for use with Cascading, but might
        result in the expression<code>first-name.trim()</code>, which will
        cause a compilation error.
      </para>
    </section>

    <section>
      <title>Debugging Planner Failures</title>

      <para>The
        <classname>FlowConnector</classname>
        will sometimes fail when
        attempting to plan a<classname>Flow</classname>. If the error message
        given by
        <classname>PlannerException</classname>
        is vague, use the
        method
        <code>PlannerException.writeDOT()</code>
        to export a
        representation of the internal pipe assembly. DOT files can be opened by
        GraphViz and OmniGraffle. These plans are only partial, but you will be
        able to see where the Cascading planner failed.
      </para>

      <para>Note that you can also create a DOT file from a
        <classname>Flow</classname>, by using
        <code>Flow.writeDOT()</code>.
      </para>
    </section>

    <section>
      <title>Optimizing Joins</title>

      <para>When joining two streams via a CoGroup Pipe, try to put the
        largest of the streams in the leftmost argument to the CoGroup. The
        reason for this is that joining multiple streams requires some
        accumulation of values before the join operator can begin, but the
        leftmost stream is not accumulated, so this technique should improve the
        performance of most joins.
      </para>
    </section>

    <section>
      <title>Debugging Streams</title>

      <para>When creating complex assemblies, it's safe to embed
        <classname>Debug</classname>
        operations (see<xref
          linkend="debug-function"/>) at appropriate debug levels as needed. Use
        the planner to remove them at runtime for production and staging runs,
        to avoid wasting resources.
      </para>
    </section>

    <section>
      <title xreflabel="Handling Good and Bad Data" xml:id="handling-bad-data"
             xml:lang="">Handling Good and Bad Data
      </title>

      <para>It's very common when processing raw data streams to encounter
        data that is corrupt or malformed in some way. For instance, bad content
        may be fetched from the web via a crawler upstream, or a bug may have
        leaked corrupt data into a browser widget somewhere that sends user
        behavior information back for analysis. Whatever the cause, it's a good
        practice to define a set of rules for identifying and discarding
        questionable records.
      </para>

      <para>It is tempting to simply throw an exception and have a Trap
        capture the offending<classname>Tuple</classname>, but Traps were not
        designed as a filtering mechanism, and consequently much valuable
        information would be lost.
      </para>

      <para>Instead of traps, use filters. Create a
        <classname>SubAssembly</classname>
        that applies rules to the stream by
        setting a binary field that marks the tuple as good or bad. After all
        the rules are applied, split the stream based on the value of the good
        or bad Boolean value. Consider setting a reason field that states why
        the Tuple was marked bad.
      </para>
    </section>

    <section>
      <title>Maintaining State in Operations</title>

      <para>When creating custom Operations
        (<classname>Function</classname>,<classname>
          Filter</classname>,<classname>Aggregator</classname>, or
        <classname>Buffer</classname>) do not store operation state in class
        fields. For example, if implementing a custom "counter"
        <classname>Aggregator</classname>, do not create a field named "count"
        and increment it on every
        <methodname>Aggregator.aggregate()</methodname>
        call. There is no
        guarantee your Operation will be called from a single thread in a JVM,
        future version of Hadoop could execute the same operation from multiple
        threads.
      </para>

      <para>To maintain state across
        <classname>Operation</classname>
        calls,
        create and initialize a "context" object that is maintained by the
        appropriate
        <classname>OperationCall</classname>
        (<classname>FilterCall</classname>,<classname>FunctionCall</classname>,
        <classname>AggregatorCall</classname>, and
        <classname>BufferCall</classname>). In the example above, store an
        Integer 0 in the
        <classname>AggregatorCall</classname>
        passed to the
        <methodname>Aggregator.start()</methodname>
        method and increment it in
        the
        <methodname>Aggregator.aggregate()</methodname>
        method.
      </para>
    </section>

    <section>
      <title>Custom Types</title>

      <para>It is generally frowned upon to pass a custom class through a
        Tuple stream. One one hand this increases coupling of custom Operations
        to a particular type, and it removes opportunities for reducing the
        amount of data that passes over the network (or is
        serialized/deserialized).
      </para>

      <para>To overcome the first objection, with every custom type with
        multiple instance fields, attempt to provide Functions that can promote
        a value from the custom object to a position in a Tuple or demote the
        Tuple value to a particular field back into the custom type. This allows
        existing operations (like ExpressionFunction or RegexFilter) to operate
        on values owned by a custom type. For example, if you have a Person
        object, have a Function named GetPersonAge that takes Person as an
        argument and only returns the age as the result. The next operation can
        then Filter all Persons based on their age. This may seem like more work
        and less effiicient, but it keeps your application flexible and reduces
        the am ount of duplicate code (the only alternative here is to create a
        PersonAgeFilter which results in one more thing to test).
      </para>
    </section>

    <section>
      <title>Fields Constants</title>

      <para>Instead of having String field names strewn about, create an
        Interface that holds a constant value for each field name:
      </para>

      <para>
        <code>public static Fields FIRST_NAME = new Fields( "firstname" );
        </code>
      </para>

      <para>Using the Fields class, instead of String, allows for building
        more complex constants:
      </para>

      <para>
        <code>public static Fields NAME = FIRST_NAME.append( LAST_NAME );
        </code>
      </para>
    </section>

    <section>
      <title>Checking the Source Code</title>

      <para>When in doubt, look at the Cascading source code. If something is
        not documented in this User Guide or Javadoc, and it's a feature of
        Cascading, the source code will give you clear instructions on what to
        do or expect.
      </para>
    </section>
  </chapter>

  <chapter>
    <info>
      <title xml:id="extending-cascading">Extending Cascading</title>
    </info>

    <para>Possible introductory text here.</para>

    <section>
      <title>Scripting</title>

      <para>The Cascading API was designed with scripting in mind. Any
        Java-compatible scripting language can import and instantiate Cascading
        classes, create pipe assemblies and flows, and execute those flows. And
        if the scripting language in question supports Domain Specific Language
        (DSL) creation, users can create their own DSL's to handle common
        idioms.
      </para>

      <para>The Cascading website includes information on scripting language
        bindings that are publicly available.
      </para>
    </section>

    <section>
      <title xreflabel="Custom Types"
             xml:id="custom-types"><?oxy_comment_start author="Jim" timestamp="20120327T003300-0700" comment="Chris, I think you said you needed to add some information to this section to cover some major changes you
        &apos;ve made...?"?>Custom
        taps and schemes<?oxy_comment_end ?></title>

      <para>Cascading is designed to be easily configured and enhanced by
        developers. In addition to creating custom Operations, developers can
        create custom
        <classname>tap</classname>
        and
        <classname>Scheme</classname>
        types that let applications connect to
        systems external to Hadoop.
      </para>

      <para>A tap represents something physical, like a file or a database
        table. Consequently, tap implementations are responsible for life-cycle
        issues around the resource they represent, such as tests for existence,
        or eventual deletion.
      </para>

      <para>A scheme represents a format or representation - such as a text
        format for a file, the columns in a table, etc. Schemes are used to
        convert between the source data's native format and a
        <classname>cascading.tuple.Tuple</classname>
        instance.
      </para>

      <para>Creating custom taps and schemes can be an involved process, and
        requires some knowledge of Hadoop and the Hadoop FileSystem API. If a
        flow needs to support a new file system, passing a fully-qualified URL
        to the
        <classname>Hfs</classname>
        constructor may be sufficient - the
        <classname>Hfs</classname>
        tap will look up a file system based on the
        URL scheme via the Hadoop FileSystem API. If not, a new system is
        commonly constructed by subclassing the
        <classname>cascading.tap.Hfs</classname>
        class.
      </para>

      <para>Delegating to the Hadoop FileSystem API is not a strict
        requirement. But if not using it, the developer must implement a Hadoop
        <classname>org.apache.hadoop.mapred.InputFormat</classname>
        and/or
        <classname>org.apache.hadoop.mapred.OutputFormat</classname>
        so that
        Hadoop knows how to split and handle the incoming/outgoing data. The
        custom
        <classname>Scheme</classname>
        is responsible for setting the
        <classname>InputFormat</classname>
        and
        <classname>OutputFormat</classname>
        on the
        <classname>JobConf</classname>, via the
        <methodname>sinkInit</methodname>
        and
        <methodname>sourceInit</methodname>
        methods.
      </para>

      <para>For examples of how to implement a custom tap and scheme, see the
        <link xlink:href="???">Cascading Modules</link>
        page.
      </para>
    </section>

    <section>
      <title>Custom Types and Serialization</title>

      <para>The
        <classname>Tuple</classname>
        class is a generic container for
        all
        <classname>java.lang.Object</classname>
        instances. (Cascading 1.0
        required that all objects be of type
        <classname>java.lang.Comparable</classname>.) Consequently, any
        primitive value or custom Class can be stored in a
        <classname>Tuple</classname>
        instance, that is, returned by a
        <classname>Function</classname>,<classname>Aggregator</classname>, or
        <classname>Buffer</classname>
        as a result value.
      </para>

      <para>But for this to work, any Class that isn't a primitive value or a
        Hadoop
        <classname>Writable</classname>
        type requires a corresponding
        Hadoop serialization class, registered in the Hadoop configuration
        files, for your cluster. Hadoop
        <classname>Writable</classname>
        types
        work because there is already a generic serialization implementation
        built into Hadoop. See the Hadoop documentation for information on
        registering a new serialization helper or creating
        <classname>Writable</classname>
        types. Cascading automatically inherits
        serialization implementations if they are registered.
      </para>

      <para>During serialization and deserialization of
        <classname>Tuple</classname>
        instances that contain custom types, the
        Cascading
        <classname>Tuple</classname>
        serialization framework must
        store the class name (as a<classname>String</classname>) before
        serializing the custom object. This can be very space inefficient. To
        overcome this, custom types can add the
        <classname>SerializationToken</classname>
        Java annotation to the custom
        type class. The
        <classname>SerializationToken</classname>
        annotation
        expects two arrays - one of integers that are used as tokens, and one of
        Class name strings. Both arrays must be the same size. The integer
        tokens must all have values of 128 or greater, since the first 128
        values are reserved for internal use.
      </para>

      <para>During serialization and deserialization, the token values are
        used instead of the
        <classname>String</classname>
        Class names, in order
        to reduce the amount of storage used.
      </para>

      <para>Serialization tokens may also be stored in the Hadoop config files
        or set as a property passed to the<classname>FlowConnector</classname>,
        with the property name<code>cascading.serialization.tokens</code>. The
        value of this property is a comma -eparated list of
        <code>token=classname</code>
        values.
      </para>

      <para>Note that Cascading natively serializes/deserializes all
        primitives and byte arrays (<code>byte[]</code>). It uses the token 127
        for the Hadoop
        <classname>BytesWritable</classname>
        class.
      </para>

      <para>Along with custom serialization, Cascading supports lazy
        deserialization during Tuple comparison, when Hadoop sorts keys during
        the "shuffle" phase. This is accomplished by implementing the
        <classname>StreamComparator</classname>
        interface. See the Javadoc for
        detailed instructions on implemention, and the unit tests for
        examples.
      </para>

      <para>By default, Cascading lazily deserializes each element in the
        Tuple when sorting for comparison. It's also possible to do lazy
        deserialization of fields when comparing complex or custom Java types;
        this is supported in the
        <classname>StreamComparator</classname>
        interface.
      </para>
    </section>
  </chapter>

  <chapter>
    <info>
      <title>CookBook</title>
    </info>

    <para>This chapter discusses some common idioms used in Cascading
      applications.
    </para>

    <section>
      <title>Tuples and Fields</title>

      <para>
        <variablelist>
          <varlistentry>
            <term>Copy a Tuple instance</term>

            <listitem>
              <xi:include href="cookbook-copy.xml"/>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>Nest a Tuple instance within a Tuple</term>

            <listitem>
              <xi:include href="cookbook-nest.xml"/>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>Build a longer Fields instance</term>

            <listitem>
              <xi:include href="cookbook-fieldsappend.xml"/>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>Remove a field from a longer Fields instance</term>

            <listitem>
              <xi:include href="cookbook-fieldssubtract.xml"/>
            </listitem>
          </varlistentry>
        </variablelist>
      </para>
    </section>

    <section>
      <title>Stream Shaping</title>

      <para>
        <variablelist>
          <varlistentry>
            <term>Split (branch) a Tuple Stream</term>

            <listitem>
              <xi:include href="cookbook-split.xml"/>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>Copy a field value</term>

            <listitem>
              <xi:include href="cookbook-copyfield.xml"/>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>Discard (drop) a field</term>

            <listitem>
              <xi:include href="cookbook-discardfield.xml"/>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>Rename a field</term>

            <listitem>
              <xi:include href="cookbook-renamefield.xml"/>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>Coerce field values from Strings to primitives</term>

            <listitem>
              <xi:include href="cookbook-coercefields.xml"/>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>Insert constant values into a stream</term>

            <listitem>
              <xi:include href="cookbook-insertvalue.xml"/>
            </listitem>
          </varlistentry>
        </variablelist>
      </para>
    </section>

    <section>
      <title>Common Operations</title>

      <para>
        <variablelist>
          <varlistentry>
            <term>Parse a String date/time value</term>

            <listitem>
              <xi:include href="cookbook-parsedate.xml"/>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>Format a time-stamp to a date/time value</term>

            <listitem>
              <xi:include href="cookbook-formatdate.xml"/>
            </listitem>
          </varlistentry>
        </variablelist>
      </para>
    </section>

    <section>
      <title>Stream Ordering</title>

      <para>
        <variablelist>
          <varlistentry>
            <term>Remove duplicate tuples in a stream</term>

            <listitem>
              <xi:include href="cookbook-distinctgroup.xml"/>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>Create a list of unique values</term>

            <listitem>
              <xi:include href="cookbook-distinctvalue.xml"/>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>Find first occurrence in time of a unique value</term>

            <listitem>
              <xi:include href="cookbook-distinctorder.xml"/>
            </listitem>
          </varlistentry>
        </variablelist>
      </para>
    </section>

    <section>
      <title>API Usage</title>

      <para>
        <variablelist>
          <varlistentry>
            <term>Pass properties to a custom Operation</term>

            <listitem>
              <xi:include href="cookbook-passproperties.xml"/>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>Bind multiple sources and sinks to a Flow</term>

            <listitem>
              <xi:include href="cookbook-sourcessinks.xml"/>
            </listitem>
          </varlistentry>
        </variablelist>
      </para>
    </section>
  </chapter>

  <chapter>
    <info>
      <title>How It Works</title>
    </info>

    <section>
      <title xreflabel="MapReduce Job Planner" xml:id="job-planner">MapReduce
        Job Planner
      </title>

      <para>The MapReduce Job Planner is an internal feature of
        Cascading.
      </para>

      <para>When a collection of functions, splits, and joins are all tied up
        together into a "pipe assembly", the FlowConnector object is used to
        create a new Flow instance against input and output data paths. This
        Flow is a single Cascading job.
      </para>

      <para>Internally, the FlowConnector employs an intelligent planner to
        convert the pipe assembly to a graph of dependent MapReduce jobs that
        can be executed on a Hadoop cluster.
      </para>

      <para>All this happens behind the scenes - as does the scheduling of the
        individual MapReduce jobs, as well as the cleanup of intermediate data
        sets that bind the jobs together.
      </para>

      <para>
        <inlinemediaobject>
          <imageobject role="fo">
            <imagedata align="center" contentwidth="5.5in"
                       fileref="images/planned-flow.svg"/>
          </imageobject>

          <imageobject role="html">
            <imagedata align="center" contentwidth="5.5in"
                       fileref="images/planned-flow.png"/>
          </imageobject>
        </inlinemediaobject>
      </para>

      <para>The diagram above shows how a typical Flow is partitioned into
        MapReduce jobs. Every job is delimited by a temporary file that serves
        as the sink from the first job and the source for the next.
      </para>

      <para>To create a visualization of how your Flows are partitioned, call
        the
        <classname>Flow#writeDOT()</classname>
        method. This writes a
        <link
          xlink:href="http://en.wikipedia.org/wiki/DOT_language">DOT
        </link>
        file
        out to the path specified, which can be viewed in a graphics package
        like OmniGraffle or Graphviz.
      </para>
    </section>

    <section>
      <title xreflabel="Topological Scheduling" xml:id="cascade-scheduler"
             xml:lang="">The Cascade Topological Scheduler
      </title>

      <para>Cascading has a simple class,
        <classname>Cascade</classname>
        ,
        that executes a collection of Cascading Flows on a target cluster in
        dependency order.
      </para>

      <para>Consider the following example.</para>

      <itemizedlist>
        <listitem>
          <para>Flow 1 reads input file A and outputs B.</para>
        </listitem>

        <listitem>
          <para>Flow 2 expects input B and outputs C and D.</para>
        </listitem>

        <listitem>
          <para>Flow 3 expects input C and outputs E.</para>
        </listitem>
      </itemizedlist>

      <para>A
        <classname>Cascade</classname>
        is constructed through the
        <classname>CascadeConnector</classname>
        class, by building an internal
        graph that makes each Flow a "vertex", and each file an "edge". A
        topological walk on this graph will touch each vertex in order of its
        dependencies. When a vertex has all its incoming edges (i.e., files)
        available, it is scheduled on the cluster.
      </para>

      <para>In the example above, Flow 1 goes first, Flow 2 goes second, and
        Flow 3 is last.
      </para>

      <para>If two or more Flows are independent of one another, they are
        scheduled concurrently.
      </para>

      <para>And by default, if any outputs from a Flow are newer than the
        inputs, the Flow is skipped. The assumption is that the Flow was
        executed recently, since the output isn't stale. So there is no reason
        to re-execute it and use up resources or add time to the job. This is
        similar behavior a compiler would exhibit if a source file wasn't
        updated before a recompile.
      </para>

      <para>This is very handy if you have a large set of jobs, with varying
        interdependencies between them, that needs to be executed as a logical
        unit. Just pass them to the CascadeConnector and let it sort them all
        out.
      </para>
    </section>

    <section>
      <title>Processing Mode</title>

      <para>This section is probably unnecessary but I'm throwing it in just
        in case. Local v. distributed mode are mentioned in the Hadoop section
        of the introduction, described in some detail in the Execution section,
        and discussed pro/con in the Best Practices section, just after
        Testing.
      </para>
    </section>
  </chapter>
</book>
