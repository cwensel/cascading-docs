<?xml version="1.0" encoding="UTF-8"?>
<book version="5.0" xml:lang="en" xmlns="http://docbook.org/ns/docbook"
      xmlns:xlink="http://www.w3.org/1999/xlink"
      xmlns:xi="http://www.w3.org/2001/XInclude"
      xmlns:ns5="http://www.w3.org/1998/Math/MathML"
      xmlns:ns4="http://www.w3.org/2000/svg"
      xmlns:ns3="http://www.w3.org/1999/xhtml"
      xmlns:ns="http://docbook.org/ns/docbook">
  <info>
    <title>Cascading - User Guide</title>

    <pubdate>December, 2008</pubdate>

    <copyright>
      <year>2007</year>

      <year>2008</year>

      <holder>Concurrent, Inc</holder>
    </copyright>

    <releaseinfo>V 1.0-preview</releaseinfo>

    <productname>Cascading</productname>

    <authorgroup>
      <author>
        <orgname>Concurrent, Inc</orgname>
      </author>
    </authorgroup>

    <mediaobject>
      <imageobject role="fo">
        <imagedata contentwidth="1in" fileref="images/cascading-logo.svg"></imagedata>
      </imageobject>

      <imageobject role="html">
        <imagedata contentwidth="1in" fileref="images/cascading-logo.png"></imagedata>
      </imageobject>
    </mediaobject>
  </info>

  <toc></toc>

  <chapter>
    <info>
      <title>Cascading</title>
    </info>

    <section>
      <title>What is Cascading?</title>

      <para>Cascading is an API for defining, sharing, and executing data
      processing workflows on a distributed data grid or cluster.</para>

      <para>Cascading relies on Apache Hadoop. To use Cascading, Hadoop must
      be installed locally for development and testing, and a Hadoop cluster
      must be deployed for production applications.</para>

      <para>Cascading greating simplifies the complexities with Hadoop
      application development, job creation, and job scheduling.</para>
    </section>

    <section>
      <title>Who should use Cascading?</title>

      <para>Cascading was developed to allow organizations to rapidly develop
      complex data processing applications. These applications come in two
      extremes.</para>

      <para>On one hand, there is too much data for a single computing system
      to manage effecitively. Developers have decided to adopt Apache Hadoop
      as the base computing infrastructure, but realize that developing
      reasonably useful applications on Hadoop is not trivial. Cascading eases
      the burden on developers by allowing them to rapidly create, refactor,
      test, and execute complex applications that scale linearly across a
      cluster of computers.</para>

      <para>On the other hand, managers and developers realize the complexity
      of the processes in their datacenter is getting out of hand with one-off
      data-processing applications living wherever there is enough disk space.
      Subsequently they have decided to adopt Apache Hadoop to gain access to
      its "Global Namespace" filesystem which allows for a single reliable
      storage framework. Cascading eases the learning curve for developers to
      convert their existing applications for execution on a Hadoop cluster.
      It further allows for developers to create reusable libraries and
      application for use by analysts who need to extract data from the Hadoop
      filesystem.</para>

      <para>Cascading was designed to support three user roles. The
      application Executor, process Assembler, and the operation
      Developer.</para>

      <para>The application Executor is someone, a developer or analyst, or
      some system (like a cron job) which runs a data processing application
      on a given cluster. This is typically done via the command line using a
      pre-packaged Java Jar file compiled against the Apache Hadoop and
      Cascading libraries. This application may accept parameters to customize
      it for an given execution and generally results in a set of data the
      user will export from the Hadoop filesystem for some specific
      purpose.</para>

      <para>The process Assembler is someone who assembles data processing
      workflows into unique applications. This is generally a development task
      of chaining together operations that act on input data sets to produce
      one or more output data sets. This task can be done using the raw Java
      Cascading API or via a scripting language like Groovy, JRuby, or
      Jython.</para>

      <para>The operation Developer is someone who writes individual functions
      or operations, typically in Java, or reusable sub-assemblies that act on
      the data that pass through the data processing workflow. A simple
      example would be a parser that takes a string and converts it to an
      Integer. Operations are equivalent to Java functions in the sense that
      they take input arguments and return data. And they can execute at any
      granularity, simply parsing a string, or performing some complex routine
      on the argument data using third-party libraries.</para>

      <para>All three roles can be a developer, but the API allows for a clean
      separation of responsibilities for larger organizations that need
      non-developers to run ad-hoc applications or build production processes
      on a Hadoop cluster.</para>
    </section>

    <section>
      <title>What is Apache Hadoop</title>

      <para>From the Hadoop website, it <quote>is a software platform that
      lets one easily write and run applications that process vast amounts of
      data</quote>.</para>

      <para>To be a little more specific, Hadoop provides a storage layer that
      holds vast amounts of data, and an execution layer for running an
      application in parallel across the cluster against parts of the stored
      data.</para>

      <para>The storage layer, the Hadoop File System (HDFS), looks like a
      single storage volume that has been optimized for many concurrent
      serialized reads of large data files. Where "large" ranges from
      Gigabytes to Petabytes. But it only supports a single writer. Thus
      random access to the data is not really possible in an efficient manner.
      But this is why it is so performant and reliable. Reliable in part
      because this restriction allows for the data to be replicated across the
      cluster reducing the chance of data loss.</para>

      <para>The execution layer relies on a "divide and conquer" strategy
      called MapReduce. MapReduce is beyond the scope of this document, but
      suffice it to say, it can be so difficult to develop "real world"
      applications against that Cascading was created to offset the
      complexity.</para>

      <para>Apache Hadoop is an Open Source Apache project and is freely
      available. It can be downloaded from here the Hadoop website, <link
      xlink:href="http://hadoop.apache.org/core/">http://hadoop.apache.org/core/</link>.</para>
    </section>
  </chapter>

  <chapter>
    <info>
      <title>Diving In</title>
    </info>

    <para>Counting words in a document is the most common example presented to
    new Hadoop (and MapReduce) developers, it is the Hadoop equivalent to the
    "Hello World" application.</para>

    <para>Word counting is where a document is parsed into individual words,
    and the frequency of those words are counted.</para>

    <para>For example, if we counted the last paragraph "is" would be counted
    twice, and "document" counted once.</para>

    <para>In the code example below, we will use Cascading to read each line
    of text from a file (our document), parse it into words, then count the
    number of time the word is encountered.</para>

    <example>
      <title>Word Counting</title>

      <xi:include href="basic-word-count.xml" />
    </example>

    <para>There are a couple things to take away from this example.</para>

    <para>First, the pipe assembly is not coupled to the data (the Tap
    instances) until the last momement before execution. That is, file paths
    or references are not embedded in the pipe assembly. The pipe assembly
    remains independent of <emphasis>which</emphasis> data it processes until
    execution. The only dependency is <emphasis>what</emphasis> the data looks
    like, its "scheme", or the field names that make it up.</para>

    <para>That brings up fields. Every input and output file has field names
    associated with it, and every processing element of the pipe assembly
    either expects certain fields, or creates new fields. This allows the
    developer to self document their code, and allows the Cascading planner to
    "fail fast" during planning if a dependency between elements isn't
    satisfied (used a missing or wrong field name).</para>

    <para>It is also important to point out that pipe assemblies are
    assemblied through constructor chaining. This may seem odd but is done for
    two reasons. It keeps the code more concise. And it prevents developers
    from creating "cycles" in the resulting pipe assembly. Pipe assemblies are
    Directed Acyclic Graphs (or DAGs). The Cascading planner cannot handle
    processes that feed themselves, that have cycles (not to say there are
    ways around this that are much safer).</para>

    <para>Notice the very first <code>Pipe</code> instance has a name. That
    instance is the "head" of this particular pipe assembly. Pipe assemblies
    can have any number of heads, and any number of tails. This example does
    not name the tail assembly, but for complex asssemblies tails must be
    named for reasons described below.</para>

    <para>Heads and tails of pipe assemblies generally need names, this is how
    sources and sinks are "bound" to them during planning. In our example
    above, there is only one head and one tail, and subsequently only one
    source and one sink, respectively. So naming in this case is optional,
    it's obvious what goes where. Naming is also useful for self documenting
    pipe assemblies, especially where there are splits, joins, and merges in
    the assembly.</para>

    <para>To paraphrase, our example will:</para>

    <itemizedlist>
      <listitem>
        <para>read each line of text from a file and give it the field name
        "line",</para>
      </listitem>

      <listitem>
        <para>parse each "line" into words by the <code>RegexGenerator</code>
        object which in turn returns each word in the field named
        "word",</para>
      </listitem>

      <listitem>
        <para>groups on the field named "word" using the <code>GroupBy</code>
        object,</para>
      </listitem>

      <listitem>
        <para>then counts the number of elements in each grouping using the
        <code>Count()</code> object and stores this value in the "count"
        field,</para>
      </listitem>

      <listitem>
        <para>finally the "word" and "count" fields are written out.</para>
      </listitem>
    </itemizedlist>
  </chapter>

  <chapter>
    <info>
      <title>Data Processing</title>
    </info>

    <section>
      <title>Introduction</title>

      <para>The Cascading processing model is based on a "pipes and filters"
      metaphor. The developer uses the Cascading API to assemble pipelines
      that split, merge, group, or join streams of data while applying
      operations to each data record or groups of records.</para>

      <para>In Cascading, we call a data record a Tuple, a pipeline a pipe
      assembly, and a series of Tuples passing through a pipe assembly is
      called a a tuple stream.</para>

      <para>Pipe assemblies are assemblied independently from what data they
      will process. Before a pipe assembly can be executed, it must be bound
      to data sources and data sinks, called Taps. The process of binding pipe
      assemblies to sources and sinks is called planning, and the result of a
      planner is a Flow. Flows can be executed on a data cluster.</para>

      <para>Finally, many Flows can be grouped together and executed as a
      single job. If one Flow depends on the output of another Flow, it will
      not be executed until all its data dependencies are satisfied. This
      collection of Flows is called a Cascade.</para>
    </section>

    <section>
      <title>Pipe Assemblies</title>

      <para>Pipe assemblies define what work should be done against a tuple
      stream, where during runtime tuple streams are read from Tap sources and
      are written to Tap sinks. Pipe assemblies may have multiple sources and
      multiple sinks and they can define splits, merges, and joins to
      manipulate how the tuple streams interact.</para>

      <mediaobject>
        <imageobject role="fo">
          <imagedata align="center" contentwidth="3in"
                     fileref="images/pipes.svg"></imagedata>
        </imageobject>

        <imageobject role="html">
          <imagedata align="center" contentwidth="3in"
                     fileref="images/pipes.png"></imagedata>
        </imageobject>
      </mediaobject>

      <para>There are only five Pipe types: Pipe, Each, GroupBy, CoGroup,
      Every, and SubAssembly.</para>

      <variablelist>
        <varlistentry>
          <term>Pipe</term>

          <listitem>
            <para>The <classname>cascading.pipe.Pipe</classname> class is used
            to name branches of pipe assemblies. These names are used during
            planning to bind Taps as either sources or sinks (or as traps, an
            advanced topic). It is also the base class for all other pipes
            described below.</para>
          </listitem>
        </varlistentry>

        <varlistentry>
          <term>Each</term>

          <listitem>
            <para>The <classname>cascading.pipe.Each</classname> pipe applies
            a Function or Filter Operation to each Tuple that passes through
            it. The results of the Function are appended, by default, to the
            <classname>Each</classname> input Tuple and returned to the next
            <classname>Pipe</classname>.</para>
          </listitem>
        </varlistentry>

        <varlistentry>
          <term>GroupBy</term>

          <listitem>
            <para><classname>cascading.pipe.GroupBy</classname> manages one
            input Tuple stream and does exactly as it sounds, that is, groups
            the stream on selected fields in the tuple stream.
            <classname>GroupBy</classname> also allows for "merging" of two or
            more tuple stream that share the same field names.</para>
          </listitem>
        </varlistentry>

        <varlistentry>
          <term>CoGroup</term>

          <listitem>
            <para><classname>cascading.pipe.CoGroup</classname> allows for
            "joins" on a common set of values, just like a SQL join. The
            output tuple stream of <classname>CoGroup</classname> is the
            joined input tuple streams, where a join can be an Inner, Outer,
            Left, or Right join.</para>
          </listitem>
        </varlistentry>

        <varlistentry>
          <term>Every</term>

          <listitem>
            <para>The <classname>cascading.pipe.Every</classname> pipe applies
            an Aggregator (like count, or sum) or Buffer (a sliding window)
            Operation to every group of Tuples that pass through it.</para>
          </listitem>
        </varlistentry>

        <varlistentry>
          <term>SubAssembly</term>

          <listitem>
            <para>The <classname>cascading.pipe.SubAssembly</classname> pipe
            allows for nesting reusable pipe assemblies into a Pipe class for
            inclusion in a larger pipe assembly. See the section on <xref
            linkend="subassemblies" />.</para>
          </listitem>
        </varlistentry>
      </variablelist>

      <section>
        <title>Assembling Pipe Assemblies</title>

        <para>Pipe assemblies are created by chaining
        <classname>cascading.pipe.Pipe</classname> classes and
        <classname>Pipe</classname> subclasses together. Chaining is
        accomplished by passing previous <classname>Pipe</classname> instances
        to the constructor of the next <classname>Pipe</classname>
        instance.</para>

        <example>
          <title>Chaining Pipes</title>

          <xi:include href="simple-pipe-assembly.xml" />
        </example>

        <para>The above example, if visualized, would look like the diagram
        below.</para>

        <mediaobject>
          <imageobject role="fo">
            <imagedata align="center" contentwidth="5in"
                       fileref="images/simple-pipe-assembly.svg"></imagedata>
          </imageobject>

          <imageobject role="html">
            <imagedata align="center" contentwidth="5in"
                       fileref="images/simple-pipe-assembly.png"></imagedata>
          </imageobject>
        </mediaobject>

        <para>Besides defining the paths tuple streams take through splits,
        merges, grouping, and joining, pipe assemblies transform or filter the
        stored values in each Tuple. This is accomplished by applying an
        Operation to each Tuple or group of Tuples as the tuple stream passes
        through the pipe assembly. To do that, the values in the Tuple
        typically are given field names, in the same fashion columns are named
        in a database so that they can be referenced or selected.</para>

        <variablelist>
          <varlistentry>
            <term>Operation</term>

            <listitem>
              <para>Operations
              (<classname>cascading.operation.Operation</classname>) accept an
              input argument Tuple, and output zero or more result Tuples.
              There are a few sub-types of operations defined below. Cascading
              has a number of generic Operations that can be reused, or
              developers can create their own custom Operations.</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>Tuple</term>

            <listitem>
              <para>In Cascading, we call each record of data a Tuple
              (<classname>cascading.tuple.Tuple</classname>), and a series of
              Tuples are a tuple stream. Think of a Tuple as an Array of
              values where each value can be any
              <classname>java.lang.Comparable</classname> Java type.</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>Fields</term>

            <listitem>
              <para>Fields (<classname>cascading.tuple.Fields</classname>)
              either declare the field names in a Tuple. Or reference values
              in a Tuple as a selector. Fields can either be string names
              ("first_name"), integer positions (<code>-1</code> for the last
              value), or a substitution ( <code>Fields.ALL</code> to select
              all values in the Tuple, like an asterisk (<code>*</code>) in
              SQL).</para>
            </listitem>
          </varlistentry>
        </variablelist>
      </section>

      <section>
        <title>Each and Every Pipes</title>

        <para>The Each and Every pipe types are the only pipes that can be
        used to apply Operations to the tuple stream.</para>

        <para>The Each pipe applies an Operation to "each" Tuple as it passes
        through the pipe assembly. The Every pipe applies an Operation to
        "every" group of Tuples as they pass through the pipe assembly, on the
        tail end of a GroupBy or CoGroup pipe.</para>

        <para><programlisting>new Each( previousPipe, argumentSelector, operation, outputSelector )</programlisting></para>

        <para><programlisting>new Every( previousPipe, argumentSelector, operation, outputSelector )</programlisting></para>

        <para>Both the Each and Every pipe take a Pipe instance, an argument
        selector, Operation instance, and a output selector on the
        constructor. Where each selector is a Fields instance.</para>

        <para>The Each pipe may only apply Functions and Filters to the tuple
        stream as these operations may only operate on one Tuple at a
        time.</para>

        <para>The Every pipe may only apply Aggregators and Buffers to the
        tuple stream as these operations may only operate on groups of tuples,
        one grouping at a time.</para>

        <mediaobject>
          <imageobject role="fo">
            <imagedata align="center" contentwidth="6in"
                       fileref="images/pipe-operation-relationship.svg"></imagedata>
          </imageobject>

          <imageobject role="html">
            <imagedata align="center" contentwidth="6in"
                       fileref="images/pipe-operation-relationship.png"></imagedata>
          </imageobject>
        </mediaobject>

        <para>The argument selector selects values from the input Tuple to be
        passed to the Operation as argument values. The output selector
        selects the output Tuple from an "appended" version of the input Tuple
        and the Operation result Tuple. The output Tuple becomes the input
        Tuple to the next pipe in the pipe assembly.</para>

        <mediaobject>
          <imageobject role="fo">
            <imagedata align="center" contentwidth="6in"
                       fileref="images/each-operation-relationship.svg"></imagedata>
          </imageobject>

          <imageobject role="html">
            <imagedata align="center" contentwidth="6in"
                       fileref="images/each-operation-relationship.png"></imagedata>
          </imageobject>
        </mediaobject>

        <para>If the argument selector is not given, the whole input Tuple
        (<code>Fields.ALL</code>) is passed to the Operation as argument
        values. If the result selector is not given on an Each pipe, the
        Operation results are returned by default
        (<code>Fields.RESULTS</code>), replacing the input Tuple values in the
        tuple stream. This really only applies to Functions as Filters either
        discard the input Tuple, or return the input Tuple intact. There is no
        opportunity to provide an output selector.</para>

        <mediaobject>
          <imageobject role="fo">
            <imagedata align="center" contentwidth="6in"
                       fileref="images/every-operation-relationship.svg"></imagedata>
          </imageobject>

          <imageobject role="html">
            <imagedata align="center" contentwidth="6in"
                       fileref="images/every-operation-relationship.png"></imagedata>
          </imageobject>
        </mediaobject>

        <para>For the Every pipe, the Operation results are appended to the
        input Tuple (<code>Fields.ALL</code>) by default.</para>

        <para>It is important to note that the Every pipe associates Operation
        results with the current group Tuple. For example, if you are grouping
        on the field "department" and counting the number of "names" grouped
        by that department, the output Fields would be
        ["department","num_employees"]. This is true for both Aggregator, seen
        above, and Buffer.</para>

        <para>If you were also adding up the salaries associated with each
        "name" in each "department", the output Fields would be
        ["department","num_employees","total_salaries"]. This is only true for
        chains of Aggregator Operations, you may not chain Buffer
        Operations.</para>
      </section>

      <section>
        <title>GroupBy and CoGroup Pipes</title>

        <para>The GroupBy and CoGroup pipes serve two roles. First, they emit
        sorted grouped tuple streams allowing for Operations to be applied to
        sets of related Tuple instances. Where "sorted" means the tuple groups
        are emitted from the GroupBy and CoGroup pipes in sort order of the
        field values the groups were grouped on.</para>

        <para>Second, they allow for two streams to be either merged or
        joined. Where merging allows for two or more tuple streams originating
        from different sources to be treated as a single stream. And joining
        allows two or more streams to be "joined" (in the SQL sense) on a
        common key or set of Tuple values in a Tuple.</para>

        <para>It is not required that an Every follow either GroupBy or
        CoGroup, an Each may follow immediately after. But an Every many not
        follow an Each..</para>

        <para>GroupBy accepts one or more tuple streams. If two or more, they
        must all have the same field names.</para>

        <example>
          <title>Grouping a Tuple Stream</title>

          <xi:include href="simple-groupby.xml" />
        </example>

        <para>The example above simply creates a new tuple stream where Tuples
        with the same values in "group1" and "group2" can be processed as a
        set by an Aggregator or Buffer Operation. The resulting stream of
        tuples will be sorted by the values in "group1" and "group2".</para>

        <example>
          <title>Merging a Tuple Stream</title>

          <xi:include href="simple-groupby-merge.xml" />
        </example>

        <para>This example merges two streams ("lhs" and "rhs") into one tuple
        stream and groups the resulting stream on the fields "group1" and
        "group2", in the same fashion as the previous example.</para>

        <para>CoGroup accepts two or more tuple streams and does not require
        any common field names. The grouping fields must be provided for each
        tuple stream.</para>

        <example>
          <title>Joining a Tuple Stream</title>

          <xi:include href="simple-cogroup.xml" />
        </example>

        <para>This example joins two streams ("lhs" and "rhs") on common
        values. Note that common field names are not required here. Actually,
        if there were any common field names, the Cascading planner would
        throw an error as duplicate field names are not allowed.</para>

        <para>This is significant because of the nature of joining
        streams.</para>

        <para>The first stage of joining has to do with identifying field
        names that represent the grouping key for a given stream. The second
        stage is emitting a new Tuple with the joined values, this includes
        the grouping values, and the other values.</para>

        <mediaobject>
          <imageobject role="fo">
            <imagedata align="center" contentwidth="5in"
                       fileref="images/cogrouping-fields-fail.svg"></imagedata>
          </imageobject>

          <imageobject role="html">
            <imagedata align="center" contentwidth="5in"
                       fileref="images/cogrouping-fields-fail.png"></imagedata>
          </imageobject>
        </mediaobject>

        <para>In the above example, we see what "logically" happens during a
        join. Here we join two streams on the "url" field which happens to be
        common to both streams. The result is simply two Tuple instances with
        the same "url" appended together into a new Tuple. In practice this
        would fail since the result Tuple has duplicate field names. The
        CoGroup pipe has the declaredFields argument allowing the developer to
        declare new unique field names for the resulting tuple.</para>

        <example>
          <title>Joining a Tuple Stream with Duplicate Fields</title>

          <xi:include href="duplicate-cogroup.xml" />
        </example>

        <mediaobject>
          <imageobject role="fo">
            <imagedata align="center" contentwidth="5in"
                       fileref="images/cogrouping-fields-pass.svg"></imagedata>
          </imageobject>

          <imageobject role="html">
            <imagedata align="center" contentwidth="5in"
                       fileref="images/cogrouping-fields-pass.png"></imagedata>
          </imageobject>
        </mediaobject>

        <para>Here we see an example of what the developer could have named
        the fields so the planner would not fail.</para>

        <para>In the example above, we explicitly set a Joiner class to join
        our data. The reason CoGroup is named CoGroup and not Join is becuase
        joining data is done after all the parallel streams are co-grouped by
        their common keys. The details are not terribly important, but note
        that a "bag" of data for every input tuple stream must be created
        before an join operation can be performed. Each bag consists of all
        the Tuple instances associated with a given grouping Tuple.</para>

        <mediaobject>
          <imageobject role="fo">
            <imagedata align="center" contentwidth="5in"
                       fileref="images/cogrouped-values.svg"></imagedata>
          </imageobject>

          <imageobject role="html">
            <imagedata align="center" contentwidth="5in"
                       fileref="images/cogrouped-values.png"></imagedata>
          </imageobject>
        </mediaobject>

        <para>Above we see two bags, one for each tuple stream ("lhs" and
        "rhs"). Each Tuple in bag is independent but all Tuples in both bags
        have the same "url" value since we are grouping on url, from the
        previous example. A Joiner will match up every Tuple on the "lhs" with
        a Tuple on the "rhs". An InnerJoin is the most common. This is where
        each Tuple on the "lhs" is matched with every Tuple on the "rhs". This
        is the default behaviour one would see in SQL when doing a join. If
        one of the bags was empty, no Tuples would be joined. An OuterJoin
        allows for either bag to be empty, and if that is the case, a Tuple
        full of <code>null</code> values would be substituted.</para>

        <mediaobject>
          <imageobject role="fo">
            <imagedata align="center" contentwidth="3in"
                       fileref="images/joins.svg"></imagedata>
          </imageobject>

          <imageobject role="html">
            <imagedata align="center" contentwidth="3in"
                       fileref="images/joins.png"></imagedata>
          </imageobject>
        </mediaobject>

        <para>Above we see all supported Joiner types.</para>

        <para><programlisting>LHS = [0,a] [1,b] [2,c]
RHS = [0,A] [2,C] [3,D]</programlisting>Using the above simple data sets, we
        will define each join type where the values are joined on the first
        position, a numeric value.<variablelist>
            <varlistentry>
              <term>InnerJoin</term>

              <listitem>
                <para>An Inner join will only return a joined Tuple if neither
                bag has is empty.<programlisting>[0,a,A] [2,c,C]</programlisting></para>
              </listitem>
            </varlistentry>

            <varlistentry>
              <term>OuterJoin</term>

              <listitem>
                <para>An Outer join will join if either the left or right bag
                is empty.<programlisting>[0,a,A] [1,b,null] [2,c,C] [3,null,D]</programlisting></para>
              </listitem>
            </varlistentry>

            <varlistentry>
              <term>LeftJoin</term>

              <listitem>
                <para>A Left join can also be stated as a Left Inner and Right
                Outer join, where it is ok if the right bag is empty.</para>

                <programlisting>[0,a,A] [1,b,null] [2,c,C]</programlisting>
              </listitem>
            </varlistentry>

            <varlistentry>
              <term>RightJoin</term>

              <listitem>
                <para>A Right join can also be stated as a Left Outer and
                Right Inner join, where it is ok if the left bag is
                empty.<programlisting>[0,a,A] [2,c,C] [3,null,D]</programlisting></para>
              </listitem>
            </varlistentry>

            <varlistentry>
              <term>MixedJoin</term>

              <listitem>
                <para>A Mixed join is where 3 or more tuple streams are
                joined, and each pair must be joined differently. See the
                <classname>cascading.pipe.cogroup.MixedJoin</classname> class
                for more details.</para>
              </listitem>
            </varlistentry>

            <varlistentry>
              <term><emphasis>Custom</emphasis></term>

              <listitem>
                <para>A custom join is where the developer subclasses the
                <classname>cascading.pipe.cogroup.Joiner</classname>
                class.</para>
              </listitem>
            </varlistentry>
          </variablelist></para>
      </section>
    </section>

    <section>
      <title>Source and Sink Taps</title>

      <para>All input data comes from, and all output data feeds to, a
      <classname>cascading.tap.Tap</classname> instance.</para>

      <para>A Tap represents a resource like a data file on the local
      filesystem, on a Hadoop distributed filesystem, or even on Amazon S3.
      Taps can be read from, which makes it a "source", or written to, which
      makes it a "sink". Or, more commonly, Taps can act as both sinks and
      sources when shared between Flows.</para>

      <para>All Taps must have a Scheme associated with them. If the Tap is
      about where the data is, and how to get it, the Scheme is about what the
      data is. Cascading provides two Scheme classes, TextLine and
      SequenceFile.</para>

      <variablelist>
        <varlistentry>
          <term>TextLine</term>

          <listitem>
            <para>TextLine reads and writes text files and returns Tuples with
            two field names by default, "offset" and "line". These values are
            inherited from Hadoop. When written to, all Tuple values are
            converted to Strings and joined with the TAB character
            (\t).</para>
          </listitem>
        </varlistentry>

        <varlistentry>
          <term>SequenceFile</term>

          <listitem>
            <para>SequenceFile is based on the Hadoop Sequence file, which is
            a binary format. When written or read from, all Tuple values are
            saved in their native binary form. This is the most efficient file
            format, but being binary, the result files can only be read by
            Hadoop applications.</para>
          </listitem>
        </varlistentry>
      </variablelist>

      <para><example>
          <title>Creating a new Tap</title>

          <xi:include href="simple-tap.xml" />
        </example></para>

      <para>The above example creates a new Hadoop Filesytem Tap that can
      read/write text files. Since only one field name was provided, the
      "offset" field is discarded, resulting in an input tuple stream with
      only "line" values.</para>

      <para>The three most common Tap classes used are, Hfs, Dfs, and Lfs. The
      MultiTap is a utility Tap.</para>

      <variablelist>
        <varlistentry>
          <term>Lfs</term>

          <listitem>
            <para>The <classname>cascading.tap.Lfs</classname> Tap is used to
            reference local files. Local files are files on the same machine
            your Cascading application is started. Even if a remote Hadoop
            cluster is configured, if a Lfs Tap is used as either a source or
            sink in a Flow, Cascading will be forced to run in "local mode"
            and not on the cluster. This is useful when creating applications
            to read local files and import them into the Hadoop distributed
            filesystem.</para>
          </listitem>
        </varlistentry>

        <varlistentry>
          <term>Dfs</term>

          <listitem>
            <para>The <classname>cascading.tap.Dfs</classname> Tap is used to
            reference files on the Hadoop distributed filesystem.</para>
          </listitem>
        </varlistentry>

        <varlistentry>
          <term>Hfs</term>

          <listitem>
            <para>The <classname>cascading.tap.Hfs</classname> Tap uses the
            current Hadoop default filesystem. If Hadoop is configured for
            "local mode" its default filesystem will be the local filesystem.
            If configured as a cluster, the default filesystem is likey the
            Hadoop distributed filesystem. The Hfs is convenient when writing
            Cascading applications that may or may not be run on a cluster.
            Lhs and Dfs subclass the Hfs Tap.</para>
          </listitem>
        </varlistentry>

        <varlistentry>
          <term>MultiTap</term>

          <listitem>
            <para>The <classname>cascading.tap.MultiTap</classname> is used to
            tie multiple Tap instances into a single Tap. The only restriction
            is that all the Tap instances passed to a new MultiTap share the
            same Scheme classes (not necessarily the same Scheme
            instance).</para>
          </listitem>
        </varlistentry>
      </variablelist>

      <para>Keep in mind Hadoop cannot source data from directories with
      nested sub-directories, and it cannot write to directories that already
      exist.</para>

      <para>To get around the last issue, the Hadoop related Taps allow for a
      <classname>SinkMode</classname> value to be set when constructed.</para>

      <para><example>
          <title>Overwriting An Existing Resource</title>

          <xi:include href="simple-replace-tap.xml" />
        </example></para>

      <para>Here are all the modes available by the built-in Tap types.</para>

      <variablelist>
        <varlistentry>
          <term><classname>SinkMode.KEEP</classname></term>

          <listitem>
            <para>This is the default behavior. If the resource exists,
            attempting to write to it will fail.</para>
          </listitem>
        </varlistentry>

        <varlistentry>
          <term><classname>SinkMode.REPLACE</classname></term>

          <listitem>
            <para>This allows Cascading to delete the file immediately after
            the Flow is started.</para>
          </listitem>
        </varlistentry>

        <varlistentry>
          <term><classname>SinkMode.APPEND</classname></term>

          <listitem>
            <para>Currently unsupported, but future versions of Hadoop will
            support appends fully.</para>
          </listitem>
        </varlistentry>
      </variablelist>
    </section>

    <section>
      <title>Flows</title>

      <para>When pipe assemblies are bound to source and sink Taps, a Flow is
      created. Flows are executable in the sense that once created they can be
      "started" and will begin execution on a configured Hadoop
      cluster.</para>

      <para>Think of a Flow as a data processing workflow that reads data from
      sources, processes the data as defined by the pipe assembly, and writes
      data to the sinks. Input source data does not need to exist when the
      Flow is created, but it must exist when the Flow is executed (unless
      executed as part of a Cascade, see <xref linkend="cascades" />).</para>

      <para>The most common pattern is to create a Flow from an existing pipe
      assembly. But there are cases where a MapReduce job has already been
      created and it makes sense to encapsulate it in a Flow class so that it
      may participate in a Cascade and be scheduled with other Flow instances.
      Both patterns are covered here.</para>

      <section>
        <title>Creating Flows from Pipe Assemblies</title>

        <example>
          <title>Creating a new Flow</title>

          <xi:include href="simple-flow.xml" />
        </example>

        <para>To create a Flow, it must be planned though the FlowConnector
        object. The <code>connect()</code> method is used to create new Flow
        instances based on a set of sink Taps, source Taps, and a pipe
        assembly. The example above is quite trivial.</para>

        <example>
          <title>Binding Taps in a Flow</title>

          <xi:include href="complex-flow.xml" />
        </example>

        <para>The example above expands on our previous pipe assembly example
        by creating source and sink Taps and planning a Flow. Note there are
        two branches in the pipe assembly, one named "lhs" and the other
        "rhs". Cascading uses those names to bind the source Taps to the pipe
        assembly. A HashMap of names and taps must be passed to FlowConnector
        in order to bind Taps to branches.</para>

        <para>Since there is only one tail, the "join" pipe, we don't need to
        bind the sink to a branch name. Nor do we need to pass the heads of
        the assembly to the FlowConnector, it can determine the heads of the
        pipe assembly on the fly. When creating more complex Flows with
        multiple heads and tails, all Taps will need to be explicitly named,
        and the proper <code>connect()</code> method will need be
        called.</para>
      </section>

      <section>
        <title>Configuring Flows</title>

        <para>The FlowConnector contructor accepts the
        <classname>java.util.Property</classname> object so that default
        Cascading and Hadoop properties can be passed down through the planner
        to the Hadoop runtime. Subsequently any relevant Hadoop
        <code>hadoop-default.xml</code> properties may be added
        (<code>mapred.map.tasks.speculative.execution</code>,
        <code>mapred.reduce.tasks.speculative.execution</code>, or
        <code>mapred.child.java.opts</code> would be very common).</para>

        <para>One property that must be set for production applications is the
        application Jar class or Jar path.</para>

        <example>
          <title>Configuring the Application Jar</title>

          <xi:include href="flow-properties.xml" />
        </example>

        <para>More information on packaging production applications can be
        found in <xref linkend="executing-processes" />.</para>

        <para>Note the pattern of using a static property setter method
        (<classname>cascading.flow.FlowConnector.setApplicationJarPath</classname>),
        other classes that can be used to set properties are
        <classname>cascading.flow.MultiMapReducePlanner</classname> and
        <classname>cascading.flow.Flow</classname>.</para>

        <para>Since the <classname>FlowConnector</classname> can be reused,
        any properties passed on the constructor will be handed to all the
        Flows it is used to create. If Flows need to be created with different
        default properties, a new FlowConnector will need to be instantiated
        with those properties.</para>
      </section>

      <section>
        <title xreflabel="Skipping Flows" xml:id="skipping-flows">Skipping
        Flows</title>

        <para>When a Flow participates in a Cascade, the
        <classname>Flow#isSkip()</classname> method is consulted before
        calling <classname>Flow#start()</classname> on the flow. By default
        <methodname>isSkip()</methodname> returns true if any of the sinks are
        stale in relation to the Flow sources. Where stale is if they don't
        exist or the resources are older than the sources.</para>

        <para>This behavior is pluggable through the
        <classname>cascading.flow.FlowSkipStrategy</classname> interface. A
        new strategy can be set on a <classname>Flow</classname> instance
        after its created.</para>

        <variablelist>
          <varlistentry>
            <term>FlowSkipIfSinkStale</term>

            <listitem>
              <para>The
              <classname>cascading.flow.FlowSkipIfSinkStale</classname>
              strategy is the default strategy. Sinks are stale if they don't
              exist or the resources are older than the sources. If the
              SinkMode for the sink Tap is REPLACE, then the Tap will be
              treated as stale.</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>FlowSkipIfSinkExists</term>

            <listitem>
              <para>The
              <classname>cascading.flow.FlowSkipIfSinkExists</classname>
              strategy will skip a Flow if the sink Tap exists, regardless of
              age. If the <classname>SinkMode</classname> for the sink Tap is
              <code>REPLACE</code>, then the Tap will be treated as
              stale.</para>
            </listitem>
          </varlistentry>
        </variablelist>

        <para>Note <classname>Flow#start()</classname> and
        <classname>Flow#complete()</classname> will not consult the
        <methodname>isSkip()</methodname> method and subequently will always
        try to start the Flow if called. It is up to user code to call
        <classname>isSkip()</classname> to decide if the current strategy
        suggests the Flow should be skipped.</para>
      </section>
    </section>

    <section>
      <title>Creating Flows from a JobConf</title>

      <para>If a MapReduce job already exists and needs to be managed by a
      Cascade, then the <classname>cascading.flow.MapReduceFlow</classname>
      class should be used. After creating a Hadoop
      <classname>JobConf</classname> instance, just pass it into the
      <classname>MapReduceFlow</classname> constructor. The resulting
      <classname>Flow</classname> instance can be used like any other
      Flow.</para>
    </section>

    <section>
      <title xreflabel="Cascades" xml:id="cascades">Cascades</title>

      <mediaobject>
        <imageobject role="fo">
          <imagedata align="center" contentwidth="2 in"
                     fileref="images/cascade.svg"></imagedata>
        </imageobject>

        <imageobject role="html">
          <imagedata align="center" contentwidth="2 in"
                     fileref="images/cascade.png"></imagedata>
        </imageobject>
      </mediaobject>

      <para>A Cascade allows multiple Flow instances to be executed as a
      single logical unit. If there are dependencies between the Flows, they
      will be executed in the correct order. Further, Cascades act like ant
      build or unix "make" files. When run, a Cascade will only execute Flows
      that have stale sinks (output data that is older than the input data),
      by default.</para>

      <example>
        <title>Creating a new Cascade</title>

        <xi:include href="simple-cascade.xml" />
      </example>

      <para>When passing Flows to the CascadeConnector, order is not
      important. The CascadeConnector will automatically determine what the
      dependencies are between the given Flows and create a scheduler that
      will start each flow as its data sources become available. If two or
      more Flow instances have no dependencies, they will be submitted
      together so they can execute in parallel.</para>

      <para>For more information, see the section on <xref
      linkend="cascade-scheduler" />.</para>

      <para>If an instance of
      <classname>cascading.flow.FlowSkipStrategy</classname> is given to an
      <classname>Cascade</classname> instance via the
      <classname>Cascade#setFlowSkipStrategy()</classname> method, it will be
      consulted for every Flow instance managed by the Cascade, all skip
      strategies on the Flow instances will be ignored. For more information
      on skip strategies, see <xref linkend="skipping-flows" />.</para>
    </section>
  </chapter>

  <chapter>
    <info>
      <title xreflabel="Executing Processes"
      xml:id="executing-processes">Executing Processes</title>
    </info>

    <section>
      <title>Introduction</title>

      <para>Cascading requires Hadoop to be installed and correctly
      configured. Apache Hadoop is an Open Source Apache project and is freely
      available. It can be downloaded from here the Hadoop website, <link
      xlink:href="http://hadoop.apache.org/core/">http://hadoop.apache.org/core/</link>.</para>
    </section>

    <section>
      <title xreflabel="Building Cascading Applications"
      xml:id="building-processes">Building</title>

      <para>Cascading ships with a handful of jars.</para>

      <variablelist>
        <varlistentry>
          <term>cascading-1.0.x.jar</term>

          <listitem>
            <para>all relevant Cascading class files and libraries, with a
            <filename>lib</filename> folder containing all third-party
            dependencies</para>
          </listitem>
        </varlistentry>

        <varlistentry>
          <term>cascading-core-1.0.x.jar</term>

          <listitem>
            <para>all Cascading Core class files, should be packaged with
            <filename>lib/*.jar</filename></para>
          </listitem>
        </varlistentry>

        <varlistentry>
          <term>cascading-xml-1.0.x.jar</term>

          <listitem>
            <para>all Cascading XML module class files, should be packeaged
            with <filename>lib/xml/*.jar</filename></para>
          </listitem>
        </varlistentry>

        <varlistentry>
          <term>cascading-test-1.0.x.jar</term>

          <listitem>
            <para>all Cascading unit tests. If writing custom modules for
            cascading, subclassing
            <classname>cascading.CascadingTestCase</classname> might be
            helpful</para>
          </listitem>
        </varlistentry>
      </variablelist>

      <para>Cascading will run with Hadoop in its default 'local' or 'stand
      alone' mode, or configured as a distributed cluster.</para>

      <para>When used on a cluster, a Hadoop job Jar must be created with
      Cascading jars and dependent thrid-party jars in the job jar
      <filename>lib</filename> directory, per the Hadoop documentation.</para>

      <example>
        <title>Sample Ant Build - Properties</title>

        <xi:include href="sample-build-properties.xml" />
      </example>

      <example>
        <title>Sample Ant Build - Target</title>

        <xi:include href="sample-build-target.xml" />
      </example>

      <para>The above Ant snippets can be used in your project to create a
      Hadoop jar for submission on your cluster. Again, all Hadoop
      applications that are intended to be run in a cluster must be packaged
      with all third-party libraries in a directory named
      <filename>lib</filename> in the final application Jar file, regardless
      if they are Cascading application or Hadoop MapReduce
      applications.</para>

      <para>Note, the snippets above is only intended to show how to include
      Cascading libraries, you still need to compile your project into the
      <property>build.classes</property> path.</para>
    </section>

    <section>
      <title>Executing</title>

      <para>Running a Cascading application is exactly the same as running any
      Hadoop application. After packaging your application into a single jar
      (see <xref linkend="building-processes" />), you must use
      <filename>bin/hadoop</filename> to submit the application to the
      cluster.</para>

      <para>For example, to execute an application stuffed into
      <filename>your-application.jar</filename>, call the Hadoop shell
      script:</para>

      <example>
        <title>Running a Cascading Application</title>

        <para><programlisting>$HADOOP_HOME/bin/hadoop jar your-application.jar [some params]</programlisting></para>
      </example>

      <para>If the configuration scripts in <envar>$HADOOP_CONF_DIR</envar>
      are configured to use a cluster, the Jar will be pushed into the cluster
      for execution.</para>

      <para>Cascading does not rely on any environment variables like
      <envar>$HADOOP_HOME</envar> or <envar>$HADOOP_CONF_DIR</envar>, only
      <filename>bin/hadoop</filename> does.</para>
    </section>
  </chapter>

  <chapter>
    <info>
      <title>Using and Developing Operations</title>
    </info>

    <section>
      <title>Introduction</title>

      <para>To use Cascading, it is not strictly necessary to create custom
      Operations. There are a number of Operations in the Cascading library
      that can be combined into very rubust applications. In the same way you
      can chaing <command>sed</command>, <command>grep</command>,
      <command>sort</command>, <command>uniq</command>,
      <command>awk</command>, etc in Unix, you can chain existing Cascading
      operations. But developing customs Opeations is very simple in
      Cascading.</para>

      <para>There are four kinds of Operations: Function, Filter, Aggregator,
      and Buffer.</para>

      <mediaobject>
        <imageobject role="fo">
          <imagedata align="center" contentwidth="3in"
                     fileref="images/operations.svg"></imagedata>
        </imageobject>

        <imageobject role="html">
          <imagedata align="center" contentwidth="3in"
                     fileref="images/operations.png"></imagedata>
        </imageobject>
      </mediaobject>

      <para>All Operations operate on an input argument Tuple and all
      Operations other than Filter may return zero or more Tuple object
      results. That is, a Function can parse a string and return a new Tuple
      for every value parsed out (one Tuple for each 'word'), or it may create
      a single Tuple with every parsed value as an element in the Tuple object
      (one Tuple with "first-name" and "last-name" fields).</para>

      <para>In practice, a Function that returns no results is a Filter, but
      the Filter type has been optimized and can be combined with "logical"
      filter Operations like Not, And, Or, etc.</para>

      <para>During runtime, Operations actually receive arguments as an
      instance of the TupleEntry object. The TupleEntry object holds both an
      instance of <classname>Fields</classname> and the current
      <classname>Tuple</classname> the <classname>Fields</classname> object
      defines fields for. <classname>TupleEntry</classname> is a helper class
      that allows for tuple operations by using simple field names and is
      typically only exposed to developers writing custom Operations.</para>

      <para>All Operations, other than Filter, must declare result Fields. For
      example, if a Function was written to parse words out of a String and
      return a new Tuple for each word, this Function must declare that it
      intends to return a Tuple with one field named "word". If the Function
      mistakenly returns more values in the Tuple other than a 'word', the
      process will fail. Operations that do return arbitrary numbers of values
      in a result Tuple may declare <code>Fields.UNKNOWN</code>.</para>

      <para>The Cascading planner always attempts to "fail fast" where
      possible by checking the field name dependencies between Pipes and
      Operations, but some cases the planner can't account for.</para>

      <para>All Operations must be wrapped by either an Each or an Every pipe
      instance. The pipe is responsible for passing in an argument Tuple and
      accepting the result Tuple.</para>
    </section>

    <section>
      <title>Functions</title>

      <para>A Function expects a single argument <classname>Tuple</classname>,
      and may return zero or more result Tuples.</para>

      <para>A Function may only be used with a <classname>Each</classname>
      pipe which may follow any other pipe type.</para>

      <para>To create a custom Function, subclass the class
      <code>cascading.operation.BaseOperation</code> and implement the
      interface<code> cascading.operation.Function</code>. Because
      <code>BaseOperation</code> has been subclassed, the <code>operate</code>
      method, as defined on the <code>Function</code> interface, is the only
      method that must be implemented.</para>

      <example>
        <title>Custom Function</title>

        <xi:include href="custom-function.xml" />
      </example>

      <para>Functions should declare both the number of argument values they
      expect, and the field names of the Tuple they will return.</para>

      <para>Functions must accept 1 or more values in a Tuple as arguments, by
      default they will accept any number (<code>Operation.ANY</code>) of
      values. Cascading will verify that the number of arguments selected
      match the number of arguments expected during the planning phase.</para>

      <para>Functions may optionally declare the field names they return, by
      default Functions declare<code> Fields.UNKNOWN</code>.</para>

      <para>Both declarations must be done on the constructor, either by
      passing default values to the <code>super</code> constructor, or by
      accepting the values from the user via a constructor
      implementation.</para>

      <example>
        <title>Add Values Function</title>

        <xi:include href="sum-function.xml" />
      </example>

      <para>The example above implements a fully functional Function that
      accepts two values in the argument Tuple, adds them together, and
      returns the result in a new Tuple.</para>

      <para>The first constructor assumes a default field name this function
      will return, but it is a best practice to always give the user the
      option to override the declared field names to prevent any field name
      collisions that would cause the planner to fail.</para>
    </section>

    <section>
      <title>Filter</title>

      <para>A Filter expects a single argument Tuple and returns a boolean
      value stating whether or not the current Tuple in the tuple stream
      should be discarded.</para>

      <para>A Filter may only be used with a <classname>Each</classname> pipe,
      and it may follow any other pipe type.</para>

      <para>To create a custom Filter, subclass the class
      <code>cascading.operation.BaseOperation</code> and implement the
      interface<code>cascading.operation.Filter</code>. Because
      <code>BaseOperation</code> has been subclassed, the
      <code>isRemove</code> method, as defined on the <code>Filter</code>
      interface, is the only method that must be implemented.</para>

      <example>
        <title>Custom Filter</title>

        <xi:include href="custom-filter.xml" />
      </example>

      <para>Filters should declare the number of argument values they
      expect.</para>

      <para>Filters must accept 1 or more values in a Tuple as arguments, by
      default they will accept any number (<code> Operation.ANY</code>) of
      values. Cascading will verify the number of arguments selected match the
      number of arguments expected.</para>

      <para>The number of arguments declarations must be done on the
      constructor, either by passing a default value to the <code>super</code>
      constructor, or by accepting the value from the user via a constructor
      implementation.</para>

      <example>
        <title>String Length Filter</title>

        <xi:include href="stringlength-filter.xml" />
      </example>

      <para>The example above implements a fully functional Filter that
      accepts two arguments and filters out the current Tuple if the first
      argument String length is greater than the integer value of the second
      argument.</para>
    </section>

    <section>
      <title>Aggregator</title>

      <para>An Aggregator expects set of argument Tuples in the same grouping,
      and may return zero or more result Tuples.</para>

      <para>An Aggregator may only be used with an
      <classname>Every</classname> pipe, and it may only follow a
      <classname>GroupBy</classname>,<classname>CoGroup</classname>, or
      another <classname>Every</classname> pipe type.</para>

      <para>To create a custom Aggregator, subclass the class
      <code>cascading.operation.BaseOperation</code> and implement the
      interface<code>cascading.operation.Aggregator</code>. Because
      <code>BaseOperation</code> has been subclassed, the <code>start</code>,
      <code>aggregate</code>, and <code>complete</code> methods, as defined on
      the <code>Aggregator</code> interface, are the only methods that must be
      implemented.</para>

      <example>
        <title>Custom Aggregator</title>

        <xi:include href="custom-aggregator.xml" />
      </example>

      <para>Aggregators should declare both the number of argument values they
      expect, and the field names of the Tuple they will return.</para>

      <para>Aggregators must accept 1 or more values in a Tuple as arguments,
      by default they will accept any number ( <code>Operation.ANY</code>) of
      values. Cascading will verify the number of arguments selected match the
      number of arguments expected.</para>

      <para>Aggregators may optionally declare the field names they return, by
      default Aggregators declare<code> Fields.UNKNOWN</code>.</para>

      <para>Both declarations must be done on the constructor, either by
      passing default values to the <code>super</code> constructor, or by
      accepting the values from the user via a constructor
      implementation.</para>

      <example>
        <title>Add Tuples Aggregator</title>

        <xi:include href="sum-aggregator.xml" />
      </example>

      <para>The example above implements a fully functional Aggregator that
      accepts one value in the argument Tuple, adds all these argument Tuples
      in the current grouping, and returns the result as a new Tuple.</para>

      <para>The first constructor assumes a default field name this Aggregator
      will return, but it is a best practice to always give the user the
      option to override the declared field names to prevent any field name
      collisions that would cause the planner to fail.</para>
    </section>

    <section>
      <title>Buffer</title>

      <para>A Buffer expects set of argument Tuples in the same grouping, and
      may return zero or more result Tuples.</para>

      <para>The Buffer is very similiar to an Aggregator except it receives
      the current Grouping Tuple and an iterator of all the arguments it
      expects for every value Tuple in the current grouping, all on the same
      method call. This is very similar to the typical Reducer interface, and
      is best used for operations that need greater visibility to the previous
      and next elements in the stream. For example, smoothing a series of
      time-stamps where there are missing values.</para>

      <para>An Buffer may only be used with an <classname>Every</classname>
      pipe, and it may only follow a <classname>GroupBy</classname> or
      <classname>CoGroup</classname> pipe type.</para>

      <para>To create a custom Buffer, subclass the class
      <code>cascading.operation.BaseOperation</code> and implement the
      interface<code>cascading.operation.Buffer</code>. Because
      <code>BaseOperation</code> has been subclassed, the <code>operate</code>
      method, as defined on the <code>Buffer</code> interface, is the only
      method that must be implemented.</para>

      <example>
        <title>Custom Buffer</title>

        <xi:include href="custom-buffer.xml" />
      </example>

      <para>Buffer should declare both the number of argument values they
      expect, and the field names of the Tuple they will return.</para>

      <para>Buffers must accept 1 or more values in a Tuple as arguments, by
      default they will accept any number ( <code>Operation.ANY</code>) of
      values. Cascading will verify the number of arguments selected match the
      number of arguments expected.</para>

      <para>Buffers may optionally declare the field names they return, by
      default Buffers declare <code>Fields.UNKNOWN</code>.</para>

      <para>Both declarations must be done on the constructor, either by
      passing default values to the <code>super</code> constructor, or by
      accepting the values from the user via a constructor
      implementation.</para>

      <example>
        <title>Average Buffer</title>

        <xi:include href="average-buffer.xml" />
      </example>

      <para>The example above implements a fully functional buffer that
      accepts one value in argument Tuple, adds all these argument Tuples in
      the current grouping, and returns the result divided by the number of
      argument tuples counted in a new Tuple.</para>

      <para>The first constructor assumes a default field name this Buffer
      will return, but it is a best practice to always give the user the
      option to override the declared field names to prevent any field name
      collisions that would cause the planner to fail.</para>

      <para>Note this example is somewhat fabricated, in practice a
      <classname>Aggregator</classname> should be implemented to compute
      averages. A Buffer would be better suited for "running averages" across
      very large spans, for example.</para>
    </section>

    <section>
      <title>Operation and BaseOperation</title>

      <para>In all the above sections, the
      <classname>cascading.operation.BaseOperation</classname> class was
      subclassed. This class is an implementation of the
      <classname>cascading.operation.Operation</classname> interface and
      provides a few default method implementations. It is not strictly
      required to extend <classname>BaseOperation</classname>, but it is very
      convenient to do so.</para>

      <para>When developing custom operations, the developer may need to
      intialize and destroy a resource. For example, when doing pattern
      matching, a <classname>java.util.regex.Matcher</classname> may need to
      be initialized and used in a thread-safe way. Or a remote connection may
      need to be opened and eventually closed. But for performance reasons,
      the operation should not create/destroy the connection for each Tuple or
      every Tuple group that passes through.</para>

      <para>The interface <interfacename>Operation</interfacename> declares
      two methods, <methodname>prepare()</methodname> and
      <methodname>cleanup()</methodname>. In the case of Hadoop and MapReduce,
      the <methodname>prepare()</methodname> and
      <methodname>cleanup()</methodname> methods are called once per Map or
      Reduce task. <methodname>prepare()</methodname> is called before any
      argument Tuple is passed in, and <methodname>cleanup()</methodname> is
      called after all Tuple arguments have been operated on. Within each of
      these methods, the developer can initialize a "context" object that can
      hold an open socket connection, or <classname>Matcher</classname>
      instance. The "context" is user defined and is the same mechanism used
      by the Aggregator operation, except the Aggregator is also given the
      opportunity to initialize and destroy its context via the
      <classname>start()</classname> and <classname>complete()</classname>
      methods.</para>
    </section>
  </chapter>

  <chapter>
    <info>
      <title>Advanced Processing</title>
    </info>

    <section>
      <title xreflabel="SubAssemblies"
      xml:id="subassemblies">SubAssemblies</title>

      <para>Cascading SubAssemblies are reusable pipe assemblies that are
      linked into larger pipe assemblies. Think of them as subroutines in a
      programming language. The help organize complex pipe assemblies and
      allow for commonly used pipe assemblies to be packaged into libraries
      for inclusion by other users.</para>

      <para>To create a SubAssembly, the
      <classname>cascading.pipe.SubAssembly</classname> class must be
      subclasses.</para>

      <example>
        <title>Creating a SubAssembly</title>

        <xi:include href="custom-subassembly.xml" />
      </example>

      <para>In the above example, we pass in via the constructor pipes we wish
      to continue assembling against, and the last line we register the 'join'
      pipe as a tail. This allows SubAssemblies to be nested within larger
      pipe assemblies or other SubAssemblies.</para>

      <example>
        <title>Using a SubAssembly</title>

        <xi:include href="simple-subassembly.xml" />
      </example>

      <para>Above we see how natural it is to include a SubAssembly into a new
      pipe assembly.</para>

      <para>If we had a SubAssembly that represented a split, that is, had two
      or more tails, we could use the <methodname>getTails()</methodname>
      method to get at the array of "tails" set internally by the
      <methodname>setTails()</methodname> method.</para>
    </section>

    <section>
      <title xreflabel="Stream Assertions" xml:id="stream-assertions">Stream
      Assertions</title>

      <para><inlinemediaobject>
          <imageobject role="fo">
            <imagedata align="center" contentwidth="6in"
                       fileref="images/stream-assertions.svg"></imagedata>
          </imageobject>

          <imageobject role="html">
            <imagedata align="center" contentwidth="6in"
                       fileref="images/stream-assertions.png"></imagedata>
          </imageobject>

          <textobject>
            <phrase>A Flow with Stream Assertions</phrase>
          </textobject>
        </inlinemediaobject></para>

      <para>Stream assertions are simply a mechanism to 'assert' that one or
      more values in a tuple stream meet certain criteria. This is similar to
      the Java language 'assert' keyword, or a unit test. An example would be
      'assert not null' or 'assert matches'.</para>

      <para>Assertions are treated like any other function or aggregator in
      Cascading. They are embedded directly into the pipe assembly by the
      developer. If an assertion fails, the processing stops, by default.
      Alternately they can trigger a Failure Trap.</para>

      <para>As with any test, sometimes they are wanted, and sometimes they
      are unnecessary. Thus stream assertions are embedded as either 'strict'
      or 'validating'.</para>

      <para>When running a tests against regression data, it makes sense to
      use strict assertions. This regression data should be small and
      represent many of the edge cases the processing assembly must support
      robustly. When running tests in staging, or with data that may vary in
      quality since it is from an unmanaged source, using validating
      assertions make much sense. Then there are obvious cases where
      assertions just get in the way and slow down processing and it would be
      nice to just bypass them.</para>

      <para>During runtime, Cascading can be instructed to plan out strict,
      validating, or all assertions before building the final MapReduce jobs
      via the MapReduce Job Planner. And they are truly planned out of the
      resulting job, not just switched off, providing the best
      performance.</para>

      <para>This is just one feature of lazily building MapReduce jobs via a
      planner, instead of hard coding them.</para>

      <example>
        <title>Adding Assertions</title>

        <xi:include href="simple-assertion.xml" />
      </example>

      <para>Again, assertions are added to a pipe assembly like any other
      operation, except the <classname>AssertionLevel</classname> must be set,
      so the planner knows how to treat the assertion during planning.</para>

      <example>
        <title>Planning Out Assertions</title>

        <xi:include href="simple-assertion-planner.xml" />
      </example>

      <para>To configure the planner to remove some or all assertions, a
      property must be set via the
      <classname>FlowConnector#setAssertionLevel()</classname> mehod.
      <classname>AssertionLevel.NONE</classname> removes all assertions.
      <classname>AssertionLevel.VALID</classname> keeps <code>VALID</code>
      assertions but removes <code>STRICT</code> ones. And
      <classname>AssertionLevel.STRICT</classname> keeps all assertions, which
      is the planner default value.</para>
    </section>

    <section>
      <title>Failure Traps</title>

      <para><inlinemediaobject>
          <imageobject role="fo">
            <imagedata align="center" contentwidth="6in"
                       fileref="images/failure-traps.svg"></imagedata>
          </imageobject>

          <imageobject role="html">
            <imagedata align="center" contentwidth="6in"
                       fileref="images/failure-traps.png"></imagedata>
          </imageobject>

          <textobject>
            <phrase>A Flow with Failure Traps</phrase>
          </textobject>
        </inlinemediaobject></para>

      <para>Failure Traps are the same as a Tap sink (opposed to a source),
      except being bound to a particular tail element of the pipe assembly,
      traps can be bound to intermediate pipe assembly segments, like to a
      Stream Assertion.</para>

      <para>Whenever an operation fails and throws an exception, and there is
      an associated trap, the offending Tuple will be saved to the resource
      specified by the trap Tap. This allows the job to continue processing
      without any data loss.</para>

      <para>By design, clusters are hardware fault tolerant. Lose a node, the
      cluster continues working.</para>

      <para>But software fault tolerance is a little different. Failure Traps
      provide a means for the processing to continue without losing track of
      the data that caused the fault. For high fidelity applications, this may
      not be so attractive, but low fidelity applications (like web page
      indexing) this can dramatically improve processing reliability.</para>

      <example>
        <title>Setting Traps</title>

        <xi:include href="simple-traps.xml" />
      </example>

      <para>In the above example, we bind our trap Tap to the pipe assembly
      segment named "assertions". Note how we can name branches and segments
      by using a single <classname>Pipe</classname> instance and it applies to
      all subsequent <classname>Pipe</classname> instances.</para>

      <para><note>
          <para>Even though Hadoop 0.19.x supports file appends on HDFS, this
          functionality has not leaked up to the interfaces that allow Mapper
          and Reducers to output data. Subsequently, the Cascading planner
          will throw <classname>PlannerExceptions</classname> if a trap is
          bound to a pipe assembly segment that crosses a Map or Reduce
          boundary. To get around these errors, either insert new
          <classname>Pipe</classname> instances that rename the current
          branch, or give <classname>GroupBy</classname> or
          <classname>CoGroup</classname> explicit names. This is done to
          prevent subsequent Map or Reduce tasks from writing to the same path
          after the Tap has been written to and closed in a previous Map or
          Reduce task.</para>
        </note></para>
    </section>

    <section>
      <title>Event Handling</title>

      <para>Each Flow, has the ability to execute callbacks via an event
      listener. This is very useful when external application need to be
      notified that a Flow has completed.</para>

      <para>A good example is when running Flows on an Amazon EC2 Hadoop
      cluster. After the Flow is completed, a SQS event can be sent notifying
      another application it can now fetch the job results from S3. In tandem,
      it can start the process of shutting down the cluster if no more work is
      queued up for it.</para>

      <para>Flows support event listeners through the
      <classname>cascading.flow.FlowListener</classname> interface. The
      FlowListener interface supports four events, onStarting, onStopping,
      onCompleted, and onThrowable.</para>

      <variablelist>
        <varlistentry>
          <term>onStarting</term>

          <listitem>
            <para>The onStarting event is fired when a Flow instance receives
            the <code>start()</code> message.</para>
          </listitem>
        </varlistentry>

        <varlistentry>
          <term>onStopping</term>

          <listitem>
            <para>The onStopping event is fired when a Flow instance receives
            the <code>stop()</code> message.</para>
          </listitem>
        </varlistentry>

        <varlistentry>
          <term>onCompleted</term>

          <listitem>
            <para>The onCompleted event is fired when a Flow instance has
            completed all work whether if was success or failed. If there was
            a thrown exception, onThrowable will be fired before this
            event.</para>
          </listitem>
        </varlistentry>

        <varlistentry>
          <term>onThrowable</term>

          <listitem>
            <para>The onThrowable event is fired if any internal job client
            throws a Throwable type. This throwable is passed as an argument
            to the event. onThrowable should return true if the given
            throwable was handled and should not be rethrown from the
            <code>Flow.complete()</code> method.</para>
          </listitem>
        </varlistentry>
      </variablelist>

      <para>FlowListeners are useful when external systems must be notified
      when a Flow has completed or failed.</para>
    </section>

    <section>
      <title>Template Taps</title>

      <para>The <classname>TemplateTap</classname> <classname>Tap</classname>
      class provides a simple means to break large datasets into smaller sets
      based on values in the dataset. Typically this is called 'binning' the
      data, where each 'bin' of data is named after values shared by the data
      in that bin. For example, organizing log files by month and year.</para>

      <xi:include href="template-tap.xml" />

      <para>In the above example, we construct a parent
      <classname>Hfs</classname> <classname>Tap</classname> and pass it to the
      constructor of a <classname>TemplateTap</classname> instance along with
      a String format 'template'. This format template is populated in the
      order values are declared via the <classname>Scheme</classname> class.
      If more complex path formatting is necessary then you may subclass the
      <classname>TemplateTap</classname>.</para>

      <para>Note that you can only create sub-directories to bin data into.
      Hadoop must still write 'part' files into each bin directory.</para>

      <para>One last thing to keep in mind is whether or not 'binning' happens
      during the Map or Reduce phase. By doing a
      <classname>GroupBy</classname> on the values that will be used to
      populate the template, binning will happen during the Reduce phase and
      likely scale much better if there are a very large number of unique
      grouping keys.</para>
    </section>

    <section>
      <title>Scripting</title>

      <para>Cascading was designed with scripting in mind. Since it is just an
      API, any Java compatible scripting language can import and instantiate
      Cascading classes and create pipe assemblies, flows, and execute those
      flows.</para>

      <para>And if the scripting language in question supports Domain Specific
      Language (DSL) creation, the user can create her own DSL to handle
      common idioms.</para>

      <para>See the Cascading website for publicly available scripting
      languate bindings.</para>
    </section>
  </chapter>

  <chapter>
    <info>
      <title>Built-In Operations</title>
    </info>

    <section>
      <title>Identity Function</title>

      <para>The <classname>cascading.operation.Identify</classname> function
      is used to "shape" a tuple stream. Here are some common patterns.</para>

      <para><variablelist>
          <varlistentry>
            <term>Discard unused fields</term>

            <listitem>
              <para>Here Identity passes its arguments out as results, thanks
              to the <code>Fields.ARGS</code> field declaration.</para>

              <xi:include href="identity-discard-fields-long.xml" />

              <para>In practice the field declaration can be left out as
              <code>Field.ARGS</code> is the default declaration for the
              Identity function. Additionally <code>Fields.RESULTs</code> can
              be left off as it is the default for the Every pipe.</para>

              <xi:include href="identity-discard-fields.xml" />
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>Rename all fields</term>

            <listitem>
              <para>Here Identity renames the incoming arguments. Since
              Fields.RESULTS is implied, the incoming Tuple is replaced by the
              arguments selected and given new field names as declared on
              Identity.</para>

              <xi:include href="identity-rename-fields-explicit.xml" />

              <para>In the above example, if there were more fields than "ip"
              and "method", it would work fine, all the extra fields would be
              discared. If the same was true for the next example, the planner
              would fail.</para>

              <xi:include href="identity-rename-fields-long.xml" />

              <para>Since <code>Fields.ALL</code> is the default argument
              selector for the Each pipe, it can be left out.</para>

              <xi:include href="identity-rename-fields.xml" />
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>Rename a single field</term>

            <listitem>
              <para>Here we rename a single field, but return it along with an
              input Tuple field as the result.</para>

              <xi:include href="identity-rename-some.xml" />
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>Coerce values to specific primitive types</term>

            <listitem>
              <para>Here we replace the Tuple String values "status" and
              "size" with <classname>int</classname> and
              <classname>long</classname>, respectively.</para>

              <xi:include href="identity-coerce.xml" />
            </listitem>
          </varlistentry>
        </variablelist></para>
    </section>

    <section>
      <title>Debug Function</title>

      <para>The <classname>cascading.operation.Debug</classname> function is a
      utility Function (actually, its a Filter) that will print the current
      argument Tuple to either stdout or stderr.</para>

      <para>It is only intended to be used during development to help debug a
      difficult pipe assembly.</para>
    </section>

    <section>
      <title>Sample and Limit Functions</title>

      <para>The Sample and Limit functions are useed to limit the number of
      Tuples that pass through a pipe assembly.</para>

      <para><variablelist>
          <varlistentry>
            <term>Sample</term>

            <listitem>
              <para>The
              <classname>cascading.operation.filter.Sample</classname> filter
              allows a percentage of tuples to pass.</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>Limit</term>

            <listitem>
              <para>The
              <classname>cascading.operation.filter.Limit</classname> filter
              allows a set number of Tuples to pass.</para>
            </listitem>
          </varlistentry>
        </variablelist></para>
    </section>

    <section>
      <title>Insert Function</title>

      <para>The <classname>cascading.operation.Insert</classname> function
      allows for insertion of literal values into the tuple stream.</para>

      <para>This is most useful when a splitting a tuple stream and one of the
      branches needs some identifying value. Or when some missing parameter or
      value, like a date String for todays date, needs to be inserted.</para>
    </section>

    <section>
      <title>Text Functions</title>

      <para>Cascading includes a number of text functions in the
      <classname>cascading.operation.text</classname> package.</para>

      <para><variablelist>
          <varlistentry>
            <term>FieldJoiner</term>

            <listitem>
              <para>The
              <classname>cascading.operation.text.FieldJoiner</classname>
              function joins all the values in a Tuple with a given delimiter
              and stuffs the result into a new field.</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>FieldFormatter</term>

            <listitem>
              <para>The
              <classname>cascading.operation.text.FieldFormatter</classname>
              function formats Tuple values with a given String format and
              stuffs the result into a new field. The
              <classname>java.util.Formatter</classname> class is used to
              create a new formatted String.</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>DateParser</term>

            <listitem>
              <para>The
              <classname>cascading.operation.text.DateParser</classname>
              function is used to convert a text date String to a timestamp
              using the <classname>java.text.SimpleDateFormat</classname>
              syntax. The timestamp is a <classname>long</classname> value
              representing the number of milliseconds since January 1, 1970,
              00:00:00 GMT. By default it emits a field with the name "ts" for
              timestamp, but this can be overridden by passing a declared
              Fields value.</para>

              <xi:include href="text-create-timestamp.xml" />

              <para>Above we convert an Apache log style date-time field into
              a <classname>long</classname> timestamp.</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>DateFormatter</term>

            <listitem>
              <para>The
              <classname>cascading.operation.text.DateFormatter</classname>
              function is used to convert a date timestamp to a formatted
              String. This function expects a <classname>long</classname>
              value representing the number of milliseconds since January 1,
              1970, 00:00:00 GMT. And uses the
              <classname>java.text.SimpleDateFormat</classname> syntax.</para>

              <xi:include href="text-format-date.xml" />

              <para>Above we convert a <classname>long</classname> timestamp
              ("ts") to a date String.</para>
            </listitem>
          </varlistentry>
        </variablelist></para>
    </section>

    <section>
      <title>Regular Expression Operations</title>

      <para><variablelist>
          <varlistentry>
            <term>RegexSplitter</term>

            <listitem>
              <para>The
              <classname>cascading.operation.regex.RegexSplitter</classname>
              function will split an argument value by a regex pattern String.
              Internally, this function uses
              <classname>java.util.regex.Pattern#split()</classname>, thus
              behaves accordingly. By default this function splits on the TAB
              character ("\t"). If a known number of values will emerge from
              this function, it can declare field names. In this case, if the
              splitter encounters more split values than field names, the
              remaining values will be discarded, see
              <classname>java.util.regex.Pattern#split( input, limit
              )</classname> for more information.</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>RegexParser</term>

            <listitem>
              <para>The
              <classname>cascading.operation.regex.RegexParser</classname>
              function is used to extract a regular expression matched value
              from an incoming argument value. If the regular expression is
              sufficiently complex, and int array may be provided which
              specifies which regex groups should be returned into which field
              names.</para>

              <xi:include href="regex-parser.xml" />

              <para>Above, we parse an Apache log "line" into its parts. Note
              the int[] groups array starts at 1, not 0. Group 0 is the whole
              group, so if included the first field would be a copy of "line"
              and not "ip".</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>RegexReplace</term>

            <listitem>
              <para>The
              <classname>cascading.operation.regex.RegexReplace</classname>
              function is used to replace a regex matched value with a
              replacement value. It maybe used in a "replace all" or "replace
              first" mode. See
              <classname>java.util.regex.Matcher#replaceAll()</classname> and
              <classname>java.util.regex.Matcher#replaceFirst()</classname>
              methods.</para>

              <xi:include href="regex-replace.xml" />

              <para>Above we replace all adjoined white space characters with
              a single space character.</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>RegexFilter</term>

            <listitem>
              <para>The
              <classname>cascading.operation.regex.RegexFilter</classname>
              function will apply a regular expression pattern String against
              every input Tuple value and filter the Tuple stream accordingly.
              By default, Tuples that match the given pattern are kept, and
              Tuples that do not match are filtered out. This can be changed
              by setting "removeMatch" to <code>true</code>. Also, by default,
              the whole Tuple is matched against the given pattern String (TAB
              delimited). If "matchEachElement" is set to <code>true</code>,
              the pattern is applied to each Tuple value individually. See the
              <classname>java.util.regex.Matcher#find()</classname>
              method.</para>

              <xi:include href="regex-filter.xml" />

              <para>Above we keep all lines where the "ip" address starts with
              "68.".</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>RegexGenerator</term>

            <listitem>
              <para>The
              <classname>cascading.operation.regex.RegexGenerator</classname>
              function will emit a new Tuple for every matched regular
              expression group, instead of a Tuple with every group as a
              value.</para>

              <xi:include href="regex-generator.xml" />

              <para>Above each "line" in a document is parsed into unique
              words and stored in the "word" field of each result
              Tuple.</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>RegexSplitGenerator</term>

            <listitem>
              <para>The
              <classname>cascading.operation.regex.RegexSplitGenerator</classname>
              function will emit a new Tuple for every split on the incoming
              argument value delimited by the given pattern String. The
              behavior is similar to the RegexSplitter function.</para>
            </listitem>
          </varlistentry>
        </variablelist></para>
    </section>

    <section>
      <title xreflabel="Expression Operations"
      xml:id="operation-expression">Java Expression Operations</title>

      <para>Cascading provides some support for dynamically compiled Java
      expression to be used as either Functions or Filters. This functionality
      is provided by the Janino embedded compiler. Janino and its documetation
      can be found on its website, <link
      xlink:href="http://www.janino.net/">http://www.janino.net/</link>. But
      in short, an Expression is a single line of Java, for example <code>a +
      3 * 2</code> or <code>a &lt; 7</code>. The first would resolve to some
      number, the second to a boolean value. Where <code>a</code> and
      <code>b</code> are field names passed in as Tuple arguments to the
      Operation. Janino will compile this expression into bytecode giving
      compiled code processing speeds.</para>

      <para><variablelist>
          <varlistentry>
            <term>ExpressionFunction</term>

            <listitem>
              <para>The
              <classname>cascading.operation.expressionExpressionFunction</classname>
              function dynamically resolves a given expression using argument
              Tuple values as inputs to the fields specified in the
              expression.</para>

              <xi:include href="expression-function.xml" />

              <para>Above, we create a new String value form our expression.
              Note we must declare the type of every input Tuple value so the
              expression compiler knows how to treat the variables in the
              expression.</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>ExpressionFilter</term>

            <listitem>
              <para>The
              <classname>cascading.operation.expressionExpressionFilter</classname>
              filter dynamically resolves a given expression using argument
              Tuple values as inputs to the fields specified in the
              expression. Any Tuple that returns true for the given expression
              will be removed from the stream.</para>

              <xi:include href="expression-filter.xml" />

              <para>Above, every line in the Apache log that does not have a
              "200" status will be filtered out. Notice that the "status"
              would be a String in this example if it was emitted from a
              RegexParser, if so the ExpressionFilter will coerce the value
              from a String to an <classname>int</classname> for the
              comparison.</para>
            </listitem>
          </varlistentry>
        </variablelist></para>
    </section>

    <section>
      <title>XML Operations</title>

      <para>All XML Operations are kept in a module other than core, so can be
      included in a Cascading application by including the
      <filename>cascading-xml-x.y.z.jar</filename> in the project. This module
      has one dependency, the TagSoup library, which allows for HTML and XML
      "tidying". More about TagSoup can be read on its website, <link
      xlink:href="http://home.ccil.org/~cowan/XML/tagsoup/">http://home.ccil.org/~cowan/XML/tagsoup/</link>.</para>

      <para><variablelist>
          <varlistentry>
            <term>XPathParser</term>

            <listitem>
              <para>The
              <classname>cascading.operation.xml.XPathParser</classname>
              function will extract a value from the passed Tuple argument
              into a new Tuple field value. One Tuple value for every given
              XPath expression will be created. This function effectively
              converts an XML document into a table. If the returned value of
              the expression is a <classname>NodeList</classname>, only the
              first <classname>Node</classname> is used. The
              <classname>Node</classname> is converted to a new XML document
              and converted to a String. If only the text values are required,
              search on the <code>text()</code> nodes, or consider using
              XPathGenerator to handle multiple
              <classname>NodeList</classname> values.</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>XPathGenerator</term>

            <listitem>
              <para>The
              <classname>cascading.operation.xml.XPathGenerator</classname>
              function is a generator function that will emit a new Tuple for
              every <classname>Node</classname> returned by the given XPath
              expression.</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>XPathFilter</term>

            <listitem>
              <para>The
              <classname>cascading.operation.xml.XPathFilter</classname>
              filter will filter out a Tuple if the given XPath expression
              returns <code>false</code>. Set the removeMatch paramter to
              <code>true</code> if the filter should be reversed.</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>TapSoupParser</term>

            <listitem>
              <para>The
              <classname>cascading.operation.xml.TagSoupParser</classname>
              function uses the Tag Soup library to convert incoming HTML to
              clean XHTML. Use the <code>setFeature( feature, value )</code>
              method to set TagSoup specific features (as documented on the
              TagSoup website listed above).</para>
            </listitem>
          </varlistentry>
        </variablelist></para>
    </section>

    <section>
      <title>Assertions</title>

      <para>Cascading Stream Assertions are used to build robust reusable pipe
      assemblies. They can be planned out of a Flow instance during runtime.
      For more information see the section on <xref
      linkend="stream-assertions" />. Below we describe the Assertions
      available in the core library.</para>

      <para><variablelist>
          <varlistentry>
            <term>AssertEquals</term>

            <listitem>
              <para>The
              <classname>cascading.operation.assertion.AssertEquals</classname>
              Assertion asserts the number of values given on the constructor
              is equal to the number of argument Tuple values and that each
              constructor value is <code>.equals()</code> to its corresponding
              argument value.</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>AssertEqualsAll</term>

            <listitem>
              <para>The
              <classname>cascading.operation.assertion.AssertEqualsAll</classname>
              Assertion asserts that every value in the argument Tuple is
              <code>.equals()</code> to the single value given on the
              constructor.</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>AssertExpression</term>

            <listitem>
              <para>The
              <classname>cascading.operation.assertion.AssertExpression</classname>
              Assertion dynamically resolves a given Java expression (see
              <xref linkend="operation-expression" />) using argument Tuple
              values. Any Tuple that returns <code>true</code> for the given
              expression passes the assertion.</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>AssertMatches</term>

            <listitem>
              <para>The
              <classname>cascading.operation.assertion.AssertMatches</classname>
              Assertion matches the given regular expression pattern String
              against the whole argument Tuple by joining each individual
              element of the Tuple with a TAB character (\t).</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>AssertMatchesAll</term>

            <listitem>
              <para>The
              <classname>cascading.operation.assertion.AssertMatchesAll</classname>
              Assertion matches the given regular expression pattern String
              against each argument Tuple value individually.</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>AssertNotNull</term>

            <listitem>
              <para>The
              <classname>cascading.operation.assertion.AssertNotNull</classname>
              Assertion asserts that every value in the argument Tuple is not
              a <code>null</code> value.</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>AssertNull</term>

            <listitem>
              <para>The
              <classname>cascading.operation.assertion.AssertNull</classname>
              Assertion asserts that every value in the argument Tuple is a
              <code>null</code> value.</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>AssertSizeEquals</term>

            <listitem>
              <para>The
              <classname>cascading.operation.assertion.AssertSizeEquals</classname>
              Assertion asserts that the current Tuple in the tuple stream is
              exactly the given size. On evaluation, <code>Tuple#size()</code>
              is called (note Tuples may hold <code>null</code>
              values).</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>AssertSizeLessThan</term>

            <listitem>
              <para>The
              <classname>cascading.operation.assertion.AssertSizeLessThan</classname>
              Assertion asserts that the current Tuple in the stream has a
              size less than (<code>&lt;</code>) the given size. On
              evaluation, <code>Tuple#size()</code> is called (note Tuples may
              hold <code>null</code> values).</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>AssertSizeMoreThan</term>

            <listitem>
              <para>The
              <classname>cascading.operation.assertion.AssertSizeMoreThan</classname>
              Assertion asserts that the current Tuple in the stream has a
              size more than (<code>&gt;</code>) the given size. On
              evaluation, <code>Tuple#size()</code> is called (note Tuples may
              hold <code>null</code> values).</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>AssertGroupSizeEquals</term>

            <listitem>
              <para>The
              <classname>cascading.operation.assertion.AssertGroupSizeEquals</classname>
              Group Assertion asserts that the number of items in the current
              grouping is equal (<code>==</code>) the given size. If a pattern
              String is given, only grouping keys that match the regular
              expression will have this assertion applied where multiple key
              values are delimited by a TAB character.</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>AssertGroupSizeLessThan</term>

            <listitem>
              <para>The
              <classname>cascading.operation.assertion.AssertGroupSizeEquals</classname>
              Group Assertion asserts that the number of items in the current
              grouping is less than (<code>&lt;</code>) the given size. If a
              pattern String is given, only grouping keys that match the
              regular expression will have this assertion applied where
              multiple key values are delimited by a TAB character.</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>AssertGroupSizeMoreThan</term>

            <listitem>
              <para>The
              <classname>cascading.operation.assertion.AssertGroupSizeEquals</classname>
              Group Assertion asserts that the number of items in the current
              grouping is more than (<code>&gt;</code>) the given size. If a
              pattern String is given, only grouping keys that match the
              regular expression will have this assertion applied where
              multiple key values are delimited by a TAB character.</para>
            </listitem>
          </varlistentry>
        </variablelist></para>
    </section>

    <section>
      <title>Logical Filter Operators</title>

      <para>The logical Filter operators allows the user to assemble more
      complex filtering to be used in a single Pipe, instead of chaining
      multiple Pipes together to get the same effect.</para>

      <para><variablelist>
          <varlistentry>
            <term>And</term>

            <listitem>
              <para>The <classname>cascading.operation.filter.And</classname>
              Filter will logically 'and' the results of the constructor
              provided Filter instances. Logically, if
              <methodname>Filter#isRemove()</methodname> returns
              <code>true</code> for all given instances, this filter will
              return <code>true</code>.</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>Or</term>

            <listitem>
              <para>The <classname>cascading.operation.filter.Or</classname>
              Filter will logically 'or' the results of the constructor
              provided Filter instances. Logically, if
              <methodname>Filter#isRemove()</methodname> returns
              <code>true</code> for any of the given instances, this filter
              will return <code>true</code>.</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>Not</term>

            <listitem>
              <para>The <classname>cascading.operation.filter.Not</classname>
              Filter will logically 'not' (negation) the results of the
              constructor provided Filter instances. Logically, if
              <methodname>Filter#isRemove()</methodname> returns
              <code>true</code> for the given instance, this filter will
              return the opposite, <code>false</code>.</para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>Xor</term>

            <listitem>
              <para>The <classname>cascading.operation.filter.Xor</classname>
              Filter will logically 'xor' (exclusive or) the results of the
              constructor provided Filter instances. Logically, if
              <methodname>Filter#isRemove()</methodname> returns
              <code>true</code> for all given instances, or returns
              <code>false</code> for all given instances, this filter will
              return <code>true</code>. Note that Xor can only be applied to
              two values.</para>
            </listitem>
          </varlistentry>
        </variablelist></para>

      <example>
        <title>Combining Filters</title>

        <xi:include href="filter-and.xml" />
      </example>

      <para>Above, we are "and-ing" the two filters. Both must be satified for
      the data to pass through this one Pipe.</para>
    </section>
  </chapter>

  <chapter>
    <info>
      <title>How It Works</title>
    </info>

    <section>
      <title>MapReduce Job Planner</title>

      <para>The MapReduce Job Planner is an internal feature of
      Cascading.</para>

      <para>When a collection of functions, splits, and joins are all tied up
      together into a 'pipe assembly', the FlowConnector object is used to
      create a new Flow instance against input and output data paths. This
      Flow is a single Cascading job.</para>

      <para>Internally the FlowConnector employs an intelligent planner to
      convert the pipe assembly to a graph of dependent MapReduce jobs that
      can be executed on a Hadoop cluster.</para>

      <para>All this happens under the scenes. As is the scheduling of the
      individual MapReduce jobs, and the clean up of intermediate data sets
      that bind the jobs together.</para>

      <para><inlinemediaobject>
          <imageobject role="fo">
            <imagedata align="center" contentwidth="7in"
                       fileref="images/planned-flow.svg"></imagedata>
          </imageobject>

          <imageobject role="html">
            <imagedata align="center" contentwidth="7in"
                       fileref="images/planned-flow.png"></imagedata>
          </imageobject>

          <textobject>
            <phrase>A Flow partitioned by MapReduce tasks</phrase>
          </textobject>
        </inlinemediaobject></para>

      <para>Above we can see how a reasonably normal Flow would be partitioned
      into MapReduce jobs. Every job is delimited by a temporary file that is
      the sink from the first job, and then the source to the next job.</para>

      <para>To see how your Flows are partitioned, call the
      <classname>Flow#writeDOT()</classname> method. This will write a <link
      xlink:href="http://en.wikipedia.org/wiki/DOT_language">DOT</link> file
      out to the path specified, and can be imported into a graphics package
      like OmniGraffle or Graphviz.</para>
    </section>

    <section>
      <title xreflabel="Topological Scheduling" xml:id="cascade-scheduler"
      xml:lang="">The Cascade Topological Scheduler</title>

      <para>Cascading has a simple class, <classname>Cascade</classname>
, that will take a collection of
      Cascading Flows and execute them on the target cluster in dependency
      order.</para>

      <para>Consider the following example.</para>

      <itemizedlist>
        <listitem>
          <para>Flow 'first' reads input file A and outputs B.</para>
        </listitem>

        <listitem>
          <para>Flow 'second' expects input B and outputs C and D.</para>
        </listitem>

        <listitem>
          <para>Flow 'third' expects input C and outputs E.</para>
        </listitem>
      </itemizedlist>

      <para>A <classname>Cascade</classname>
 is constructed through the
      <classname>CascadeConnector</classname> class, by building an internal
      graph that makes each Flow a 'vertex', and each file an 'edge'. A
      topological walk on this graph will touch each vertex in order of its
      dependencies. When a vertex has all it's incoming edges (files)
      available, it will be scheduled on the cluster.</para>

      <para>In the example above, 'first' goes first, 'second' goes second,
      and 'third' is last.</para>

      <para>If two or more Flows are independent of one another, they will be
      scheduled concurrently.</para>

      <para>And by default, if any outputs from a Flow are newer than the
      inputs, the Flow is skipped. The assumption is that the Flow was
      executed recently, since the output isn't stale. So there is no reason
      to re-execute it and use up resources or add time to the job. This is
      similiar behaviour a compiler would exhibit if a source file wasn't
      updated before a recompile.</para>

      <para>This is very handy if you have a large number of jobs that should
      be executed as a logical unit with varying dependencies between them.
      Just pass them to the CascadeConnector, and let it sort them all
      out.</para>
    </section>
  </chapter>
</book>
